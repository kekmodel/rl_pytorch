{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IWnm3qot3o1W"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "from collections import deque\n",
    "from copy import deepcopy\n",
    "\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical, Dirichlet\n",
    "from torch.utils.data import DataLoader\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 708,
     "status": "ok",
     "timestamp": 1534482400648,
     "user": {
      "displayName": "윤승제",
      "photoUrl": "//lh5.googleusercontent.com/-EucKC7DmcQI/AAAAAAAAAAI/AAAAAAAAAGA/gQU1NPEmNFA/s50-c-k-no/photo.jpg",
      "userId": "105654037995838004821"
     },
     "user_tz": -540
    },
    "id": "maRVADiTlzHD",
    "outputId": "783b7610-95c2-4b54-b2ce-d8e853c484ba"
   },
   "outputs": [],
   "source": [
    "SEED = 1\n",
    "BATCH_SIZE = 32\n",
    "LR = 0.0003\n",
    "UP_COEF = 0.01\n",
    "ENT_COEF = 0.01\n",
    "GAMMA = 0.99\n",
    "\n",
    "# set device\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device('cuda' if use_cuda else 'cpu')\n",
    "\n",
    "# random seed\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if use_cuda:\n",
    "    torch.cuda.manual_seed_all(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9Ffkl_5C4R81"
   },
   "outputs": [],
   "source": [
    "class QNet(nn.Module):\n",
    "    def __init__(self, obs_space, action_space):\n",
    "        super().__init__()\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(obs_space, obs_space*10),\n",
    "            nn.SELU()\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(obs_space*10, 512),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(512, action_space)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.head(x)\n",
    "        q = self.fc(out).reshape(out.shape[0], -1)\n",
    "\n",
    "        return q\n",
    "\n",
    "\n",
    "class PolicyNet(nn.Module):\n",
    "    def __init__(self, obs_space, action_space):\n",
    "        super().__init__()\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(obs_space, obs_space*10),\n",
    "            nn.SELU()\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(obs_space*10, 512),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(512, action_space)\n",
    "        )\n",
    "\n",
    "        self.log_softmax = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.head(x)\n",
    "        logit = self.fc(out).reshape(out.shape[0], -1)\n",
    "        log_p = self.log_softmax(logit)\n",
    "\n",
    "        return log_p\n",
    "\n",
    "\n",
    "class ValueNet(nn.Module):\n",
    "    def __init__(self, obs_space):\n",
    "        super().__init__()\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(obs_space, obs_space*10),\n",
    "            nn.SELU()\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(obs_space*10, 512),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(512, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.head(x)\n",
    "        v = self.fc(out).reshape(out.shape[0], 1)\n",
    "\n",
    "        return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "\n",
    "\n",
    "def learn(q_net, p_net, v_net, v_tgt, optimizer, rep_memory):\n",
    "    global action_space\n",
    "    \n",
    "    q_net.train()\n",
    "    p_net.train()\n",
    "    v_net.train()\n",
    "    v_tgt.train()\n",
    "\n",
    "    train_data = random.sample(rep_memory, BATCH_SIZE)\n",
    "    dataloader = DataLoader(train_data,\n",
    "                            batch_size=BATCH_SIZE,\n",
    "                            pin_memory=use_cuda)\n",
    "\n",
    "    for i, (s, a, r, _s, d) in enumerate(dataloader):\n",
    "        s_batch = s.to(device).float()\n",
    "        a_batch = a.to(device).long()\n",
    "        _s_batch = _s.to(device).float()\n",
    "        r_batch = r.to(device).float()\n",
    "        done_mask = 1. - d.to(device).float()\n",
    "        discount = torch.full_like(r_batch, GAMMA)\n",
    "        \n",
    "        q_batch = q_net(s_batch)\n",
    "        q_acting = q_batch[range(BATCH_SIZE), a_batch]\n",
    "        q_acting_ = q_acting.detach() \n",
    "        \n",
    "        v_batch = v_net(s_batch)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            _log_p_batch = p_net(_s_batch)\n",
    "            _log_p_acting = _log_p_batch[range(BATCH_SIZE), a_batch]\n",
    "            v_target = q_acting_ - ENT_COEF * _log_p_acting\n",
    "            \n",
    "        v_loss = (v_target - v_batch).pow(2).mean()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            _v_batch = v_tgt(_s_batch) * done_mask\n",
    "            q_target = r_batch + _v_batch * discount\n",
    "        \n",
    "        q_loss = (q_target - q_acting).pow(2).mean()\n",
    "        \n",
    "        log_p_batch = p_net(s_batch)\n",
    "        q_batch_ = q_net(s_batch)\n",
    "        entropy = -(ENT_COEF * log_p_batch.exp() * q_batch).sum(dim=-1).mean()\n",
    "        \n",
    "        loss = v_loss + q_loss + entropy\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "#         nn.utils.clip_grad_norm_(total_params, max_norm=0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "def select_action(obs, p_net):\n",
    "    p_net.eval()\n",
    "    with torch.no_grad():\n",
    "        state = torch.tensor([obs]).to(device).float()\n",
    "        log_p = p_net(state)\n",
    "        m = Categorical(log_p.exp())\n",
    "        action = m.sample()\n",
    "\n",
    "    return action.item()\n",
    "\n",
    "\n",
    "def plot():\n",
    "    clear_output(True)\n",
    "    plt.figure(figsize=(16, 5))\n",
    "    plt.subplot(121)\n",
    "    plt.plot(ep_rewards)\n",
    "    plt.title('Reward')\n",
    "    plt.subplot(122)\n",
    "    plt.plot(losses)\n",
    "    plt.title('Loss')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 3377
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 135196,
     "status": "ok",
     "timestamp": 1534482559393,
     "user": {
      "displayName": "윤승제",
      "photoUrl": "//lh5.googleusercontent.com/-EucKC7DmcQI/AAAAAAAAAAI/AAAAAAAAAGA/gQU1NPEmNFA/s50-c-k-no/photo.jpg",
      "userId": "105654037995838004821"
     },
     "user_tz": -540
    },
    "id": "PnifSBJglzHh",
    "outputId": "94177345-918e-4a96-d9a8-d8aba0a4bc9a",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jay/anaconda3/lib/python3.7/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    }
   ],
   "source": [
    "# make an environment\n",
    "env = gym.make('CartPole-v0')\n",
    "# env = gym.make('CartPole-v1')\n",
    "# env = gym.make('MountainCar-v0')\n",
    "# env = gym.make('LunarLander-v2')\n",
    "\n",
    "env.seed(SEED)\n",
    "obs_space = env.observation_space.shape[0]\n",
    "action_space = env.action_space.n\n",
    "\n",
    "# hyperparameter\n",
    "n_episodes = 10000\n",
    "learn_start = 1500\n",
    "memory_size = 50000\n",
    "update_frq = 1\n",
    "use_eps_decay = False\n",
    "n_eval = env.spec.trials\n",
    "\n",
    "# global values\n",
    "total_steps = 0\n",
    "learn_steps = 0\n",
    "rewards = []\n",
    "reward_eval = deque(maxlen=n_eval)\n",
    "is_learned = False\n",
    "is_solved = False\n",
    "\n",
    "# make two nerual networks\n",
    "q_net = QNet(obs_space, action_space).to(device)\n",
    "p_net = PolicyNet(obs_space, action_space).to(device)\n",
    "v_net = ValueNet(obs_space).to(device)\n",
    "v_tgt = deepcopy(v_net)\n",
    "\n",
    "# make optimizer\n",
    "total_params = list(q_net.parameters()) + list(p_net.parameters()) +  list(v_net.parameters())\n",
    "optimizer = optim.Adam(total_params, lr=LR, eps=1e-5)\n",
    "\n",
    "# make a memory\n",
    "rep_memory = deque(maxlen=memory_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.spec.max_episode_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.spec.trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "195.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.spec.reward_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 3377
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 135196,
     "status": "ok",
     "timestamp": 1534482559393,
     "user": {
      "displayName": "윤승제",
      "photoUrl": "//lh5.googleusercontent.com/-EucKC7DmcQI/AAAAAAAAAAI/AAAAAAAAAGA/gQU1NPEmNFA/s50-c-k-no/photo.jpg",
      "userId": "105654037995838004821"
     },
     "user_tz": -540
    },
    "id": "PnifSBJglzHh",
    "outputId": "94177345-918e-4a96-d9a8-d8aba0a4bc9a",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1 Episode in    15 steps, reward 15.00\n",
      "  2 Episode in    41 steps, reward 26.00\n",
      "  3 Episode in    51 steps, reward 10.00\n",
      "  4 Episode in    62 steps, reward 11.00\n",
      "  5 Episode in    72 steps, reward 10.00\n",
      "  6 Episode in   105 steps, reward 33.00\n",
      "  7 Episode in   151 steps, reward 46.00\n",
      "  8 Episode in   163 steps, reward 12.00\n",
      "  9 Episode in   189 steps, reward 26.00\n",
      " 10 Episode in   206 steps, reward 17.00\n",
      " 11 Episode in   224 steps, reward 18.00\n",
      " 12 Episode in   242 steps, reward 18.00\n",
      " 13 Episode in   267 steps, reward 25.00\n",
      " 14 Episode in   282 steps, reward 15.00\n",
      " 15 Episode in   295 steps, reward 13.00\n",
      " 16 Episode in   307 steps, reward 12.00\n",
      " 17 Episode in   324 steps, reward 17.00\n",
      " 18 Episode in   344 steps, reward 20.00\n",
      " 19 Episode in   364 steps, reward 20.00\n",
      " 20 Episode in   383 steps, reward 19.00\n",
      " 21 Episode in   430 steps, reward 47.00\n",
      " 22 Episode in   452 steps, reward 22.00\n",
      " 23 Episode in   469 steps, reward 17.00\n",
      " 24 Episode in   488 steps, reward 19.00\n",
      " 25 Episode in   511 steps, reward 23.00\n",
      " 26 Episode in   524 steps, reward 13.00\n",
      " 27 Episode in   546 steps, reward 22.00\n",
      " 28 Episode in   559 steps, reward 13.00\n",
      " 29 Episode in   577 steps, reward 18.00\n",
      " 30 Episode in   591 steps, reward 14.00\n",
      " 31 Episode in   609 steps, reward 18.00\n",
      " 32 Episode in   630 steps, reward 21.00\n",
      " 33 Episode in   643 steps, reward 13.00\n",
      " 34 Episode in   703 steps, reward 60.00\n",
      " 35 Episode in   717 steps, reward 14.00\n",
      " 36 Episode in   771 steps, reward 54.00\n",
      " 37 Episode in   793 steps, reward 22.00\n",
      " 38 Episode in   818 steps, reward 25.00\n",
      " 39 Episode in   834 steps, reward 16.00\n",
      " 40 Episode in   849 steps, reward 15.00\n",
      " 41 Episode in   869 steps, reward 20.00\n",
      " 42 Episode in   898 steps, reward 29.00\n",
      " 43 Episode in   913 steps, reward 15.00\n",
      " 44 Episode in   923 steps, reward 10.00\n",
      " 45 Episode in   957 steps, reward 34.00\n",
      " 46 Episode in   975 steps, reward 18.00\n",
      " 47 Episode in  1001 steps, reward 26.00\n",
      " 48 Episode in  1011 steps, reward 10.00\n",
      " 49 Episode in  1028 steps, reward 17.00\n",
      " 50 Episode in  1049 steps, reward 21.00\n",
      " 51 Episode in  1060 steps, reward 11.00\n",
      " 52 Episode in  1079 steps, reward 19.00\n",
      " 53 Episode in  1089 steps, reward 10.00\n",
      " 54 Episode in  1103 steps, reward 14.00\n",
      " 55 Episode in  1117 steps, reward 14.00\n",
      " 56 Episode in  1130 steps, reward 13.00\n",
      " 57 Episode in  1151 steps, reward 21.00\n",
      " 58 Episode in  1178 steps, reward 27.00\n",
      " 59 Episode in  1200 steps, reward 22.00\n",
      " 60 Episode in  1218 steps, reward 18.00\n",
      " 61 Episode in  1229 steps, reward 11.00\n",
      " 62 Episode in  1241 steps, reward 12.00\n",
      " 63 Episode in  1263 steps, reward 22.00\n",
      " 64 Episode in  1277 steps, reward 14.00\n",
      " 65 Episode in  1300 steps, reward 23.00\n",
      " 66 Episode in  1340 steps, reward 40.00\n",
      " 67 Episode in  1352 steps, reward 12.00\n",
      " 68 Episode in  1372 steps, reward 20.00\n",
      " 69 Episode in  1388 steps, reward 16.00\n",
      " 70 Episode in  1405 steps, reward 17.00\n",
      " 71 Episode in  1420 steps, reward 15.00\n",
      " 72 Episode in  1459 steps, reward 39.00\n",
      " 73 Episode in  1469 steps, reward 10.00\n",
      " 74 Episode in  1480 steps, reward 11.00\n",
      " 75 Episode in  1497 steps, reward 17.00\n",
      "\n",
      "============  Start Learning  ============\n",
      "\n",
      " 76 Episode in  1513 steps, reward 16.00\n",
      " 77 Episode in  1524 steps, reward 11.00\n",
      " 78 Episode in  1544 steps, reward 20.00\n",
      " 79 Episode in  1556 steps, reward 12.00\n",
      " 80 Episode in  1568 steps, reward 12.00\n",
      " 81 Episode in  1591 steps, reward 23.00\n",
      " 82 Episode in  1609 steps, reward 18.00\n",
      " 83 Episode in  1622 steps, reward 13.00\n",
      " 84 Episode in  1634 steps, reward 12.00\n",
      " 85 Episode in  1644 steps, reward 10.00\n",
      " 86 Episode in  1654 steps, reward 10.00\n",
      " 87 Episode in  1664 steps, reward 10.00\n",
      " 88 Episode in  1678 steps, reward 14.00\n",
      " 89 Episode in  1688 steps, reward 10.00\n",
      " 90 Episode in  1698 steps, reward 10.00\n",
      " 91 Episode in  1708 steps, reward 10.00\n",
      " 92 Episode in  1718 steps, reward 10.00\n",
      " 93 Episode in  1728 steps, reward 10.00\n",
      " 94 Episode in  1737 steps, reward 9.00\n",
      " 95 Episode in  1746 steps, reward 9.00\n",
      " 96 Episode in  1756 steps, reward 10.00\n",
      " 97 Episode in  1766 steps, reward 10.00\n",
      " 98 Episode in  1776 steps, reward 10.00\n",
      " 99 Episode in  1787 steps, reward 11.00\n",
      "100 Episode in  1801 steps, reward 14.00\n",
      "101 Episode in  1824 steps, reward 23.00\n",
      "102 Episode in  1840 steps, reward 16.00\n",
      "103 Episode in  1852 steps, reward 12.00\n",
      "104 Episode in  1865 steps, reward 13.00\n",
      "105 Episode in  1877 steps, reward 12.00\n",
      "106 Episode in  1894 steps, reward 17.00\n",
      "107 Episode in  1918 steps, reward 24.00\n",
      "108 Episode in  1937 steps, reward 19.00\n",
      "109 Episode in  1963 steps, reward 26.00\n",
      "110 Episode in  2005 steps, reward 42.00\n",
      "111 Episode in  2039 steps, reward 34.00\n",
      "112 Episode in  2065 steps, reward 26.00\n",
      "113 Episode in  2087 steps, reward 22.00\n",
      "114 Episode in  2107 steps, reward 20.00\n",
      "115 Episode in  2132 steps, reward 25.00\n",
      "116 Episode in  2179 steps, reward 47.00\n",
      "117 Episode in  2201 steps, reward 22.00\n",
      "118 Episode in  2231 steps, reward 30.00\n",
      "119 Episode in  2282 steps, reward 51.00\n",
      "120 Episode in  2344 steps, reward 62.00\n",
      "121 Episode in  2365 steps, reward 21.00\n",
      "122 Episode in  2378 steps, reward 13.00\n",
      "123 Episode in  2388 steps, reward 10.00\n",
      "124 Episode in  2396 steps, reward 8.00\n",
      "125 Episode in  2406 steps, reward 10.00\n",
      "126 Episode in  2414 steps, reward 8.00\n",
      "127 Episode in  2423 steps, reward 9.00\n",
      "128 Episode in  2434 steps, reward 11.00\n",
      "129 Episode in  2447 steps, reward 13.00\n",
      "130 Episode in  2457 steps, reward 10.00\n",
      "131 Episode in  2469 steps, reward 12.00\n",
      "132 Episode in  2479 steps, reward 10.00\n",
      "133 Episode in  2494 steps, reward 15.00\n",
      "134 Episode in  2517 steps, reward 23.00\n",
      "135 Episode in  2537 steps, reward 20.00\n",
      "136 Episode in  2562 steps, reward 25.00\n",
      "137 Episode in  2580 steps, reward 18.00\n",
      "138 Episode in  2594 steps, reward 14.00\n",
      "139 Episode in  2606 steps, reward 12.00\n",
      "140 Episode in  2618 steps, reward 12.00\n",
      "141 Episode in  2631 steps, reward 13.00\n",
      "142 Episode in  2647 steps, reward 16.00\n",
      "143 Episode in  2663 steps, reward 16.00\n",
      "144 Episode in  2678 steps, reward 15.00\n",
      "145 Episode in  2688 steps, reward 10.00\n",
      "146 Episode in  2698 steps, reward 10.00\n",
      "147 Episode in  2707 steps, reward 9.00\n",
      "148 Episode in  2719 steps, reward 12.00\n",
      "149 Episode in  2734 steps, reward 15.00\n",
      "150 Episode in  2753 steps, reward 19.00\n",
      "151 Episode in  2772 steps, reward 19.00\n",
      "152 Episode in  2791 steps, reward 19.00\n",
      "153 Episode in  2806 steps, reward 15.00\n",
      "154 Episode in  2817 steps, reward 11.00\n",
      "155 Episode in  2830 steps, reward 13.00\n",
      "156 Episode in  2841 steps, reward 11.00\n",
      "157 Episode in  2852 steps, reward 11.00\n",
      "158 Episode in  2869 steps, reward 17.00\n",
      "159 Episode in  2889 steps, reward 20.00\n",
      "160 Episode in  2907 steps, reward 18.00\n",
      "161 Episode in  2925 steps, reward 18.00\n",
      "162 Episode in  2937 steps, reward 12.00\n",
      "163 Episode in  2948 steps, reward 11.00\n",
      "164 Episode in  2958 steps, reward 10.00\n",
      "165 Episode in  2967 steps, reward 9.00\n",
      "166 Episode in  2977 steps, reward 10.00\n",
      "167 Episode in  2987 steps, reward 10.00\n",
      "168 Episode in  2997 steps, reward 10.00\n",
      "169 Episode in  3007 steps, reward 10.00\n",
      "170 Episode in  3016 steps, reward 9.00\n",
      "171 Episode in  3026 steps, reward 10.00\n",
      "172 Episode in  3036 steps, reward 10.00\n",
      "173 Episode in  3045 steps, reward 9.00\n",
      "174 Episode in  3054 steps, reward 9.00\n",
      "175 Episode in  3063 steps, reward 9.00\n",
      "176 Episode in  3074 steps, reward 11.00\n",
      "177 Episode in  3082 steps, reward 8.00\n",
      "178 Episode in  3093 steps, reward 11.00\n",
      "179 Episode in  3103 steps, reward 10.00\n",
      "180 Episode in  3114 steps, reward 11.00\n",
      "181 Episode in  3124 steps, reward 10.00\n",
      "182 Episode in  3137 steps, reward 13.00\n",
      "183 Episode in  3153 steps, reward 16.00\n",
      "184 Episode in  3218 steps, reward 65.00\n",
      "185 Episode in  3230 steps, reward 12.00\n",
      "186 Episode in  3241 steps, reward 11.00\n",
      "187 Episode in  3250 steps, reward 9.00\n",
      "188 Episode in  3260 steps, reward 10.00\n",
      "189 Episode in  3270 steps, reward 10.00\n",
      "190 Episode in  3281 steps, reward 11.00\n",
      "191 Episode in  3289 steps, reward 8.00\n",
      "192 Episode in  3298 steps, reward 9.00\n",
      "193 Episode in  3308 steps, reward 10.00\n",
      "194 Episode in  3318 steps, reward 10.00\n",
      "195 Episode in  3327 steps, reward 9.00\n",
      "196 Episode in  3337 steps, reward 10.00\n",
      "197 Episode in  3345 steps, reward 8.00\n",
      "198 Episode in  3354 steps, reward 9.00\n",
      "199 Episode in  3363 steps, reward 9.00\n",
      "200 Episode in  3373 steps, reward 10.00\n",
      "201 Episode in  3383 steps, reward 10.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "202 Episode in  3392 steps, reward 9.00\n",
      "203 Episode in  3400 steps, reward 8.00\n",
      "204 Episode in  3408 steps, reward 8.00\n",
      "205 Episode in  3418 steps, reward 10.00\n",
      "206 Episode in  3427 steps, reward 9.00\n",
      "207 Episode in  3436 steps, reward 9.00\n",
      "208 Episode in  3446 steps, reward 10.00\n",
      "209 Episode in  3454 steps, reward 8.00\n",
      "210 Episode in  3463 steps, reward 9.00\n",
      "211 Episode in  3473 steps, reward 10.00\n",
      "212 Episode in  3481 steps, reward 8.00\n",
      "213 Episode in  3491 steps, reward 10.00\n",
      "214 Episode in  3499 steps, reward 8.00\n",
      "215 Episode in  3508 steps, reward 9.00\n",
      "216 Episode in  3518 steps, reward 10.00\n",
      "217 Episode in  3528 steps, reward 10.00\n",
      "218 Episode in  3536 steps, reward 8.00\n",
      "219 Episode in  3545 steps, reward 9.00\n",
      "220 Episode in  3555 steps, reward 10.00\n",
      "221 Episode in  3563 steps, reward 8.00\n",
      "222 Episode in  3572 steps, reward 9.00\n",
      "223 Episode in  3580 steps, reward 8.00\n",
      "224 Episode in  3590 steps, reward 10.00\n",
      "225 Episode in  3600 steps, reward 10.00\n",
      "226 Episode in  3610 steps, reward 10.00\n",
      "227 Episode in  3620 steps, reward 10.00\n",
      "228 Episode in  3629 steps, reward 9.00\n",
      "229 Episode in  3638 steps, reward 9.00\n",
      "230 Episode in  3646 steps, reward 8.00\n",
      "231 Episode in  3655 steps, reward 9.00\n",
      "232 Episode in  3664 steps, reward 9.00\n",
      "233 Episode in  3673 steps, reward 9.00\n",
      "234 Episode in  3682 steps, reward 9.00\n",
      "235 Episode in  3691 steps, reward 9.00\n",
      "236 Episode in  3701 steps, reward 10.00\n",
      "237 Episode in  3711 steps, reward 10.00\n",
      "238 Episode in  3719 steps, reward 8.00\n",
      "239 Episode in  3729 steps, reward 10.00\n",
      "240 Episode in  3739 steps, reward 10.00\n",
      "241 Episode in  3749 steps, reward 10.00\n",
      "242 Episode in  3759 steps, reward 10.00\n",
      "243 Episode in  3768 steps, reward 9.00\n",
      "244 Episode in  3777 steps, reward 9.00\n",
      "245 Episode in  3787 steps, reward 10.00\n",
      "246 Episode in  3796 steps, reward 9.00\n",
      "247 Episode in  3804 steps, reward 8.00\n",
      "248 Episode in  3813 steps, reward 9.00\n",
      "249 Episode in  3823 steps, reward 10.00\n",
      "250 Episode in  3832 steps, reward 9.00\n",
      "251 Episode in  3841 steps, reward 9.00\n",
      "252 Episode in  3851 steps, reward 10.00\n",
      "253 Episode in  3860 steps, reward 9.00\n",
      "254 Episode in  3869 steps, reward 9.00\n",
      "255 Episode in  3878 steps, reward 9.00\n",
      "256 Episode in  3887 steps, reward 9.00\n",
      "257 Episode in  3897 steps, reward 10.00\n",
      "258 Episode in  3906 steps, reward 9.00\n",
      "259 Episode in  3914 steps, reward 8.00\n",
      "260 Episode in  3922 steps, reward 8.00\n",
      "261 Episode in  3931 steps, reward 9.00\n",
      "262 Episode in  3940 steps, reward 9.00\n",
      "263 Episode in  3950 steps, reward 10.00\n",
      "264 Episode in  3959 steps, reward 9.00\n",
      "265 Episode in  3968 steps, reward 9.00\n",
      "266 Episode in  3976 steps, reward 8.00\n",
      "267 Episode in  3986 steps, reward 10.00\n",
      "268 Episode in  3995 steps, reward 9.00\n",
      "269 Episode in  4004 steps, reward 9.00\n",
      "270 Episode in  4014 steps, reward 10.00\n",
      "271 Episode in  4024 steps, reward 10.00\n",
      "272 Episode in  4033 steps, reward 9.00\n",
      "273 Episode in  4042 steps, reward 9.00\n"
     ]
    }
   ],
   "source": [
    "# play\n",
    "for i in range(1, n_episodes + 1):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    ep_reward = 0\n",
    "    while not done:\n",
    "#         env.render()\n",
    "        action = select_action(obs, p_net)\n",
    "        _obs, reward, done, _ = env.step(action)\n",
    "        rep_memory.append((obs, action, reward, _obs, done))\n",
    "\n",
    "        obs = _obs\n",
    "        total_steps += 1\n",
    "        ep_reward += reward\n",
    "\n",
    "        if len(rep_memory) >= learn_start:\n",
    "            if len(rep_memory) == learn_start:\n",
    "                print('\\n============  Start Learning  ============\\n')\n",
    "            learn(q_net, p_net, v_net, v_tgt, optimizer, rep_memory)\n",
    "            learn_steps += 1\n",
    "\n",
    "        if learn_steps == update_frq:\n",
    "            # target smoothing update\n",
    "            with torch.no_grad():\n",
    "                for t, n in zip(v_tgt.parameters(), v_net.parameters()):\n",
    "                    t.data = UP_COEF * n.data + (1 - UP_COEF) * t.data\n",
    "            learn_steps = 0\n",
    "    if done:\n",
    "        rewards.append(ep_reward)\n",
    "        reward_eval.append(ep_reward)\n",
    "        plot()\n",
    "#         print('{:3} Episode in {:5} steps, reward {:.2f}'.format(\n",
    "#             i, total_steps, ep_reward))\n",
    "\n",
    "        if len(reward_eval) >= n_eval:\n",
    "            if np.mean(reward_eval) >= env.spec.reward_threshold:\n",
    "                print('\\n{} is sloved! {:3} Episode in {:3} steps'.format(\n",
    "                    env.spec.id, i, total_steps))\n",
    "                torch.save(target_net.state_dict(),\n",
    "                           f'./test/saved_models/{env.spec.id}_ep{i}_clear_model_sac.pt')\n",
    "                break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "plt.title('Reward')\n",
    "plt.plot(rewards)\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.title('Loss')\n",
    "plt.plot(losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[\n",
    "    ('CartPole-v0', 299, 0.25),\n",
    "    ('CartPole-v1', 413, 0.025),\n",
    "    ('MountainCar-v0', None ,0.05)\n",
    "]"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "C51_tensorflow.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
