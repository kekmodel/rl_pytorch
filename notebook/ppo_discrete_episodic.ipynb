{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IWnm3qot3o1W"
   },
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "from copy import deepcopy\n",
    "from pprint import pprint\n",
    "\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 708,
     "status": "ok",
     "timestamp": 1534482400648,
     "user": {
      "displayName": "윤승제",
      "photoUrl": "//lh5.googleusercontent.com/-EucKC7DmcQI/AAAAAAAAAAI/AAAAAAAAAGA/gQU1NPEmNFA/s50-c-k-no/photo.jpg",
      "userId": "105654037995838004821"
     },
     "user_tz": -540
    },
    "id": "maRVADiTlzHD",
    "outputId": "783b7610-95c2-4b54-b2ce-d8e853c484ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda: True\n"
     ]
    }
   ],
   "source": [
    "SEED = 1\n",
    "BATCH_SIZE = 4\n",
    "LR = 0.00025\n",
    "EPOCHS = 4\n",
    "CLIP = 0.1\n",
    "GAMMA = 0.99\n",
    "LAMBDA = 0.95\n",
    "ENT_COEF = 0.01\n",
    "V_CLIP = False\n",
    "\n",
    "# set device\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print('cuda:', use_cuda)\n",
    "device = torch.device('cuda' if use_cuda else 'cpu')\n",
    "\n",
    "# random seed\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if use_cuda:\n",
    "    torch.cuda.manual_seed_all(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9Ffkl_5C4R81"
   },
   "outputs": [],
   "source": [
    "class ActorCriticNet(nn.Module):\n",
    "    def __init__(self, obs_space, action_space):\n",
    "        super().__init__()\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(obs_space, 256),\n",
    "            nn.SELU()\n",
    "        )\n",
    "\n",
    "        self.pol = nn.Sequential(\n",
    "            nn.Linear(256, 256),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(256, action_space)\n",
    "        )\n",
    "        \n",
    "        self.val = nn.Sequential(\n",
    "            nn.Linear(256, 256),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "        \n",
    "        self.log_softmax = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.head(x)\n",
    "        logit = self.pol(out).reshape(out.shape[0], -1)\n",
    "        value = self.val(out).reshape(out.shape[0], 1)\n",
    "        log_p = self.log_softmax(logit)\n",
    "        \n",
    "        return log_p, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "\n",
    "\n",
    "def learn(net, old_net, optimizer, train_memory):\n",
    "    net.train()\n",
    "    old_net.train()\n",
    "    dataloader = DataLoader(train_memory,\n",
    "                            batch_size=BATCH_SIZE,\n",
    "                            shuffle=True,\n",
    "                            pin_memory=use_cuda)\n",
    "\n",
    "    for i in range(EPOCHS):\n",
    "        for (s, a, _s, ret, adv) in dataloader:\n",
    "            s_batch = s.to(device).float()\n",
    "            a_batch = a.detach().to(device).long()\n",
    "            _s_batch = _s.to(device).float()\n",
    "            ret_batch = ret.to(device).float()\n",
    "            adv_batch = adv.to(device).float()\n",
    "            batch_size = s_batch.shape[0]\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                log_p_batch_old, v_batch_old = old_net(s_batch)\n",
    "                log_p_acting_old = log_p_batch_old[range(batch_size), a_batch]\n",
    "                \n",
    "            log_p_batch, v_batch = net(s_batch)\n",
    "            log_p_acting = log_p_batch[range(batch_size), a_batch]\n",
    "            p_ratio = (log_p_acting - log_p_acting_old).exp()\n",
    "            p_ratio_clip = torch.clamp(p_ratio, 1. - CLIP, 1. + CLIP)\n",
    "            p_loss = torch.min(p_ratio * adv_batch, p_ratio_clip * adv_batch).mean()\n",
    "            if V_CLIP:\n",
    "                v_clip = v_batch_old + torch.clamp(v_batch - v_batch_old, -CLIP, CLIP)\n",
    "                v_loss1 = (ret_batch - v_clip).pow(2)\n",
    "                v_loss2 = (ret_batch - v_batch).pow(2)\n",
    "                v_loss = torch.max(v_loss1, v_loss2).mean()\n",
    "            else:\n",
    "                v_loss = (ret_batch - v_batch).pow(2).mean()\n",
    "            entropy = -(log_p_batch.exp() * log_p_batch).sum(dim=1).mean()\n",
    "\n",
    "            # loss\n",
    "            loss = -(p_loss - v_loss + ENT_COEF * entropy)\n",
    "            losses.append(loss)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "def get_action_and_value(obs, net):\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        state = torch.tensor([obs]).to(device).float()\n",
    "        log_p, v = net(state)\n",
    "        m = Categorical(log_p.exp())\n",
    "        action = m.sample()\n",
    "\n",
    "    return action.item(), v.item()\n",
    "\n",
    "\n",
    "def compute_adv(rewards, values, roll_memory):\n",
    "    dis_r = np.array(\n",
    "        [GAMMA**(i) * r for i, r in enumerate(rewards)]\n",
    "    )\n",
    "    for i, (roll, val) in enumerate(zip(roll_memory, values)):\n",
    "        ret = sum(dis_r[i:] / GAMMA**(i))\n",
    "        adv = ret - val \n",
    "        roll.extend([ret, adv])\n",
    "\n",
    "    return roll_memory\n",
    "\n",
    "\n",
    "def compute_adv_with_gae(rewards, values, roll_memory):\n",
    "    rew = np.array(rewards, 'float')\n",
    "    val = np.array(values[:-1], 'float')\n",
    "    _val = np.array(values[1:], 'float')\n",
    "    delta = rew + GAMMA * _val - val\n",
    "    dis_r = np.array([GAMMA**(i) * r for i, r in enumerate(rewards)], 'float')\n",
    "    gae_dt = np.array([(GAMMA * LAMBDA)**(i) * dt for i, dt in enumerate(delta.tolist())], 'float')\n",
    "    for i, data in enumerate(roll_memory):\n",
    "        data.append(sum(dis_r[i:] / GAMMA**(i)))\n",
    "        data.append(sum(gae_dt[i:] / (GAMMA * LAMBDA)**(i)))\n",
    "\n",
    "    return roll_memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 3377
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 135196,
     "status": "ok",
     "timestamp": 1534482559393,
     "user": {
      "displayName": "윤승제",
      "photoUrl": "//lh5.googleusercontent.com/-EucKC7DmcQI/AAAAAAAAAAI/AAAAAAAAAGA/gQU1NPEmNFA/s50-c-k-no/photo.jpg",
      "userId": "105654037995838004821"
     },
     "user_tz": -540
    },
    "id": "PnifSBJglzHh",
    "outputId": "94177345-918e-4a96-d9a8-d8aba0a4bc9a",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# make an environment\n",
    "# env = gym.make('CartPole-v0')\n",
    "# env = gym.make('CartPole-v1')\n",
    "env = gym.make('MountainCar-v0')\n",
    "# env = gym.make('LunarLander-v2')\n",
    "\n",
    "env.seed(SEED)\n",
    "obs_space = env.observation_space.shape[0]\n",
    "action_space = env.action_space.n\n",
    "\n",
    "# hyperparameter\n",
    "n_episodes = 1000\n",
    "n_roll_ep = 5\n",
    "n_eval = env.spec.trials\n",
    "\n",
    "# global values\n",
    "steps = 0\n",
    "learn_steps = 0\n",
    "ep_rewards = []\n",
    "reward_eval = deque(maxlen=n_eval)\n",
    "is_rollout = False\n",
    "is_solved = False\n",
    "\n",
    "# make memories\n",
    "net_memory = deque(maxlen=2)\n",
    "train_memory = []\n",
    "roll_memory = []\n",
    "rewards = []\n",
    "values = []\n",
    "\n",
    "# make nerual networks\n",
    "net = ActorCriticNet(obs_space, action_space).to(device)\n",
    "old_net = deepcopy(net)\n",
    "net_memory.appendleft(net.state_dict())\n",
    "\n",
    "# make a optimizer\n",
    "optimizer = optim.Adam(net.parameters(), lr=LR, eps=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.spec.max_episode_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.spec.trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-110.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.spec.reward_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 3377
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 135196,
     "status": "ok",
     "timestamp": 1534482559393,
     "user": {
      "displayName": "윤승제",
      "photoUrl": "//lh5.googleusercontent.com/-EucKC7DmcQI/AAAAAAAAAAI/AAAAAAAAAGA/gQU1NPEmNFA/s50-c-k-no/photo.jpg",
      "userId": "105654037995838004821"
     },
     "user_tz": -540
    },
    "id": "PnifSBJglzHh",
    "outputId": "94177345-918e-4a96-d9a8-d8aba0a4bc9a",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1 Episode in   200 steps, reward -200.00\n",
      "  2 Episode in   400 steps, reward -200.00\n",
      "  3 Episode in   600 steps, reward -200.00\n",
      "  4 Episode in   800 steps, reward -200.00\n",
      "  5 Episode in  1000 steps, reward -200.00\n",
      "  6 Episode in  1200 steps, reward -200.00\n",
      "  7 Episode in  1400 steps, reward -200.00\n",
      "  8 Episode in  1600 steps, reward -200.00\n",
      "  9 Episode in  1800 steps, reward -200.00\n",
      " 10 Episode in  2000 steps, reward -200.00\n",
      " 11 Episode in  2200 steps, reward -200.00\n",
      " 12 Episode in  2400 steps, reward -200.00\n",
      " 13 Episode in  2600 steps, reward -200.00\n",
      " 14 Episode in  2800 steps, reward -200.00\n",
      " 15 Episode in  3000 steps, reward -200.00\n",
      " 16 Episode in  3200 steps, reward -200.00\n",
      " 17 Episode in  3400 steps, reward -200.00\n",
      " 18 Episode in  3600 steps, reward -200.00\n",
      " 19 Episode in  3800 steps, reward -200.00\n",
      " 20 Episode in  4000 steps, reward -200.00\n",
      " 21 Episode in  4200 steps, reward -200.00\n",
      " 22 Episode in  4400 steps, reward -200.00\n",
      " 23 Episode in  4600 steps, reward -200.00\n",
      " 24 Episode in  4800 steps, reward -200.00\n",
      " 25 Episode in  5000 steps, reward -200.00\n",
      " 26 Episode in  5200 steps, reward -200.00\n",
      " 27 Episode in  5400 steps, reward -200.00\n",
      " 28 Episode in  5600 steps, reward -200.00\n",
      " 29 Episode in  5800 steps, reward -200.00\n",
      " 30 Episode in  6000 steps, reward -200.00\n",
      " 31 Episode in  6200 steps, reward -200.00\n",
      " 32 Episode in  6400 steps, reward -200.00\n",
      " 33 Episode in  6600 steps, reward -200.00\n",
      " 34 Episode in  6800 steps, reward -200.00\n",
      " 35 Episode in  7000 steps, reward -200.00\n",
      " 36 Episode in  7200 steps, reward -200.00\n",
      " 37 Episode in  7400 steps, reward -200.00\n",
      " 38 Episode in  7600 steps, reward -200.00\n",
      " 39 Episode in  7800 steps, reward -200.00\n",
      " 40 Episode in  8000 steps, reward -200.00\n",
      " 41 Episode in  8200 steps, reward -200.00\n",
      " 42 Episode in  8400 steps, reward -200.00\n",
      " 43 Episode in  8600 steps, reward -200.00\n",
      " 44 Episode in  8800 steps, reward -200.00\n",
      " 45 Episode in  9000 steps, reward -200.00\n",
      " 46 Episode in  9200 steps, reward -200.00\n",
      " 47 Episode in  9400 steps, reward -200.00\n",
      " 48 Episode in  9600 steps, reward -200.00\n",
      " 49 Episode in  9800 steps, reward -200.00\n",
      " 50 Episode in 10000 steps, reward -200.00\n",
      " 51 Episode in 10200 steps, reward -200.00\n",
      " 52 Episode in 10400 steps, reward -200.00\n",
      " 53 Episode in 10600 steps, reward -200.00\n",
      " 54 Episode in 10800 steps, reward -200.00\n",
      " 55 Episode in 11000 steps, reward -200.00\n",
      " 56 Episode in 11200 steps, reward -200.00\n",
      " 57 Episode in 11400 steps, reward -200.00\n",
      " 58 Episode in 11600 steps, reward -200.00\n",
      " 59 Episode in 11800 steps, reward -200.00\n",
      " 60 Episode in 12000 steps, reward -200.00\n",
      " 61 Episode in 12200 steps, reward -200.00\n",
      " 62 Episode in 12400 steps, reward -200.00\n",
      " 63 Episode in 12600 steps, reward -200.00\n",
      " 64 Episode in 12800 steps, reward -200.00\n",
      " 65 Episode in 13000 steps, reward -200.00\n",
      " 66 Episode in 13200 steps, reward -200.00\n",
      " 67 Episode in 13400 steps, reward -200.00\n",
      " 68 Episode in 13600 steps, reward -200.00\n",
      " 69 Episode in 13800 steps, reward -200.00\n",
      " 70 Episode in 14000 steps, reward -200.00\n",
      " 71 Episode in 14200 steps, reward -200.00\n",
      " 72 Episode in 14400 steps, reward -200.00\n",
      " 73 Episode in 14600 steps, reward -200.00\n",
      " 74 Episode in 14800 steps, reward -200.00\n",
      " 75 Episode in 15000 steps, reward -200.00\n",
      " 76 Episode in 15200 steps, reward -200.00\n",
      " 77 Episode in 15400 steps, reward -200.00\n",
      " 78 Episode in 15600 steps, reward -200.00\n",
      " 79 Episode in 15800 steps, reward -200.00\n",
      " 80 Episode in 16000 steps, reward -200.00\n",
      " 81 Episode in 16200 steps, reward -200.00\n",
      " 82 Episode in 16400 steps, reward -200.00\n",
      " 83 Episode in 16600 steps, reward -200.00\n",
      " 84 Episode in 16800 steps, reward -200.00\n",
      " 85 Episode in 17000 steps, reward -200.00\n",
      " 86 Episode in 17200 steps, reward -200.00\n",
      " 87 Episode in 17400 steps, reward -200.00\n",
      " 88 Episode in 17600 steps, reward -200.00\n",
      " 89 Episode in 17800 steps, reward -200.00\n",
      " 90 Episode in 18000 steps, reward -200.00\n",
      " 91 Episode in 18200 steps, reward -200.00\n",
      " 92 Episode in 18400 steps, reward -200.00\n",
      " 93 Episode in 18600 steps, reward -200.00\n",
      " 94 Episode in 18800 steps, reward -200.00\n",
      " 95 Episode in 19000 steps, reward -200.00\n",
      " 96 Episode in 19200 steps, reward -200.00\n",
      " 97 Episode in 19400 steps, reward -200.00\n",
      " 98 Episode in 19600 steps, reward -200.00\n",
      " 99 Episode in 19800 steps, reward -200.00\n",
      "100 Episode in 20000 steps, reward -200.00\n",
      "101 Episode in 20200 steps, reward -200.00\n",
      "102 Episode in 20400 steps, reward -200.00\n",
      "103 Episode in 20600 steps, reward -200.00\n",
      "104 Episode in 20800 steps, reward -200.00\n",
      "105 Episode in 21000 steps, reward -200.00\n",
      "106 Episode in 21200 steps, reward -200.00\n",
      "107 Episode in 21400 steps, reward -200.00\n",
      "108 Episode in 21600 steps, reward -200.00\n",
      "109 Episode in 21800 steps, reward -200.00\n",
      "110 Episode in 22000 steps, reward -200.00\n",
      "111 Episode in 22200 steps, reward -200.00\n",
      "112 Episode in 22400 steps, reward -200.00\n",
      "113 Episode in 22600 steps, reward -200.00\n",
      "114 Episode in 22800 steps, reward -200.00\n",
      "115 Episode in 23000 steps, reward -200.00\n",
      "116 Episode in 23200 steps, reward -200.00\n",
      "117 Episode in 23400 steps, reward -200.00\n",
      "118 Episode in 23600 steps, reward -200.00\n",
      "119 Episode in 23800 steps, reward -200.00\n",
      "120 Episode in 24000 steps, reward -200.00\n",
      "121 Episode in 24200 steps, reward -200.00\n",
      "122 Episode in 24400 steps, reward -200.00\n",
      "123 Episode in 24600 steps, reward -200.00\n",
      "124 Episode in 24800 steps, reward -200.00\n",
      "125 Episode in 25000 steps, reward -200.00\n",
      "126 Episode in 25200 steps, reward -200.00\n",
      "127 Episode in 25400 steps, reward -200.00\n",
      "128 Episode in 25600 steps, reward -200.00\n",
      "129 Episode in 25800 steps, reward -200.00\n",
      "130 Episode in 26000 steps, reward -200.00\n",
      "131 Episode in 26200 steps, reward -200.00\n",
      "132 Episode in 26400 steps, reward -200.00\n",
      "133 Episode in 26600 steps, reward -200.00\n",
      "134 Episode in 26800 steps, reward -200.00\n",
      "135 Episode in 27000 steps, reward -200.00\n",
      "136 Episode in 27200 steps, reward -200.00\n",
      "137 Episode in 27400 steps, reward -200.00\n",
      "138 Episode in 27600 steps, reward -200.00\n",
      "139 Episode in 27800 steps, reward -200.00\n",
      "140 Episode in 28000 steps, reward -200.00\n",
      "141 Episode in 28200 steps, reward -200.00\n",
      "142 Episode in 28400 steps, reward -200.00\n",
      "143 Episode in 28600 steps, reward -200.00\n",
      "144 Episode in 28800 steps, reward -200.00\n",
      "145 Episode in 29000 steps, reward -200.00\n",
      "146 Episode in 29200 steps, reward -200.00\n",
      "147 Episode in 29400 steps, reward -200.00\n",
      "148 Episode in 29600 steps, reward -200.00\n",
      "149 Episode in 29800 steps, reward -200.00\n",
      "150 Episode in 30000 steps, reward -200.00\n",
      "151 Episode in 30200 steps, reward -200.00\n",
      "152 Episode in 30400 steps, reward -200.00\n",
      "153 Episode in 30600 steps, reward -200.00\n",
      "154 Episode in 30800 steps, reward -200.00\n",
      "155 Episode in 31000 steps, reward -200.00\n",
      "156 Episode in 31200 steps, reward -200.00\n",
      "157 Episode in 31400 steps, reward -200.00\n",
      "158 Episode in 31600 steps, reward -200.00\n",
      "159 Episode in 31800 steps, reward -200.00\n",
      "160 Episode in 32000 steps, reward -200.00\n",
      "161 Episode in 32200 steps, reward -200.00\n",
      "162 Episode in 32400 steps, reward -200.00\n",
      "163 Episode in 32600 steps, reward -200.00\n",
      "164 Episode in 32800 steps, reward -200.00\n",
      "165 Episode in 33000 steps, reward -200.00\n",
      "166 Episode in 33200 steps, reward -200.00\n",
      "167 Episode in 33400 steps, reward -200.00\n",
      "168 Episode in 33600 steps, reward -200.00\n",
      "169 Episode in 33800 steps, reward -200.00\n",
      "170 Episode in 34000 steps, reward -200.00\n",
      "171 Episode in 34200 steps, reward -200.00\n",
      "172 Episode in 34400 steps, reward -200.00\n",
      "173 Episode in 34600 steps, reward -200.00\n",
      "174 Episode in 34800 steps, reward -200.00\n",
      "175 Episode in 35000 steps, reward -200.00\n",
      "176 Episode in 35200 steps, reward -200.00\n",
      "177 Episode in 35400 steps, reward -200.00\n",
      "178 Episode in 35600 steps, reward -200.00\n",
      "179 Episode in 35800 steps, reward -200.00\n",
      "180 Episode in 36000 steps, reward -200.00\n",
      "181 Episode in 36200 steps, reward -200.00\n",
      "182 Episode in 36400 steps, reward -200.00\n",
      "183 Episode in 36600 steps, reward -200.00\n",
      "184 Episode in 36800 steps, reward -200.00\n",
      "185 Episode in 37000 steps, reward -200.00\n",
      "186 Episode in 37200 steps, reward -200.00\n",
      "187 Episode in 37400 steps, reward -200.00\n",
      "188 Episode in 37600 steps, reward -200.00\n",
      "189 Episode in 37800 steps, reward -200.00\n",
      "190 Episode in 38000 steps, reward -200.00\n",
      "191 Episode in 38200 steps, reward -200.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "192 Episode in 38400 steps, reward -200.00\n",
      "193 Episode in 38600 steps, reward -200.00\n",
      "194 Episode in 38800 steps, reward -200.00\n",
      "195 Episode in 39000 steps, reward -200.00\n",
      "196 Episode in 39200 steps, reward -200.00\n",
      "197 Episode in 39400 steps, reward -200.00\n",
      "198 Episode in 39600 steps, reward -200.00\n",
      "199 Episode in 39800 steps, reward -200.00\n",
      "200 Episode in 40000 steps, reward -200.00\n",
      "201 Episode in 40200 steps, reward -200.00\n",
      "202 Episode in 40400 steps, reward -200.00\n",
      "203 Episode in 40600 steps, reward -200.00\n",
      "204 Episode in 40800 steps, reward -200.00\n",
      "205 Episode in 41000 steps, reward -200.00\n",
      "206 Episode in 41200 steps, reward -200.00\n",
      "207 Episode in 41400 steps, reward -200.00\n",
      "208 Episode in 41600 steps, reward -200.00\n",
      "209 Episode in 41800 steps, reward -200.00\n",
      "210 Episode in 42000 steps, reward -200.00\n",
      "211 Episode in 42200 steps, reward -200.00\n",
      "212 Episode in 42400 steps, reward -200.00\n",
      "213 Episode in 42600 steps, reward -200.00\n",
      "214 Episode in 42800 steps, reward -200.00\n",
      "215 Episode in 43000 steps, reward -200.00\n",
      "216 Episode in 43200 steps, reward -200.00\n",
      "217 Episode in 43400 steps, reward -200.00\n",
      "218 Episode in 43600 steps, reward -200.00\n",
      "219 Episode in 43800 steps, reward -200.00\n",
      "220 Episode in 44000 steps, reward -200.00\n",
      "221 Episode in 44200 steps, reward -200.00\n",
      "222 Episode in 44400 steps, reward -200.00\n",
      "223 Episode in 44600 steps, reward -200.00\n",
      "224 Episode in 44800 steps, reward -200.00\n",
      "225 Episode in 45000 steps, reward -200.00\n",
      "226 Episode in 45200 steps, reward -200.00\n",
      "227 Episode in 45400 steps, reward -200.00\n",
      "228 Episode in 45600 steps, reward -200.00\n",
      "229 Episode in 45800 steps, reward -200.00\n",
      "230 Episode in 46000 steps, reward -200.00\n",
      "231 Episode in 46200 steps, reward -200.00\n",
      "232 Episode in 46400 steps, reward -200.00\n",
      "233 Episode in 46600 steps, reward -200.00\n",
      "234 Episode in 46800 steps, reward -200.00\n",
      "235 Episode in 47000 steps, reward -200.00\n",
      "236 Episode in 47200 steps, reward -200.00\n",
      "237 Episode in 47400 steps, reward -200.00\n",
      "238 Episode in 47600 steps, reward -200.00\n",
      "239 Episode in 47800 steps, reward -200.00\n",
      "240 Episode in 48000 steps, reward -200.00\n",
      "241 Episode in 48200 steps, reward -200.00\n",
      "242 Episode in 48400 steps, reward -200.00\n",
      "243 Episode in 48600 steps, reward -200.00\n",
      "244 Episode in 48800 steps, reward -200.00\n",
      "245 Episode in 49000 steps, reward -200.00\n",
      "246 Episode in 49200 steps, reward -200.00\n",
      "247 Episode in 49400 steps, reward -200.00\n",
      "248 Episode in 49600 steps, reward -200.00\n",
      "249 Episode in 49800 steps, reward -200.00\n",
      "250 Episode in 50000 steps, reward -200.00\n",
      "251 Episode in 50200 steps, reward -200.00\n",
      "252 Episode in 50400 steps, reward -200.00\n",
      "253 Episode in 50600 steps, reward -200.00\n",
      "254 Episode in 50800 steps, reward -200.00\n",
      "255 Episode in 51000 steps, reward -200.00\n",
      "256 Episode in 51200 steps, reward -200.00\n",
      "257 Episode in 51400 steps, reward -200.00\n",
      "258 Episode in 51600 steps, reward -200.00\n",
      "259 Episode in 51800 steps, reward -200.00\n",
      "260 Episode in 52000 steps, reward -200.00\n",
      "261 Episode in 52200 steps, reward -200.00\n",
      "262 Episode in 52400 steps, reward -200.00\n",
      "263 Episode in 52600 steps, reward -200.00\n",
      "264 Episode in 52800 steps, reward -200.00\n",
      "265 Episode in 53000 steps, reward -200.00\n",
      "266 Episode in 53200 steps, reward -200.00\n",
      "267 Episode in 53400 steps, reward -200.00\n",
      "268 Episode in 53600 steps, reward -200.00\n",
      "269 Episode in 53800 steps, reward -200.00\n",
      "270 Episode in 54000 steps, reward -200.00\n",
      "271 Episode in 54200 steps, reward -200.00\n",
      "272 Episode in 54400 steps, reward -200.00\n",
      "273 Episode in 54600 steps, reward -200.00\n",
      "274 Episode in 54800 steps, reward -200.00\n",
      "275 Episode in 55000 steps, reward -200.00\n",
      "276 Episode in 55200 steps, reward -200.00\n",
      "277 Episode in 55400 steps, reward -200.00\n",
      "278 Episode in 55600 steps, reward -200.00\n",
      "279 Episode in 55800 steps, reward -200.00\n",
      "280 Episode in 56000 steps, reward -200.00\n",
      "281 Episode in 56200 steps, reward -200.00\n",
      "282 Episode in 56400 steps, reward -200.00\n",
      "283 Episode in 56600 steps, reward -200.00\n",
      "284 Episode in 56800 steps, reward -200.00\n",
      "285 Episode in 57000 steps, reward -200.00\n",
      "286 Episode in 57200 steps, reward -200.00\n",
      "287 Episode in 57400 steps, reward -200.00\n",
      "288 Episode in 57600 steps, reward -200.00\n",
      "289 Episode in 57800 steps, reward -200.00\n",
      "290 Episode in 58000 steps, reward -200.00\n",
      "291 Episode in 58200 steps, reward -200.00\n",
      "292 Episode in 58400 steps, reward -200.00\n",
      "293 Episode in 58600 steps, reward -200.00\n",
      "294 Episode in 58800 steps, reward -200.00\n",
      "295 Episode in 59000 steps, reward -200.00\n",
      "296 Episode in 59200 steps, reward -200.00\n",
      "297 Episode in 59400 steps, reward -200.00\n",
      "298 Episode in 59600 steps, reward -200.00\n",
      "299 Episode in 59800 steps, reward -200.00\n",
      "300 Episode in 60000 steps, reward -200.00\n",
      "301 Episode in 60200 steps, reward -200.00\n",
      "302 Episode in 60400 steps, reward -200.00\n",
      "303 Episode in 60600 steps, reward -200.00\n",
      "304 Episode in 60800 steps, reward -200.00\n",
      "305 Episode in 61000 steps, reward -200.00\n",
      "306 Episode in 61200 steps, reward -200.00\n",
      "307 Episode in 61400 steps, reward -200.00\n",
      "308 Episode in 61600 steps, reward -200.00\n",
      "309 Episode in 61800 steps, reward -200.00\n",
      "310 Episode in 62000 steps, reward -200.00\n",
      "311 Episode in 62200 steps, reward -200.00\n",
      "312 Episode in 62400 steps, reward -200.00\n",
      "313 Episode in 62600 steps, reward -200.00\n",
      "314 Episode in 62800 steps, reward -200.00\n",
      "315 Episode in 63000 steps, reward -200.00\n",
      "316 Episode in 63200 steps, reward -200.00\n",
      "317 Episode in 63400 steps, reward -200.00\n",
      "318 Episode in 63600 steps, reward -200.00\n",
      "319 Episode in 63800 steps, reward -200.00\n",
      "320 Episode in 64000 steps, reward -200.00\n",
      "321 Episode in 64200 steps, reward -200.00\n",
      "322 Episode in 64400 steps, reward -200.00\n",
      "323 Episode in 64600 steps, reward -200.00\n",
      "324 Episode in 64800 steps, reward -200.00\n",
      "325 Episode in 65000 steps, reward -200.00\n",
      "326 Episode in 65200 steps, reward -200.00\n",
      "327 Episode in 65400 steps, reward -200.00\n",
      "328 Episode in 65600 steps, reward -200.00\n",
      "329 Episode in 65800 steps, reward -200.00\n",
      "330 Episode in 66000 steps, reward -200.00\n",
      "331 Episode in 66200 steps, reward -200.00\n",
      "332 Episode in 66400 steps, reward -200.00\n",
      "333 Episode in 66600 steps, reward -200.00\n",
      "334 Episode in 66800 steps, reward -200.00\n",
      "335 Episode in 67000 steps, reward -200.00\n",
      "336 Episode in 67200 steps, reward -200.00\n",
      "337 Episode in 67400 steps, reward -200.00\n",
      "338 Episode in 67600 steps, reward -200.00\n",
      "339 Episode in 67800 steps, reward -200.00\n",
      "340 Episode in 68000 steps, reward -200.00\n",
      "341 Episode in 68200 steps, reward -200.00\n",
      "342 Episode in 68400 steps, reward -200.00\n",
      "343 Episode in 68600 steps, reward -200.00\n",
      "344 Episode in 68800 steps, reward -200.00\n",
      "345 Episode in 69000 steps, reward -200.00\n",
      "346 Episode in 69200 steps, reward -200.00\n",
      "347 Episode in 69400 steps, reward -200.00\n",
      "348 Episode in 69600 steps, reward -200.00\n",
      "349 Episode in 69800 steps, reward -200.00\n",
      "350 Episode in 70000 steps, reward -200.00\n",
      "351 Episode in 70200 steps, reward -200.00\n",
      "352 Episode in 70400 steps, reward -200.00\n",
      "353 Episode in 70600 steps, reward -200.00\n",
      "354 Episode in 70800 steps, reward -200.00\n",
      "355 Episode in 71000 steps, reward -200.00\n",
      "356 Episode in 71200 steps, reward -200.00\n",
      "357 Episode in 71400 steps, reward -200.00\n",
      "358 Episode in 71600 steps, reward -200.00\n",
      "359 Episode in 71800 steps, reward -200.00\n",
      "360 Episode in 72000 steps, reward -200.00\n",
      "361 Episode in 72200 steps, reward -200.00\n",
      "362 Episode in 72400 steps, reward -200.00\n",
      "363 Episode in 72600 steps, reward -200.00\n",
      "364 Episode in 72800 steps, reward -200.00\n",
      "365 Episode in 73000 steps, reward -200.00\n",
      "366 Episode in 73200 steps, reward -200.00\n",
      "367 Episode in 73400 steps, reward -200.00\n",
      "368 Episode in 73600 steps, reward -200.00\n",
      "369 Episode in 73800 steps, reward -200.00\n",
      "370 Episode in 74000 steps, reward -200.00\n",
      "371 Episode in 74200 steps, reward -200.00\n",
      "372 Episode in 74400 steps, reward -200.00\n",
      "373 Episode in 74600 steps, reward -200.00\n",
      "374 Episode in 74800 steps, reward -200.00\n",
      "375 Episode in 75000 steps, reward -200.00\n",
      "376 Episode in 75200 steps, reward -200.00\n",
      "377 Episode in 75400 steps, reward -200.00\n",
      "378 Episode in 75600 steps, reward -200.00\n",
      "379 Episode in 75800 steps, reward -200.00\n",
      "380 Episode in 76000 steps, reward -200.00\n",
      "381 Episode in 76200 steps, reward -200.00\n",
      "382 Episode in 76400 steps, reward -200.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "383 Episode in 76600 steps, reward -200.00\n",
      "384 Episode in 76800 steps, reward -200.00\n",
      "385 Episode in 77000 steps, reward -200.00\n",
      "386 Episode in 77200 steps, reward -200.00\n",
      "387 Episode in 77400 steps, reward -200.00\n",
      "388 Episode in 77600 steps, reward -200.00\n",
      "389 Episode in 77800 steps, reward -200.00\n",
      "390 Episode in 78000 steps, reward -200.00\n",
      "391 Episode in 78200 steps, reward -200.00\n",
      "392 Episode in 78400 steps, reward -200.00\n",
      "393 Episode in 78600 steps, reward -200.00\n",
      "394 Episode in 78800 steps, reward -200.00\n",
      "395 Episode in 79000 steps, reward -200.00\n",
      "396 Episode in 79200 steps, reward -200.00\n",
      "397 Episode in 79400 steps, reward -200.00\n",
      "398 Episode in 79600 steps, reward -200.00\n",
      "399 Episode in 79800 steps, reward -200.00\n",
      "400 Episode in 80000 steps, reward -200.00\n",
      "401 Episode in 80200 steps, reward -200.00\n",
      "402 Episode in 80400 steps, reward -200.00\n",
      "403 Episode in 80600 steps, reward -200.00\n",
      "404 Episode in 80800 steps, reward -200.00\n",
      "405 Episode in 81000 steps, reward -200.00\n",
      "406 Episode in 81200 steps, reward -200.00\n",
      "407 Episode in 81400 steps, reward -200.00\n",
      "408 Episode in 81600 steps, reward -200.00\n",
      "409 Episode in 81800 steps, reward -200.00\n",
      "410 Episode in 82000 steps, reward -200.00\n",
      "411 Episode in 82200 steps, reward -200.00\n",
      "412 Episode in 82400 steps, reward -200.00\n",
      "413 Episode in 82600 steps, reward -200.00\n",
      "414 Episode in 82800 steps, reward -200.00\n",
      "415 Episode in 83000 steps, reward -200.00\n",
      "416 Episode in 83200 steps, reward -200.00\n",
      "417 Episode in 83400 steps, reward -200.00\n",
      "418 Episode in 83600 steps, reward -200.00\n",
      "419 Episode in 83800 steps, reward -200.00\n",
      "420 Episode in 84000 steps, reward -200.00\n",
      "421 Episode in 84200 steps, reward -200.00\n",
      "422 Episode in 84400 steps, reward -200.00\n",
      "423 Episode in 84600 steps, reward -200.00\n",
      "424 Episode in 84800 steps, reward -200.00\n"
     ]
    }
   ],
   "source": [
    "# play\n",
    "for i in range(1, n_episodes + 1):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    ep_reward = 0\n",
    "    while not done:\n",
    "        env.render()\n",
    "\n",
    "        action, value = get_action_and_value(obs, net)\n",
    "        _obs, reward, done, _ = env.step(action)\n",
    "        \n",
    "        # store\n",
    "        roll_memory.append([obs, action, _obs])\n",
    "        rewards.append(reward)\n",
    "        values.append(value)\n",
    "        \n",
    "        obs = _obs\n",
    "        steps += 1\n",
    "        ep_reward += reward\n",
    "    \n",
    "    if done:\n",
    "        values.append(0.)\n",
    "        train_memory.extend(compute_adv_with_gae(rewards, values, roll_memory))\n",
    "        rewards.clear()\n",
    "        values.clear()\n",
    "        roll_memory.clear()\n",
    "        \n",
    "        if i % n_roll_ep == 0:\n",
    "            net_memory.appendleft(net.state_dict())\n",
    "            old_net.load_state_dict(net_memory.pop())\n",
    "            learn(net, old_net, optimizer, train_memory)\n",
    "            train_memory.clear()\n",
    "            learn_steps += 1\n",
    "        \n",
    "        ep_rewards.append(ep_reward)\n",
    "        print('{:3} Episode in {:5} steps, reward {:.2f}'.format(\n",
    "            i, steps, ep_reward))\n",
    "\n",
    "        if len(ep_rewards) >= n_eval:\n",
    "            if np.mean(list(reversed(ep_rewards))[: n_eval]) >= env.spec.reward_threshold:\n",
    "                print('\\n{} is sloved! {:3} Episode in {:3} steps'.format(\n",
    "                    env.spec.id, i, steps))\n",
    "                torch.save(old_net.state_dict(),\n",
    "                           f'./test/saved_models/{env.spec.id}_ep{i}_clear_model_ppo_ep.pt')\n",
    "                break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "plt.title('Reward')\n",
    "plt.plot(ep_rewards)\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.title('Loss')\n",
    "plt.plot(losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[\n",
    "    ('CartPole-v0', 245, 4, 5),\n",
    "    ('CartPole-v1', 319, 4, 5),\n",
    "    ('MountainCar-v0', None),\n",
    "    ('LunarLander-v2', None)\n",
    "]"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "C51_tensorflow.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
