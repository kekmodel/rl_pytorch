{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IWnm3qot3o1W"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import deque\n",
    "from copy import deepcopy\n",
    "\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 708,
     "status": "ok",
     "timestamp": 1534482400648,
     "user": {
      "displayName": "윤승제",
      "photoUrl": "//lh5.googleusercontent.com/-EucKC7DmcQI/AAAAAAAAAAI/AAAAAAAAAGA/gQU1NPEmNFA/s50-c-k-no/photo.jpg",
      "userId": "105654037995838004821"
     },
     "user_tz": -540
    },
    "id": "maRVADiTlzHD",
    "outputId": "783b7610-95c2-4b54-b2ce-d8e853c484ba"
   },
   "outputs": [],
   "source": [
    "SEED = 1\n",
    "BATCH_SIZE = 256\n",
    "LR = 0.0003\n",
    "UP_COEF = 0.1\n",
    "GAMMA = 0.99\n",
    "EPS = np.finfo(np.float32).eps\n",
    "V_MAX = 10\n",
    "V_MIN = -10\n",
    "N_ATOMS = 51\n",
    "DELTA_Z = (V_MAX - V_MIN) / (N_ATOMS - 1)\n",
    "\n",
    "# set device\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device('cuda' if use_cuda else 'cpu')\n",
    "\n",
    "# random seed\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if use_cuda:\n",
    "    torch.cuda.manual_seed_all(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9Ffkl_5C4R81"
   },
   "outputs": [],
   "source": [
    "class CategoricalDQN(nn.Module):\n",
    "    def __init__(self, obs_space, action_space, n_atoms):\n",
    "        super().__init__()\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(obs_space, 256),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.SELU()\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(256, 256),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(256, action_space * n_atoms)\n",
    "        )\n",
    "\n",
    "        self.log_softmax = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "        self.register_buffer(\n",
    "            'support', torch.arange(V_MIN, V_MAX + DELTA_Z, DELTA_Z))\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.head(x)\n",
    "        out = self.fc(out).reshape(out.shape[0], -1, N_ATOMS)\n",
    "        out = self.log_softmax(out)\n",
    "        probs = out.exp()\n",
    "\n",
    "        return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "\n",
    "\n",
    "def learn(net, tgt_net, optimizer, rep_memory):\n",
    "    net.train()\n",
    "    tgt_net.train()\n",
    "\n",
    "    dataloader = DataLoader(rep_memory,\n",
    "                            batch_size=BATCH_SIZE,\n",
    "                            shuffle=True,\n",
    "                            pin_memory=use_cuda)\n",
    "\n",
    "    for i, (s, a, r, _s, d) in enumerate(dataloader):\n",
    "        if i > 0:\n",
    "            break\n",
    "        s_batch = s.to(device).float()\n",
    "        a_batch = a.detach().to(device).long()\n",
    "        _s_batch = _s.to(device).float()\n",
    "        r_batch = r.detach().to(device).float()\n",
    "        is_done = 1. - d.detach().to(device).float()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            _p_batch_tgt = tgt_net(_s_batch)\n",
    "            _p_acting = _p_batch_tgt[range(BATCH_SIZE), a_batch]\n",
    "        _p_proj = projection(_p_acting, r_batch, is_done)\n",
    "\n",
    "        p_batch = net(s_batch)\n",
    "        p_acting = p_batch[range(BATCH_SIZE), a_batch.data]\n",
    "\n",
    "        # loss\n",
    "        loss = -(_p_proj * torch.clamp(p_acting, min=EPS).log()).sum(dim=1).mean()\n",
    "        losses.append(loss)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "def projection(_p_acting, r_batch, is_done):\n",
    "    _p_proj = np.zeros((BATCH_SIZE, N_ATOMS), dtype=np.float32)\n",
    "    r_batch_np = r_batch.cpu().numpy()\n",
    "    is_done_np = is_done.cpu().numpy()\n",
    "    _p_acting_np = _p_acting.detach().cpu().numpy()\n",
    "    batch_id = range(BATCH_SIZE)\n",
    "    for i in range(N_ATOMS):\n",
    "        z = np.clip(r_batch_np + GAMMA * (V_MIN + i * DELTA_Z) * is_done_np,\n",
    "                    V_MIN, V_MAX)\n",
    "        b = (z - V_MIN) / DELTA_Z\n",
    "        l = np.floor(b).astype(np.int64)\n",
    "        u = np.ceil(b).astype(np.int64)\n",
    "        _p_proj[batch_id, l[batch_id]] += _p_acting_np[batch_id, i] * (u - b)[batch_id]\n",
    "        _p_proj[batch_id, u[batch_id]] += _p_acting_np[batch_id, i] * (b - l)[batch_id]\n",
    "\n",
    "#     _p_proj += EPS\n",
    "#         _p_proj = np.clip(_p_proj, EPS, None)\n",
    "#     _p_proj = _p_proj / _p_proj.sum(axis=1, keepdims=1)\n",
    "    \n",
    "    return torch.tensor(_p_proj).to(device).float()\n",
    "\n",
    "\n",
    "def select_action(obs, tgt_net):\n",
    "    tgt_net.eval()\n",
    "    with torch.no_grad():\n",
    "        state = torch.tensor([obs]).to(device).float()\n",
    "        probs = target_net(state)\n",
    "        weights = probs * net.support\n",
    "        q = weights.sum(dim=2)\n",
    "        action = torch.argmax(q, dim=1)\n",
    "\n",
    "    return action.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 3377
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 135196,
     "status": "ok",
     "timestamp": 1534482559393,
     "user": {
      "displayName": "윤승제",
      "photoUrl": "//lh5.googleusercontent.com/-EucKC7DmcQI/AAAAAAAAAAI/AAAAAAAAAGA/gQU1NPEmNFA/s50-c-k-no/photo.jpg",
      "userId": "105654037995838004821"
     },
     "user_tz": -540
    },
    "id": "PnifSBJglzHh",
    "outputId": "94177345-918e-4a96-d9a8-d8aba0a4bc9a",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jay/anaconda3/lib/python3.6/site-packages/gym/__init__.py:22: UserWarning: DEPRECATION WARNING: to improve load times, gym no longer automatically loads gym.spaces. Please run \"import gym.spaces\" to load gym.spaces on your own. This warning will turn into an error in a future version of gym.\n",
      "  warnings.warn('DEPRECATION WARNING: to improve load times, gym no longer automatically loads gym.spaces. Please run \"import gym.spaces\" to load gym.spaces on your own. This warning will turn into an error in a future version of gym.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# make an environment\n",
    "# env = gym.make('CartPole-v0')\n",
    "env = gym.make('CartPole-v1')\n",
    "# env = gym.make('MountainCar-v0')\n",
    "# env = gym.make('LunarLander-v2')\n",
    "\n",
    "env.seed(SEED)\n",
    "obs_space = env.observation_space.shape[0]\n",
    "action_space = env.action_space.n\n",
    "\n",
    "# hyperparameter\n",
    "n_episodes = 1000\n",
    "learn_start = 1500\n",
    "memory_size = 50000\n",
    "update_frq = 1\n",
    "use_eps_decay = False\n",
    "epsilon = 0.001\n",
    "eps_min = 0.001\n",
    "decay_rate = 0.0001\n",
    "n_eval = env.spec.trials\n",
    "\n",
    "# global values\n",
    "total_steps = 0\n",
    "learn_steps = 0\n",
    "rewards = []\n",
    "reward_eval = deque(maxlen=n_eval)\n",
    "is_learned = False\n",
    "is_solved = False\n",
    "\n",
    "# make two nerual networks\n",
    "net = CategoricalDQN(obs_space, action_space, N_ATOMS).to(device)\n",
    "target_net = deepcopy(net)\n",
    "\n",
    "# make a optimizer\n",
    "optimizer = optim.Adam(net.parameters(), lr=LR, eps=EPS)\n",
    "\n",
    "# make memory\n",
    "rep_memory = deque(maxlen=memory_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "use_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.spec.max_episode_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.spec.trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "475.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.spec.reward_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 3377
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 135196,
     "status": "ok",
     "timestamp": 1534482559393,
     "user": {
      "displayName": "윤승제",
      "photoUrl": "//lh5.googleusercontent.com/-EucKC7DmcQI/AAAAAAAAAAI/AAAAAAAAAGA/gQU1NPEmNFA/s50-c-k-no/photo.jpg",
      "userId": "105654037995838004821"
     },
     "user_tz": -540
    },
    "id": "PnifSBJglzHh",
    "outputId": "94177345-918e-4a96-d9a8-d8aba0a4bc9a",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1 Episode in    48 steps, reward 48.00\n",
      "  2 Episode in    89 steps, reward 41.00\n",
      "  3 Episode in   164 steps, reward 75.00\n",
      "  4 Episode in   203 steps, reward 39.00\n",
      "  5 Episode in   278 steps, reward 75.00\n",
      "  6 Episode in   356 steps, reward 78.00\n",
      "  7 Episode in   462 steps, reward 106.00\n",
      "  8 Episode in   531 steps, reward 69.00\n",
      "  9 Episode in   616 steps, reward 85.00\n",
      " 10 Episode in   694 steps, reward 78.00\n",
      " 11 Episode in   733 steps, reward 39.00\n",
      " 12 Episode in   773 steps, reward 40.00\n",
      " 13 Episode in   877 steps, reward 104.00\n",
      " 14 Episode in   930 steps, reward 53.00\n",
      " 15 Episode in  1015 steps, reward 85.00\n",
      " 16 Episode in  1084 steps, reward 69.00\n",
      " 17 Episode in  1172 steps, reward 88.00\n",
      " 18 Episode in  1210 steps, reward 38.00\n",
      " 19 Episode in  1252 steps, reward 42.00\n",
      " 20 Episode in  1352 steps, reward 100.00\n",
      " 21 Episode in  1406 steps, reward 54.00\n",
      " 22 Episode in  1444 steps, reward 38.00\n",
      " 23 Episode in  1493 steps, reward 49.00\n",
      "\n",
      "============  Start Learning  ============\n",
      "\n",
      " 24 Episode in  1547 steps, reward 54.00\n",
      " 25 Episode in  1558 steps, reward 11.00\n",
      " 26 Episode in  1567 steps, reward 9.00\n",
      " 27 Episode in  1576 steps, reward 9.00\n",
      " 28 Episode in  1585 steps, reward 9.00\n",
      " 29 Episode in  1595 steps, reward 10.00\n",
      " 30 Episode in  1604 steps, reward 9.00\n",
      " 31 Episode in  1613 steps, reward 9.00\n",
      " 32 Episode in  1623 steps, reward 10.00\n",
      " 33 Episode in  1632 steps, reward 9.00\n",
      " 34 Episode in  1640 steps, reward 8.00\n",
      " 35 Episode in  1656 steps, reward 16.00\n",
      " 36 Episode in  1668 steps, reward 12.00\n",
      " 37 Episode in  1678 steps, reward 10.00\n",
      " 38 Episode in  1687 steps, reward 9.00\n",
      " 39 Episode in  1696 steps, reward 9.00\n",
      " 40 Episode in  1705 steps, reward 9.00\n",
      " 41 Episode in  1715 steps, reward 10.00\n",
      " 42 Episode in  1724 steps, reward 9.00\n",
      " 43 Episode in  1736 steps, reward 12.00\n",
      " 44 Episode in  1752 steps, reward 16.00\n",
      " 45 Episode in  1782 steps, reward 30.00\n",
      " 46 Episode in  1804 steps, reward 22.00\n",
      " 47 Episode in  1830 steps, reward 26.00\n",
      " 48 Episode in  1856 steps, reward 26.00\n",
      " 49 Episode in  1885 steps, reward 29.00\n",
      " 50 Episode in  2010 steps, reward 125.00\n",
      " 51 Episode in  2216 steps, reward 206.00\n",
      " 52 Episode in  2392 steps, reward 176.00\n",
      " 53 Episode in  2892 steps, reward 500.00\n",
      " 54 Episode in  3137 steps, reward 245.00\n",
      " 55 Episode in  3512 steps, reward 375.00\n",
      " 56 Episode in  3722 steps, reward 210.00\n",
      " 57 Episode in  3945 steps, reward 223.00\n",
      " 58 Episode in  4343 steps, reward 398.00\n",
      " 59 Episode in  4571 steps, reward 228.00\n",
      " 60 Episode in  4786 steps, reward 215.00\n",
      " 61 Episode in  5138 steps, reward 352.00\n",
      " 62 Episode in  5416 steps, reward 278.00\n",
      " 63 Episode in  5660 steps, reward 244.00\n",
      " 64 Episode in  6160 steps, reward 500.00\n",
      " 65 Episode in  6437 steps, reward 277.00\n",
      " 66 Episode in  6819 steps, reward 382.00\n",
      " 67 Episode in  7149 steps, reward 330.00\n",
      " 68 Episode in  7430 steps, reward 281.00\n",
      " 69 Episode in  7687 steps, reward 257.00\n",
      " 70 Episode in  7870 steps, reward 183.00\n",
      " 71 Episode in  8077 steps, reward 207.00\n",
      " 72 Episode in  8382 steps, reward 305.00\n",
      " 73 Episode in  8603 steps, reward 221.00\n",
      " 74 Episode in  9074 steps, reward 471.00\n",
      " 75 Episode in  9343 steps, reward 269.00\n",
      " 76 Episode in  9651 steps, reward 308.00\n",
      " 77 Episode in  9939 steps, reward 288.00\n",
      " 78 Episode in 10310 steps, reward 371.00\n",
      " 79 Episode in 10696 steps, reward 386.00\n",
      " 80 Episode in 10904 steps, reward 208.00\n",
      " 81 Episode in 11404 steps, reward 500.00\n",
      " 82 Episode in 11714 steps, reward 310.00\n",
      " 83 Episode in 12029 steps, reward 315.00\n",
      " 84 Episode in 12331 steps, reward 302.00\n",
      " 85 Episode in 12619 steps, reward 288.00\n",
      " 86 Episode in 12946 steps, reward 327.00\n",
      " 87 Episode in 13204 steps, reward 258.00\n",
      " 88 Episode in 13506 steps, reward 302.00\n",
      " 89 Episode in 13953 steps, reward 447.00\n",
      " 90 Episode in 14256 steps, reward 303.00\n",
      " 91 Episode in 14756 steps, reward 500.00\n",
      " 92 Episode in 15154 steps, reward 398.00\n",
      " 93 Episode in 15456 steps, reward 302.00\n",
      " 94 Episode in 15784 steps, reward 328.00\n",
      " 95 Episode in 16002 steps, reward 218.00\n",
      " 96 Episode in 16502 steps, reward 500.00\n",
      " 97 Episode in 16792 steps, reward 290.00\n",
      " 98 Episode in 17107 steps, reward 315.00\n",
      " 99 Episode in 17428 steps, reward 321.00\n",
      "100 Episode in 17631 steps, reward 203.00\n",
      "101 Episode in 18054 steps, reward 423.00\n",
      "102 Episode in 18423 steps, reward 369.00\n",
      "103 Episode in 18717 steps, reward 294.00\n",
      "104 Episode in 19217 steps, reward 500.00\n",
      "105 Episode in 19519 steps, reward 302.00\n",
      "106 Episode in 19797 steps, reward 278.00\n",
      "107 Episode in 20297 steps, reward 500.00\n",
      "108 Episode in 20623 steps, reward 326.00\n",
      "109 Episode in 21042 steps, reward 419.00\n",
      "110 Episode in 21355 steps, reward 313.00\n",
      "111 Episode in 21693 steps, reward 338.00\n",
      "112 Episode in 22051 steps, reward 358.00\n",
      "113 Episode in 22551 steps, reward 500.00\n",
      "114 Episode in 22801 steps, reward 250.00\n",
      "115 Episode in 23301 steps, reward 500.00\n",
      "116 Episode in 23557 steps, reward 256.00\n",
      "117 Episode in 23821 steps, reward 264.00\n",
      "118 Episode in 24166 steps, reward 345.00\n",
      "119 Episode in 24487 steps, reward 321.00\n",
      "120 Episode in 24987 steps, reward 500.00\n",
      "121 Episode in 25192 steps, reward 205.00\n",
      "122 Episode in 25584 steps, reward 392.00\n",
      "123 Episode in 25968 steps, reward 384.00\n",
      "124 Episode in 26209 steps, reward 241.00\n",
      "125 Episode in 26601 steps, reward 392.00\n",
      "126 Episode in 26797 steps, reward 196.00\n",
      "127 Episode in 27008 steps, reward 211.00\n",
      "128 Episode in 27210 steps, reward 202.00\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-78747005115f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrep_memory\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlearn_start\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n============  Start Learning  ============\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m             \u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_net\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrep_memory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m             \u001b[0mlearn_steps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-0ba784707f73>\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(net, tgt_net, optimizer, rep_memory)\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0m_p_batch_tgt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtgt_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_s_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0m_p_acting\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_p_batch_tgt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_batch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0m_p_proj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprojection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_p_acting\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_done\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mp_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-0ba784707f73>\u001b[0m in \u001b[0;36mprojection\u001b[0;34m(_p_acting, r_batch, is_done)\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0mbatch_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN_ATOMS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         z = np.clip(r_batch_np + GAMMA * (V_MIN + i * DELTA_Z) * is_done_np,\n\u001b[0m\u001b[1;32m     47\u001b[0m                     V_MIN, V_MAX)\n\u001b[1;32m     48\u001b[0m         \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mV_MIN\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mDELTA_Z\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# play\n",
    "for i in range(1, n_episodes + 1):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    ep_reward = 0\n",
    "    while not done:\n",
    "#         env.render()\n",
    "        if np.random.rand() < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = select_action(obs, target_net)\n",
    "\n",
    "        _obs, reward, done, _ = env.step(action)\n",
    "\n",
    "        rep_memory.append((obs, action, reward, _obs, done))\n",
    "\n",
    "        obs = _obs\n",
    "        total_steps += 1\n",
    "        ep_reward += reward\n",
    "\n",
    "        if use_eps_decay:\n",
    "            epsilon -= epsilon * decay_rate\n",
    "            epsilon = max(eps_min, epsilon)\n",
    "\n",
    "        if len(rep_memory) >= learn_start:\n",
    "            if len(rep_memory) == learn_start:\n",
    "                print('\\n============  Start Learning  ============\\n')\n",
    "            learn(net, target_net, optimizer, rep_memory)\n",
    "            learn_steps += 1\n",
    "\n",
    "        if learn_steps == update_frq:\n",
    "            # target smoothing update\n",
    "            for t, n in zip(target_net.parameters(), net.parameters()):\n",
    "                t.data = UP_COEF * n.data + (1 - UP_COEF) * t.data\n",
    "            learn_steps = 0\n",
    "\n",
    "    if done:\n",
    "        rewards.append(ep_reward)\n",
    "        reward_eval.append(ep_reward)\n",
    "        print('{:3} Episode in {:5} steps, reward {:.2f}'.format(\n",
    "            i, total_steps, ep_reward))\n",
    "\n",
    "        if len(reward_eval) >= n_eval:\n",
    "            if np.mean(reward_eval) >= env.spec.reward_threshold:\n",
    "                print('\\n{} is sloved! {:3} Episode in {:3} steps'.format(\n",
    "                    env.spec.id, i, total_steps))\n",
    "                torch.save(target_net.state_dict(),\n",
    "                           f'./test/saved_models/{env.spec.id}_ep{i}_clear_model_cdddqn.pt')\n",
    "                break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "plt.title('Reward')\n",
    "plt.plot(rewards)\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.title('Loss')\n",
    "plt.plot(losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[\n",
    "    ('CartPole-v0', 215, 0.25),\n",
    "    ('CartPole-v1', 291, 0.1),\n",
    "    ('MountainCar-v0', None, 0.1),\n",
    "    ('LunarLander-v2', None, 0.1)\n",
    "]"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "C51_tensorflow.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
