{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IWnm3qot3o1W"
   },
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "from copy import deepcopy\n",
    "from pprint import pprint\n",
    "\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 708,
     "status": "ok",
     "timestamp": 1534482400648,
     "user": {
      "displayName": "윤승제",
      "photoUrl": "//lh5.googleusercontent.com/-EucKC7DmcQI/AAAAAAAAAAI/AAAAAAAAAGA/gQU1NPEmNFA/s50-c-k-no/photo.jpg",
      "userId": "105654037995838004821"
     },
     "user_tz": -540
    },
    "id": "maRVADiTlzHD",
    "outputId": "783b7610-95c2-4b54-b2ce-d8e853c484ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda: True\n"
     ]
    }
   ],
   "source": [
    "SEED = 1\n",
    "BATCH_SIZE = 4\n",
    "LR = 0.0001\n",
    "EPOCHS = 4\n",
    "CLIP = 0.1\n",
    "GAMMA = 0.99\n",
    "LAMBDA = 0.95\n",
    "ENT_COEF = 0.01\n",
    "\n",
    "# set device\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print('cuda:', use_cuda)\n",
    "device = torch.device('cuda' if use_cuda else 'cpu')\n",
    "\n",
    "# random seed\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if use_cuda:\n",
    "    torch.cuda.manual_seed_all(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9Ffkl_5C4R81"
   },
   "outputs": [],
   "source": [
    "class ActorCriticNet(nn.Module):\n",
    "    def __init__(self, obs_space, action_space):\n",
    "        super().__init__()\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(obs_space, 256),\n",
    "            nn.SELU()\n",
    "        )\n",
    "\n",
    "        self.pol = nn.Sequential(\n",
    "            nn.Linear(256, 256),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(256, action_space)\n",
    "        )\n",
    "        \n",
    "        self.val = nn.Sequential(\n",
    "            nn.Linear(256, 256),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "        \n",
    "        self.log_softmax = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.head(x)\n",
    "        logit = self.pol(out).reshape(out.shape[0], -1)\n",
    "        value = self.val(out).reshape(out.shape[0], 1)\n",
    "        log_probs = self.log_softmax(logit)\n",
    "        \n",
    "        return log_probs, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "\n",
    "\n",
    "def learn(net, old_net, optimizer, train_memory):\n",
    "    global action_space\n",
    "\n",
    "    net.train()\n",
    "    old_net.train()\n",
    "    dataloader = DataLoader(train_memory,\n",
    "                        batch_size=BATCH_SIZE,\n",
    "                        shuffle=True,\n",
    "                        pin_memory=use_cuda)\n",
    "    for i in range(EPOCHS):\n",
    "#         dataloader = DataLoader(train_memory,\n",
    "#                                 batch_size=BATCH_SIZE,\n",
    "#                                 shuffle=True,\n",
    "#                                 pin_memory=use_cuda)\n",
    "        for (s, a, r, _s, d, adv) in dataloader:\n",
    "            s_batch = s.to(device).float()\n",
    "            a_batch = a.detach().to(device).long()\n",
    "            _s_batch = _s.to(device).float()\n",
    "            r_batch = r.to(device).float()\n",
    "            adv_batch = adv.to(device).float()\n",
    "#             ret_batch = ret.to(device).float()\n",
    "            done_mask = 1. - d.to(device).float()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                log_p_batch_old, v_batch_old = old_net(s_batch)\n",
    "                log_p_acting_old = log_p_batch_old[range(BATCH_SIZE), a_batch]\n",
    "\n",
    "            log_p_batch, v_batch = net(s_batch)\n",
    "            _, _v_batch = net(_s_batch)\n",
    "            log_p_acting = log_p_batch[range(BATCH_SIZE), a_batch]\n",
    "            p_ratio = (log_p_acting - log_p_acting_old).exp()\n",
    "            p_ratio_clip = torch.clamp(p_ratio, 1. - CLIP, 1. + CLIP)\n",
    "            clip_p_loss = torch.min(p_ratio * adv_batch,\n",
    "                                  p_ratio_clip * adv_batch)\n",
    "            v_loss = (r_batch + GAMMA * done_mask * _v_batch - v_batch).pow(2)\n",
    "#             clip_v_loss = v_batch_old + torch.clamp(v_batch - v_batch_old, -CLIP, CLIP)\n",
    "#             v_loss_a = (clip_v_loss - ret_batch).pow(2)\n",
    "#             v_loss_b = (v_batch - ret_batch).pow(2)\n",
    "#             v_loss = torch.min(v_loss_a, v_loss_a) \n",
    "            entropy = -(log_p_batch.exp() * log_p_batch).sum(dim=1)\n",
    "\n",
    "            # loss\n",
    "            loss = -(clip_p_loss - v_loss + ENT_COEF * entropy).mean()\n",
    "            losses.append(loss)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "def get_action_and_value(obs, old_net):\n",
    "    old_net.eval()\n",
    "    with torch.no_grad():\n",
    "        state = torch.tensor([obs]).to(device).float()\n",
    "        log_p, v = old_net(state)\n",
    "        m = Categorical(log_p.exp())\n",
    "        action = m.sample()\n",
    "\n",
    "    return action.item(), v.item()\n",
    "\n",
    "\n",
    "def compute_adv(rewards, values, dones, roll_memory):\n",
    "    rew = np.array(rewards, 'float')\n",
    "    val = np.array(values[:-1], 'float')\n",
    "    _val = np.array(values[1:], 'float')\n",
    "    done_mask = 1. - np.array(dones, 'float')\n",
    "    delta = rew + GAMMA * done_mask * _val - val\n",
    "#     disc_v = delta + val\n",
    "    gae_dt = np.array(\n",
    "        [(GAMMA * LAMBDA)**(i) * dt for i, dt in enumerate(delta.tolist())],\n",
    "        'float')\n",
    "#     gae_dv = np.array(\n",
    "#         [(GAMMA * LAMBDA)**(i) * dv for i, dv in enumerate(disc_v.tolist())],\n",
    "#         'float')\n",
    "    for i, data in enumerate(roll_memory):\n",
    "        data.append(sum(gae_dt[i:] / (GAMMA * LAMBDA)**(i)))\n",
    "#         data.append(sum(gae_dv[i:] / (GAMMA * LAMBDA)**(i)))\n",
    "#     pprint(roll_memory)\n",
    "    return roll_memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 3377
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 135196,
     "status": "ok",
     "timestamp": 1534482559393,
     "user": {
      "displayName": "윤승제",
      "photoUrl": "//lh5.googleusercontent.com/-EucKC7DmcQI/AAAAAAAAAAI/AAAAAAAAAGA/gQU1NPEmNFA/s50-c-k-no/photo.jpg",
      "userId": "105654037995838004821"
     },
     "user_tz": -540
    },
    "id": "PnifSBJglzHh",
    "outputId": "94177345-918e-4a96-d9a8-d8aba0a4bc9a",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# make an environment\n",
    "# env = gym.make('CartPole-v0')\n",
    "# env = gym.make('CartPole-v1')\n",
    "# env = gym.make('MountainCar-v0')\n",
    "env = gym.make('LunarLander-v2')\n",
    "\n",
    "env.seed(SEED)\n",
    "obs_space = env.observation_space.shape[0]\n",
    "action_space = env.action_space.n\n",
    "\n",
    "# hyperparameter\n",
    "n_episodes = 3000\n",
    "roll_len = 128\n",
    "n_eval = env.spec.trials\n",
    "\n",
    "# global values\n",
    "steps = 0\n",
    "learn_steps = 0\n",
    "ep_rewards = []\n",
    "reward_eval = deque(maxlen=n_eval)\n",
    "is_rollout = False\n",
    "is_solved = False\n",
    "\n",
    "# make nerual networks\n",
    "net = ActorCriticNet(obs_space, action_space).to(device)\n",
    "old_net = deepcopy(net)\n",
    "\n",
    "# make a optimizer\n",
    "optimizer = optim.Adam(net.parameters(), lr=LR, eps=1e-5)\n",
    "\n",
    "# make a rollout memory\n",
    "train_memory = []\n",
    "roll_memory = []\n",
    "rewards = []\n",
    "values = []\n",
    "dones = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "use_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.spec.max_episode_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.spec.trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.spec.reward_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 3377
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 135196,
     "status": "ok",
     "timestamp": 1534482559393,
     "user": {
      "displayName": "윤승제",
      "photoUrl": "//lh5.googleusercontent.com/-EucKC7DmcQI/AAAAAAAAAAI/AAAAAAAAAGA/gQU1NPEmNFA/s50-c-k-no/photo.jpg",
      "userId": "105654037995838004821"
     },
     "user_tz": -540
    },
    "id": "PnifSBJglzHh",
    "outputId": "94177345-918e-4a96-d9a8-d8aba0a4bc9a",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1 Episode in    67 steps, reward -255.99\n",
      "  2 Episode in   142 steps, reward -107.45\n",
      "  3 Episode in   222 steps, reward -130.95\n",
      "  4 Episode in   311 steps, reward -435.51\n",
      "  5 Episode in   417 steps, reward -219.62\n",
      "  6 Episode in   514 steps, reward -103.82\n",
      "  7 Episode in   579 steps, reward -87.73\n",
      "  8 Episode in   679 steps, reward -155.95\n",
      "  9 Episode in   791 steps, reward -88.26\n",
      " 10 Episode in   857 steps, reward -172.01\n",
      " 11 Episode in   947 steps, reward -287.10\n",
      " 12 Episode in  1063 steps, reward -476.11\n",
      " 13 Episode in  1151 steps, reward -91.61\n",
      " 14 Episode in  1256 steps, reward -122.05\n",
      " 15 Episode in  1315 steps, reward -93.43\n",
      " 16 Episode in  1394 steps, reward -211.80\n",
      " 17 Episode in  1461 steps, reward -114.64\n",
      " 18 Episode in  1518 steps, reward -105.39\n",
      " 19 Episode in  1639 steps, reward -154.00\n",
      " 20 Episode in  1757 steps, reward -175.40\n",
      " 21 Episode in  1832 steps, reward -229.74\n",
      " 22 Episode in  1933 steps, reward -217.33\n",
      " 23 Episode in  2012 steps, reward -72.65\n",
      " 24 Episode in  2095 steps, reward -167.37\n",
      " 25 Episode in  2213 steps, reward -369.48\n",
      " 26 Episode in  2352 steps, reward -408.17\n",
      " 27 Episode in  2509 steps, reward -241.86\n",
      " 28 Episode in  2577 steps, reward -223.26\n",
      " 29 Episode in  2674 steps, reward -441.93\n",
      " 30 Episode in  2759 steps, reward -367.94\n",
      " 31 Episode in  2947 steps, reward -511.55\n",
      " 32 Episode in  3033 steps, reward -342.98\n",
      " 33 Episode in  3123 steps, reward -331.63\n",
      " 34 Episode in  3200 steps, reward -82.07\n",
      " 35 Episode in  3269 steps, reward -82.10\n",
      " 36 Episode in  3390 steps, reward -248.90\n",
      " 37 Episode in  3482 steps, reward -341.39\n",
      " 38 Episode in  3550 steps, reward -106.28\n",
      " 39 Episode in  3690 steps, reward -595.15\n",
      " 40 Episode in  3765 steps, reward -118.58\n",
      " 41 Episode in  3891 steps, reward -163.31\n",
      " 42 Episode in  3958 steps, reward -82.91\n",
      " 43 Episode in  4049 steps, reward -139.06\n",
      " 44 Episode in  4159 steps, reward -264.03\n",
      " 45 Episode in  4274 steps, reward -244.11\n",
      " 46 Episode in  4385 steps, reward -242.34\n",
      " 47 Episode in  4568 steps, reward -76.71\n",
      " 48 Episode in  4646 steps, reward -90.95\n",
      " 49 Episode in  4756 steps, reward -189.52\n",
      " 50 Episode in  4821 steps, reward -94.29\n",
      " 51 Episode in  4896 steps, reward -72.68\n",
      " 52 Episode in  4969 steps, reward -89.01\n",
      " 53 Episode in  5088 steps, reward -125.64\n",
      " 54 Episode in  5182 steps, reward -76.35\n",
      " 55 Episode in  5264 steps, reward -92.42\n",
      " 56 Episode in  5339 steps, reward -71.17\n",
      " 57 Episode in  5425 steps, reward -103.43\n",
      " 58 Episode in  5568 steps, reward -386.08\n",
      " 59 Episode in  5655 steps, reward -113.69\n",
      " 60 Episode in  5757 steps, reward -318.88\n",
      " 61 Episode in  5886 steps, reward -75.69\n",
      " 62 Episode in  5973 steps, reward -80.24\n",
      " 63 Episode in  6081 steps, reward -80.91\n",
      " 64 Episode in  6206 steps, reward -97.38\n",
      " 65 Episode in  6336 steps, reward -191.33\n",
      " 66 Episode in  6460 steps, reward -245.79\n",
      " 67 Episode in  6618 steps, reward -277.56\n",
      " 68 Episode in  6702 steps, reward -84.01\n",
      " 69 Episode in  6954 steps, reward -244.58\n",
      " 70 Episode in  7060 steps, reward -169.51\n",
      " 71 Episode in  7173 steps, reward -70.50\n",
      " 72 Episode in  7272 steps, reward -252.40\n",
      " 73 Episode in  7374 steps, reward -201.17\n",
      " 74 Episode in  7796 steps, reward -372.03\n",
      " 75 Episode in  7998 steps, reward -173.45\n",
      " 76 Episode in  8244 steps, reward -416.89\n",
      " 77 Episode in  8417 steps, reward -146.17\n",
      " 78 Episode in  8592 steps, reward -361.79\n",
      " 79 Episode in  8713 steps, reward -342.38\n",
      " 80 Episode in  8830 steps, reward -173.61\n",
      " 81 Episode in  9055 steps, reward -357.92\n",
      " 82 Episode in  9445 steps, reward -556.19\n",
      " 83 Episode in  9642 steps, reward -197.97\n",
      " 84 Episode in  9766 steps, reward -389.82\n",
      " 85 Episode in  9884 steps, reward -8.77\n",
      " 86 Episode in 10361 steps, reward -272.71\n",
      " 87 Episode in 10731 steps, reward -794.83\n",
      " 88 Episode in 10903 steps, reward -365.99\n",
      " 89 Episode in 11015 steps, reward -44.44\n",
      " 90 Episode in 11100 steps, reward -55.55\n",
      " 91 Episode in 11265 steps, reward -374.53\n",
      " 92 Episode in 11400 steps, reward -21.13\n",
      " 93 Episode in 11475 steps, reward -82.06\n",
      " 94 Episode in 11558 steps, reward -103.65\n",
      " 95 Episode in 11690 steps, reward -96.63\n",
      " 96 Episode in 11765 steps, reward -48.82\n",
      " 97 Episode in 11913 steps, reward -87.37\n",
      " 98 Episode in 12007 steps, reward -522.55\n",
      " 99 Episode in 12120 steps, reward -99.41\n",
      "100 Episode in 12259 steps, reward -258.09\n",
      "101 Episode in 12431 steps, reward -202.20\n",
      "102 Episode in 12611 steps, reward -19.82\n",
      "103 Episode in 12750 steps, reward -187.36\n",
      "104 Episode in 12890 steps, reward -76.13\n",
      "105 Episode in 12993 steps, reward -75.15\n",
      "106 Episode in 13155 steps, reward -370.35\n",
      "107 Episode in 13357 steps, reward -122.10\n",
      "108 Episode in 13490 steps, reward -7.04\n",
      "109 Episode in 13665 steps, reward -102.54\n",
      "110 Episode in 13849 steps, reward -367.64\n",
      "111 Episode in 13996 steps, reward -4.41\n",
      "112 Episode in 14151 steps, reward -141.09\n",
      "113 Episode in 14282 steps, reward -46.09\n",
      "114 Episode in 14429 steps, reward -206.74\n",
      "115 Episode in 14553 steps, reward -0.80\n",
      "116 Episode in 15504 steps, reward -322.12\n",
      "117 Episode in 15694 steps, reward -302.79\n",
      "118 Episode in 15821 steps, reward -334.44\n",
      "119 Episode in 16821 steps, reward -58.57\n",
      "120 Episode in 17821 steps, reward -34.29\n",
      "121 Episode in 17971 steps, reward -63.46\n",
      "122 Episode in 18137 steps, reward -59.51\n",
      "123 Episode in 18241 steps, reward -49.18\n",
      "124 Episode in 18342 steps, reward -138.52\n",
      "125 Episode in 18484 steps, reward -105.50\n",
      "126 Episode in 18587 steps, reward -154.14\n",
      "127 Episode in 18687 steps, reward -109.03\n",
      "128 Episode in 18847 steps, reward -184.29\n",
      "129 Episode in 18956 steps, reward -73.18\n",
      "130 Episode in 19083 steps, reward -115.66\n",
      "131 Episode in 19233 steps, reward -53.58\n",
      "132 Episode in 19415 steps, reward -189.05\n",
      "133 Episode in 19626 steps, reward -58.76\n",
      "134 Episode in 20106 steps, reward -225.66\n",
      "135 Episode in 20311 steps, reward -86.38\n",
      "136 Episode in 20549 steps, reward -259.26\n",
      "137 Episode in 20796 steps, reward -270.33\n",
      "138 Episode in 21101 steps, reward -255.20\n",
      "139 Episode in 21351 steps, reward -283.50\n",
      "140 Episode in 21620 steps, reward -204.49\n",
      "141 Episode in 22027 steps, reward -362.02\n",
      "142 Episode in 22314 steps, reward -66.63\n",
      "143 Episode in 22646 steps, reward -227.67\n",
      "144 Episode in 22829 steps, reward -266.85\n",
      "145 Episode in 23024 steps, reward -37.64\n",
      "146 Episode in 23221 steps, reward -44.48\n",
      "147 Episode in 23477 steps, reward -73.47\n",
      "148 Episode in 23739 steps, reward -108.96\n",
      "149 Episode in 24055 steps, reward -168.89\n",
      "150 Episode in 24542 steps, reward -373.86\n",
      "151 Episode in 24777 steps, reward -242.88\n",
      "152 Episode in 25109 steps, reward -281.21\n",
      "153 Episode in 25219 steps, reward -44.54\n",
      "154 Episode in 25486 steps, reward -269.36\n",
      "155 Episode in 25888 steps, reward -298.82\n",
      "156 Episode in 26169 steps, reward -166.42\n",
      "157 Episode in 26386 steps, reward -203.25\n",
      "158 Episode in 26831 steps, reward -183.30\n",
      "159 Episode in 26934 steps, reward -23.38\n",
      "160 Episode in 27167 steps, reward -125.48\n",
      "161 Episode in 27329 steps, reward 14.76\n",
      "162 Episode in 28329 steps, reward 1.43\n",
      "163 Episode in 28673 steps, reward -183.14\n",
      "164 Episode in 29405 steps, reward -246.01\n",
      "165 Episode in 30405 steps, reward -161.70\n",
      "166 Episode in 31405 steps, reward -167.28\n",
      "167 Episode in 32249 steps, reward -294.85\n",
      "168 Episode in 32915 steps, reward -254.93\n",
      "169 Episode in 33275 steps, reward -120.96\n",
      "170 Episode in 33768 steps, reward -177.29\n",
      "171 Episode in 34275 steps, reward -214.21\n",
      "172 Episode in 34611 steps, reward -172.37\n",
      "173 Episode in 35020 steps, reward -242.93\n",
      "174 Episode in 35414 steps, reward -265.97\n",
      "175 Episode in 35713 steps, reward -220.56\n",
      "176 Episode in 36332 steps, reward -236.98\n",
      "177 Episode in 36849 steps, reward -245.97\n",
      "178 Episode in 37307 steps, reward -254.86\n",
      "179 Episode in 38044 steps, reward -262.33\n",
      "180 Episode in 38803 steps, reward -278.13\n",
      "181 Episode in 39537 steps, reward -261.27\n",
      "182 Episode in 40204 steps, reward -256.36\n",
      "183 Episode in 40761 steps, reward -225.82\n",
      "184 Episode in 41283 steps, reward -227.70\n",
      "185 Episode in 41897 steps, reward -204.22\n",
      "186 Episode in 42746 steps, reward -281.48\n",
      "187 Episode in 43746 steps, reward -198.84\n",
      "188 Episode in 44561 steps, reward -300.16\n",
      "189 Episode in 45561 steps, reward -245.61\n",
      "190 Episode in 46561 steps, reward -185.96\n",
      "191 Episode in 47561 steps, reward -164.59\n",
      "192 Episode in 48561 steps, reward -173.76\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "193 Episode in 49561 steps, reward -207.89\n",
      "194 Episode in 50561 steps, reward -201.13\n",
      "195 Episode in 51561 steps, reward -171.18\n",
      "196 Episode in 52561 steps, reward -160.73\n",
      "197 Episode in 53452 steps, reward -268.97\n",
      "198 Episode in 53866 steps, reward -155.64\n",
      "199 Episode in 54504 steps, reward -215.63\n",
      "200 Episode in 55465 steps, reward -258.56\n",
      "201 Episode in 56465 steps, reward -219.81\n",
      "202 Episode in 57012 steps, reward -198.42\n",
      "203 Episode in 57515 steps, reward -176.91\n",
      "204 Episode in 58114 steps, reward -242.49\n",
      "205 Episode in 58801 steps, reward -282.05\n",
      "206 Episode in 59125 steps, reward -155.79\n",
      "207 Episode in 59517 steps, reward -178.57\n",
      "208 Episode in 60263 steps, reward -264.22\n",
      "209 Episode in 61147 steps, reward -326.67\n",
      "210 Episode in 61859 steps, reward -268.34\n",
      "211 Episode in 62586 steps, reward -294.67\n",
      "212 Episode in 63138 steps, reward -234.33\n",
      "213 Episode in 64138 steps, reward -213.88\n",
      "214 Episode in 64599 steps, reward -224.13\n",
      "215 Episode in 65395 steps, reward -326.83\n",
      "216 Episode in 66120 steps, reward -342.55\n",
      "217 Episode in 66950 steps, reward -334.07\n",
      "218 Episode in 67718 steps, reward -381.09\n",
      "219 Episode in 68200 steps, reward -295.96\n",
      "220 Episode in 69013 steps, reward -364.51\n",
      "221 Episode in 70013 steps, reward -318.76\n",
      "222 Episode in 71013 steps, reward -296.70\n",
      "223 Episode in 72013 steps, reward -276.82\n",
      "224 Episode in 73013 steps, reward -270.95\n",
      "225 Episode in 74013 steps, reward -237.06\n",
      "226 Episode in 75013 steps, reward -230.82\n",
      "227 Episode in 76013 steps, reward -191.86\n",
      "228 Episode in 77013 steps, reward -246.81\n",
      "229 Episode in 78013 steps, reward -179.99\n",
      "230 Episode in 79013 steps, reward -169.99\n",
      "231 Episode in 80013 steps, reward -181.27\n",
      "232 Episode in 81013 steps, reward -191.43\n",
      "233 Episode in 82013 steps, reward -161.46\n",
      "234 Episode in 83013 steps, reward -97.59\n",
      "235 Episode in 84013 steps, reward -114.11\n",
      "236 Episode in 85013 steps, reward -114.78\n",
      "237 Episode in 86013 steps, reward -77.88\n",
      "238 Episode in 87013 steps, reward -148.45\n",
      "239 Episode in 87243 steps, reward -78.72\n",
      "240 Episode in 88243 steps, reward -101.77\n",
      "241 Episode in 89243 steps, reward -106.70\n",
      "242 Episode in 90243 steps, reward -106.06\n",
      "243 Episode in 91243 steps, reward -99.07\n",
      "244 Episode in 92243 steps, reward -126.05\n",
      "245 Episode in 93243 steps, reward -153.66\n",
      "246 Episode in 93401 steps, reward -69.71\n",
      "247 Episode in 94401 steps, reward -111.90\n",
      "248 Episode in 95401 steps, reward -130.93\n",
      "249 Episode in 96401 steps, reward -90.41\n",
      "250 Episode in 97401 steps, reward -109.89\n",
      "251 Episode in 98401 steps, reward -99.28\n",
      "252 Episode in 99401 steps, reward -67.77\n",
      "253 Episode in 100401 steps, reward -86.95\n",
      "254 Episode in 101401 steps, reward -62.64\n",
      "255 Episode in 102401 steps, reward -85.32\n",
      "256 Episode in 103401 steps, reward -91.26\n",
      "257 Episode in 104401 steps, reward -59.48\n",
      "258 Episode in 105401 steps, reward -78.91\n",
      "259 Episode in 106401 steps, reward -76.53\n",
      "260 Episode in 107401 steps, reward -53.72\n",
      "261 Episode in 108401 steps, reward -96.96\n",
      "262 Episode in 109401 steps, reward -97.12\n",
      "263 Episode in 110401 steps, reward -85.74\n",
      "264 Episode in 111401 steps, reward -51.88\n",
      "265 Episode in 112401 steps, reward -33.04\n",
      "266 Episode in 113401 steps, reward -33.45\n",
      "267 Episode in 114401 steps, reward -56.37\n",
      "268 Episode in 115401 steps, reward -41.47\n",
      "269 Episode in 116401 steps, reward -31.80\n",
      "270 Episode in 117401 steps, reward -60.09\n",
      "271 Episode in 118401 steps, reward -120.77\n",
      "272 Episode in 119401 steps, reward -85.82\n",
      "273 Episode in 120401 steps, reward -29.65\n",
      "274 Episode in 121401 steps, reward -38.13\n",
      "275 Episode in 122401 steps, reward -83.55\n",
      "276 Episode in 123401 steps, reward -85.63\n",
      "277 Episode in 124401 steps, reward -83.07\n",
      "278 Episode in 124768 steps, reward -89.42\n",
      "279 Episode in 125768 steps, reward -62.67\n",
      "280 Episode in 126768 steps, reward -42.52\n",
      "281 Episode in 127768 steps, reward -56.49\n",
      "282 Episode in 128768 steps, reward -53.03\n",
      "283 Episode in 129768 steps, reward -51.53\n",
      "284 Episode in 130181 steps, reward -112.13\n",
      "285 Episode in 131181 steps, reward -45.32\n",
      "286 Episode in 132181 steps, reward -85.89\n",
      "287 Episode in 133181 steps, reward -68.98\n",
      "288 Episode in 134181 steps, reward -65.36\n",
      "289 Episode in 135181 steps, reward -70.11\n",
      "290 Episode in 136181 steps, reward -69.78\n",
      "291 Episode in 137181 steps, reward 13.07\n",
      "292 Episode in 138181 steps, reward -34.65\n",
      "293 Episode in 139181 steps, reward -28.93\n",
      "294 Episode in 140181 steps, reward -23.15\n",
      "295 Episode in 141181 steps, reward -38.12\n",
      "296 Episode in 142181 steps, reward -50.26\n",
      "297 Episode in 143181 steps, reward -56.07\n",
      "298 Episode in 143827 steps, reward -199.73\n",
      "299 Episode in 144082 steps, reward -80.84\n",
      "300 Episode in 144308 steps, reward -104.01\n",
      "301 Episode in 145308 steps, reward -78.77\n",
      "302 Episode in 146308 steps, reward -137.15\n",
      "303 Episode in 147308 steps, reward -89.57\n",
      "304 Episode in 148308 steps, reward -68.56\n",
      "305 Episode in 149308 steps, reward -86.05\n",
      "306 Episode in 150308 steps, reward -98.37\n",
      "307 Episode in 151308 steps, reward -65.42\n",
      "308 Episode in 152308 steps, reward -96.68\n",
      "309 Episode in 153308 steps, reward -88.83\n",
      "310 Episode in 154308 steps, reward -58.17\n",
      "311 Episode in 155308 steps, reward -61.51\n",
      "312 Episode in 155576 steps, reward -69.83\n",
      "313 Episode in 156576 steps, reward -62.34\n",
      "314 Episode in 157576 steps, reward -88.18\n",
      "315 Episode in 158576 steps, reward -69.28\n",
      "316 Episode in 159011 steps, reward -123.35\n",
      "317 Episode in 160011 steps, reward -68.10\n",
      "318 Episode in 161011 steps, reward -35.49\n",
      "319 Episode in 162011 steps, reward -90.34\n",
      "320 Episode in 163011 steps, reward -54.67\n",
      "321 Episode in 163350 steps, reward -102.13\n",
      "322 Episode in 164350 steps, reward -87.92\n",
      "323 Episode in 164521 steps, reward -112.38\n",
      "324 Episode in 165521 steps, reward -79.30\n",
      "325 Episode in 166521 steps, reward -112.67\n",
      "326 Episode in 167521 steps, reward -70.59\n",
      "327 Episode in 168521 steps, reward -144.86\n",
      "328 Episode in 169521 steps, reward -97.83\n",
      "329 Episode in 170323 steps, reward -233.49\n",
      "330 Episode in 171323 steps, reward -139.30\n",
      "331 Episode in 172192 steps, reward -213.99\n",
      "332 Episode in 173192 steps, reward -112.57\n",
      "333 Episode in 174192 steps, reward -114.93\n",
      "334 Episode in 175192 steps, reward -107.46\n",
      "335 Episode in 176192 steps, reward -99.55\n",
      "336 Episode in 177192 steps, reward -106.10\n",
      "337 Episode in 178192 steps, reward -135.99\n",
      "338 Episode in 179192 steps, reward -77.63\n",
      "339 Episode in 180192 steps, reward -73.05\n",
      "340 Episode in 181192 steps, reward -76.34\n",
      "341 Episode in 182192 steps, reward -74.36\n",
      "342 Episode in 183192 steps, reward -124.78\n",
      "343 Episode in 184192 steps, reward -70.86\n",
      "344 Episode in 185192 steps, reward -51.32\n",
      "345 Episode in 185422 steps, reward -114.51\n",
      "346 Episode in 186422 steps, reward -94.28\n",
      "347 Episode in 186676 steps, reward -146.48\n",
      "348 Episode in 186987 steps, reward -116.11\n",
      "349 Episode in 187987 steps, reward -143.06\n",
      "350 Episode in 188284 steps, reward -125.12\n",
      "351 Episode in 188460 steps, reward -86.27\n",
      "352 Episode in 188703 steps, reward -95.41\n",
      "353 Episode in 189703 steps, reward -107.52\n",
      "354 Episode in 190703 steps, reward -143.04\n",
      "355 Episode in 191703 steps, reward -117.56\n",
      "356 Episode in 192703 steps, reward -153.59\n",
      "357 Episode in 193703 steps, reward -147.37\n",
      "358 Episode in 194703 steps, reward -138.71\n",
      "359 Episode in 195703 steps, reward -148.72\n",
      "360 Episode in 196703 steps, reward -82.26\n",
      "361 Episode in 197703 steps, reward -86.72\n",
      "362 Episode in 198703 steps, reward -112.33\n",
      "363 Episode in 198835 steps, reward -117.08\n",
      "364 Episode in 199835 steps, reward -138.92\n",
      "365 Episode in 200835 steps, reward -115.66\n",
      "366 Episode in 201835 steps, reward -100.86\n",
      "367 Episode in 202835 steps, reward -71.99\n",
      "368 Episode in 203835 steps, reward -21.40\n",
      "369 Episode in 204835 steps, reward -71.33\n",
      "370 Episode in 205835 steps, reward -87.21\n",
      "371 Episode in 206835 steps, reward -112.85\n",
      "372 Episode in 207835 steps, reward -84.40\n",
      "373 Episode in 208835 steps, reward -135.86\n",
      "374 Episode in 209835 steps, reward -89.68\n",
      "375 Episode in 210835 steps, reward -49.27\n",
      "376 Episode in 211028 steps, reward -78.85\n",
      "377 Episode in 212028 steps, reward -104.80\n",
      "378 Episode in 213028 steps, reward -55.13\n",
      "379 Episode in 213353 steps, reward -110.80\n",
      "380 Episode in 214353 steps, reward -86.97\n",
      "381 Episode in 215353 steps, reward -138.79\n",
      "382 Episode in 216353 steps, reward -110.89\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "383 Episode in 217353 steps, reward -46.71\n",
      "384 Episode in 218353 steps, reward -133.04\n",
      "385 Episode in 219353 steps, reward -43.71\n",
      "386 Episode in 220353 steps, reward -68.89\n",
      "387 Episode in 221353 steps, reward -100.38\n",
      "388 Episode in 222353 steps, reward -86.75\n",
      "389 Episode in 223353 steps, reward -91.16\n",
      "390 Episode in 224353 steps, reward -98.58\n",
      "391 Episode in 225353 steps, reward -38.16\n",
      "392 Episode in 226353 steps, reward -87.67\n",
      "393 Episode in 226568 steps, reward -106.09\n",
      "394 Episode in 226920 steps, reward -126.39\n",
      "395 Episode in 227177 steps, reward -105.24\n",
      "396 Episode in 228177 steps, reward -72.81\n",
      "397 Episode in 229177 steps, reward -98.73\n",
      "398 Episode in 229790 steps, reward -154.15\n",
      "399 Episode in 230790 steps, reward -92.86\n",
      "400 Episode in 231790 steps, reward -97.00\n",
      "401 Episode in 232790 steps, reward -91.32\n",
      "402 Episode in 233790 steps, reward -84.45\n",
      "403 Episode in 233982 steps, reward -90.34\n",
      "404 Episode in 234982 steps, reward -145.26\n",
      "405 Episode in 235982 steps, reward -71.80\n",
      "406 Episode in 236982 steps, reward -87.09\n",
      "407 Episode in 237982 steps, reward -130.21\n",
      "408 Episode in 238982 steps, reward -145.25\n",
      "409 Episode in 239982 steps, reward -92.34\n",
      "410 Episode in 240982 steps, reward -73.59\n",
      "411 Episode in 241982 steps, reward -72.26\n",
      "412 Episode in 242982 steps, reward -45.33\n",
      "413 Episode in 243982 steps, reward -121.46\n",
      "414 Episode in 244982 steps, reward -91.79\n",
      "415 Episode in 245982 steps, reward -97.46\n",
      "416 Episode in 246982 steps, reward -24.96\n",
      "417 Episode in 247982 steps, reward -79.46\n",
      "418 Episode in 248982 steps, reward -23.51\n",
      "419 Episode in 249982 steps, reward -65.89\n",
      "420 Episode in 250982 steps, reward -62.65\n",
      "421 Episode in 251982 steps, reward -53.88\n",
      "422 Episode in 252379 steps, reward -111.95\n",
      "423 Episode in 253379 steps, reward -28.59\n",
      "424 Episode in 254379 steps, reward -88.13\n",
      "425 Episode in 255379 steps, reward -99.47\n",
      "426 Episode in 256379 steps, reward -51.16\n",
      "427 Episode in 257379 steps, reward -33.26\n",
      "428 Episode in 258051 steps, reward -162.33\n",
      "429 Episode in 258631 steps, reward -139.41\n",
      "430 Episode in 259631 steps, reward -53.44\n",
      "431 Episode in 259909 steps, reward -96.92\n",
      "432 Episode in 260909 steps, reward -76.66\n",
      "433 Episode in 261909 steps, reward -120.82\n",
      "434 Episode in 262909 steps, reward -144.44\n",
      "435 Episode in 263909 steps, reward -138.35\n",
      "436 Episode in 264909 steps, reward -116.07\n",
      "437 Episode in 265909 steps, reward -62.53\n",
      "438 Episode in 266909 steps, reward -103.00\n",
      "439 Episode in 267909 steps, reward -29.18\n",
      "440 Episode in 268909 steps, reward -92.40\n",
      "441 Episode in 269909 steps, reward -92.56\n",
      "442 Episode in 270909 steps, reward -113.49\n",
      "443 Episode in 271909 steps, reward -80.02\n",
      "444 Episode in 272909 steps, reward -98.65\n",
      "445 Episode in 273909 steps, reward -64.52\n",
      "446 Episode in 274909 steps, reward -122.37\n",
      "447 Episode in 275064 steps, reward -99.46\n",
      "448 Episode in 276064 steps, reward -74.63\n",
      "449 Episode in 277064 steps, reward -96.65\n",
      "450 Episode in 278064 steps, reward -71.52\n",
      "451 Episode in 279064 steps, reward -101.93\n",
      "452 Episode in 280064 steps, reward -110.28\n",
      "453 Episode in 281064 steps, reward -47.09\n",
      "454 Episode in 282064 steps, reward -15.36\n",
      "455 Episode in 283064 steps, reward -74.48\n",
      "456 Episode in 284064 steps, reward -64.15\n",
      "457 Episode in 285064 steps, reward -50.56\n",
      "458 Episode in 286064 steps, reward -31.18\n",
      "459 Episode in 287064 steps, reward -55.74\n",
      "460 Episode in 288064 steps, reward -66.01\n",
      "461 Episode in 289064 steps, reward -66.90\n",
      "462 Episode in 290064 steps, reward -105.69\n",
      "463 Episode in 291064 steps, reward -86.74\n",
      "464 Episode in 292064 steps, reward -65.42\n",
      "465 Episode in 293064 steps, reward -10.50\n",
      "466 Episode in 294064 steps, reward -70.37\n",
      "467 Episode in 295064 steps, reward -34.90\n",
      "468 Episode in 296064 steps, reward -9.99\n",
      "469 Episode in 297064 steps, reward 6.10\n",
      "470 Episode in 298064 steps, reward -108.62\n",
      "471 Episode in 299064 steps, reward -61.32\n",
      "472 Episode in 300064 steps, reward -72.55\n",
      "473 Episode in 301064 steps, reward -55.67\n",
      "474 Episode in 302064 steps, reward -115.99\n",
      "475 Episode in 303064 steps, reward -44.62\n",
      "476 Episode in 304064 steps, reward -73.25\n",
      "477 Episode in 305064 steps, reward -82.85\n",
      "478 Episode in 305279 steps, reward -110.03\n",
      "479 Episode in 305997 steps, reward -162.25\n",
      "480 Episode in 306997 steps, reward -16.57\n",
      "481 Episode in 307997 steps, reward -59.85\n",
      "482 Episode in 308997 steps, reward -112.01\n",
      "483 Episode in 309467 steps, reward -115.69\n",
      "484 Episode in 310467 steps, reward -45.12\n",
      "485 Episode in 311467 steps, reward -140.33\n",
      "486 Episode in 312321 steps, reward -253.96\n",
      "487 Episode in 313321 steps, reward -11.24\n",
      "488 Episode in 314321 steps, reward -14.44\n",
      "489 Episode in 314823 steps, reward -151.47\n",
      "490 Episode in 315823 steps, reward -15.87\n",
      "491 Episode in 316823 steps, reward -23.41\n",
      "492 Episode in 317823 steps, reward -17.34\n",
      "493 Episode in 318823 steps, reward -13.30\n",
      "494 Episode in 319823 steps, reward -88.25\n",
      "495 Episode in 320823 steps, reward -101.09\n",
      "496 Episode in 321823 steps, reward -79.27\n",
      "497 Episode in 322495 steps, reward -225.21\n",
      "498 Episode in 323495 steps, reward -44.35\n",
      "499 Episode in 324495 steps, reward -84.73\n",
      "500 Episode in 325227 steps, reward -156.79\n",
      "501 Episode in 326034 steps, reward -215.94\n",
      "502 Episode in 326177 steps, reward -109.81\n",
      "503 Episode in 327177 steps, reward -129.23\n",
      "504 Episode in 328177 steps, reward -92.03\n",
      "505 Episode in 328412 steps, reward -101.09\n",
      "506 Episode in 329412 steps, reward -108.24\n",
      "507 Episode in 330412 steps, reward -89.80\n",
      "508 Episode in 331412 steps, reward -35.28\n",
      "509 Episode in 332412 steps, reward -87.96\n",
      "510 Episode in 333412 steps, reward -96.45\n",
      "511 Episode in 334412 steps, reward -78.88\n",
      "512 Episode in 334549 steps, reward -106.08\n",
      "513 Episode in 335549 steps, reward -150.17\n",
      "514 Episode in 335691 steps, reward -103.32\n",
      "515 Episode in 336691 steps, reward -123.92\n",
      "516 Episode in 337691 steps, reward -124.69\n",
      "517 Episode in 338691 steps, reward -136.11\n",
      "518 Episode in 339691 steps, reward -133.61\n",
      "519 Episode in 340691 steps, reward -116.11\n",
      "520 Episode in 341691 steps, reward -112.67\n",
      "521 Episode in 342691 steps, reward -75.15\n",
      "522 Episode in 343691 steps, reward -131.11\n",
      "523 Episode in 343809 steps, reward -107.71\n",
      "524 Episode in 344809 steps, reward -83.43\n"
     ]
    }
   ],
   "source": [
    "# play\n",
    "for i in range(1, n_episodes + 1):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    ep_reward = 0\n",
    "    while not done:\n",
    "        env.render()\n",
    "\n",
    "        action, value = get_action_and_value(obs, old_net)\n",
    "        _obs, reward, done, _ = env.step(action)\n",
    "        \n",
    "        # store\n",
    "        roll_memory.append([obs, action, reward, _obs, done])\n",
    "        rewards.append(reward)\n",
    "        values.append(value)\n",
    "        dones.append(done)\n",
    "        \n",
    "        obs = _obs\n",
    "        steps += 1\n",
    "        ep_reward += reward\n",
    "        \n",
    "        if done or steps % roll_len == 0:\n",
    "            _, _value = get_action_and_value(_obs, old_net)\n",
    "            values.append(_value)\n",
    "            train_memory.extend(compute_adv(rewards, values, dones, roll_memory))\n",
    "            rewards.clear()\n",
    "            values.clear()\n",
    "            dones.clear()\n",
    "            roll_memory.clear()\n",
    "            \n",
    "        if steps % roll_len == 0:\n",
    "#             print('\\n============  Start Learning  ============\\n')\n",
    "            learn(net, old_net, optimizer, train_memory)\n",
    "            learn_steps += 1\n",
    "            train_memory.clear()\n",
    "\n",
    "        if learn_steps > 1:\n",
    "            old_net.load_state_dict(net.state_dict())\n",
    "            learn_steps = 1\n",
    "    \n",
    "    if done:        \n",
    "        ep_rewards.append(ep_reward)\n",
    "        print('{:3} Episode in {:5} steps, reward {:.2f}'.format(\n",
    "            i, steps, ep_reward))\n",
    "\n",
    "        if len(ep_rewards) >= n_eval:\n",
    "            if np.mean(list(reversed(ep_rewards))[: n_eval]) >= env.spec.reward_threshold:\n",
    "                print('\\n{} is sloved! {:3} Episode in {:3} steps'.format(\n",
    "                    env.spec.id, i, steps))\n",
    "                torch.save(old_net.state_dict(),\n",
    "                           f'./test/saved_models/{env.spec.id}_ep{i}_clear_model_ppo.pt')\n",
    "                break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "plt.title('Reward')\n",
    "plt.plot(ep_rewards)\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.title('Loss')\n",
    "plt.plot(losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[\n",
    "    ('CartPole-v0', 161, 32, 256),\n",
    "    ('CartPole-v1', 162, 32, 256),\n",
    "    ('MountainCar-v0', 660),\n",
    "    ('LunarLander-v2', 260)\n",
    "]"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "C51_tensorflow.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
