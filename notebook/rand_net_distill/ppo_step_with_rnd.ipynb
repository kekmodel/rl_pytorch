{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IWnm3qot3o1W"
   },
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "from copy import deepcopy\n",
    "\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 708,
     "status": "ok",
     "timestamp": 1534482400648,
     "user": {
      "displayName": "윤승제",
      "photoUrl": "//lh5.googleusercontent.com/-EucKC7DmcQI/AAAAAAAAAAI/AAAAAAAAAGA/gQU1NPEmNFA/s50-c-k-no/photo.jpg",
      "userId": "105654037995838004821"
     },
     "user_tz": -540
    },
    "id": "maRVADiTlzHD",
    "outputId": "783b7610-95c2-4b54-b2ce-d8e853c484ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda: True\n"
     ]
    }
   ],
   "source": [
    "SEED = 5\n",
    "BATCH_SIZE = 4\n",
    "LR = 0.0001\n",
    "EPOCHS = 4\n",
    "CLIP = 0.1\n",
    "GAMMA_EX = 0.999\n",
    "GAMMA_IN = 0.99\n",
    "LAMBDA = 0.95\n",
    "ENT_COEF = 0.001\n",
    "EX_COEF = 2.0\n",
    "IN_COEF = 1.0\n",
    "EPS = 1.1920929e-07\n",
    "\n",
    "# set device\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print('cuda:', use_cuda)\n",
    "device = torch.device('cuda' if use_cuda else 'cpu')\n",
    "\n",
    "# random seed\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if use_cuda:\n",
    "    torch.cuda.manual_seed_all(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9Ffkl_5C4R81"
   },
   "outputs": [],
   "source": [
    "class ActorCriticNet(nn.Module):\n",
    "    def __init__(self, obs_space, action_space):\n",
    "        super().__init__()\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(obs_space, 256),\n",
    "            nn.SELU()\n",
    "        )\n",
    "\n",
    "        self.pol = nn.Sequential(\n",
    "            nn.Linear(256, 256),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(256, action_space)\n",
    "        )\n",
    "        \n",
    "        self.val_ex = nn.Sequential(\n",
    "            nn.Linear(256, 256),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "        \n",
    "        self.val_in = nn.Sequential(\n",
    "            nn.Linear(256, 256),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "        \n",
    "        self.log_softmax = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.head(x)\n",
    "        logit = self.pol(out).reshape(out.shape[0], -1)\n",
    "        value_ex = self.val_ex(out).reshape(out.shape[0], 1)\n",
    "        value_in = self.val_in(out).reshape(out.shape[0], 1)\n",
    "        log_probs = self.log_softmax(logit)\n",
    "        \n",
    "        return log_probs, value_ex, value_in\n",
    "    \n",
    "class RandomNet(nn.Module):\n",
    "    def __init__(self, obs_space):\n",
    "        super().__init__()\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(obs_space, 256),\n",
    "            nn.SELU()\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(256, 512)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.head(x)\n",
    "        obs_feature = self.fc(out).reshape(out.shape[0], -1)\n",
    "\n",
    "        return obs_feature\n",
    "\n",
    "\n",
    "class PredictNet(nn.Module):\n",
    "    def __init__(self, obs_space):\n",
    "        super().__init__()\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(obs_space, 256),\n",
    "            nn.SELU()\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(256, 256),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(256, 512)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.head(x)\n",
    "        obs_feature = self.fc(out).reshape(out.shape[0], -1)\n",
    "\n",
    "        return obs_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "m_losses = []\n",
    "f_losses = []\n",
    "\n",
    "\n",
    "def learn(net, old_net, pred_net, rand_net, net_optim, pred_optim, train_memory):\n",
    "    net.train()\n",
    "    old_net.train()\n",
    "    \n",
    "    dataloader = DataLoader(\n",
    "        train_memory,\n",
    "        shuffle=True,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        pin_memory=use_cuda\n",
    "    )\n",
    "\n",
    "    for i in range(EPOCHS):\n",
    "        for (s, a, _s, ret_ex, ret_in, adv_ex, adv_in) in dataloader:\n",
    "            s = s.to(device).float()\n",
    "            a  = a.to(device).long()\n",
    "            _s = _s.to(device).float()\n",
    "            _s_norm_np = normalize_obs(_s.detach().cpu().numpy())\n",
    "            _s_norm = torch.tensor(_s_norm_np).to(device).float()\n",
    "            ret_ex = ret_ex.to(device).float()\n",
    "            ret_in = ret_in.to(device).float()\n",
    "            adv = (adv_ex + adv_in).to(device).float()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                rand_f = rand_net(_s_norm)\n",
    "                log_p_old, v_ex_old, v_in_old = old_net(s)\n",
    "                log_p_a_old = log_p_old[range(BATCH_SIZE), a]\n",
    "\n",
    "            pred_f = pred_net(_s_norm)\n",
    "            log_p, v_ex, v_in = net(s)\n",
    "            log_p_a = log_p[range(BATCH_SIZE), a]\n",
    "            p_ratio = (log_p_a - log_p_a_old).exp()\n",
    "            p_r_clip = torch.clamp(p_ratio, 1. - CLIP, 1. + CLIP)\n",
    "            p_loss = torch.min(p_ratio * adv, p_r_clip * adv).mean()\n",
    "            v_ex_loss = (ret_ex - v_ex).pow(2)\n",
    "            v_in_loss = (ret_in - v_in).pow(2)\n",
    "            v_loss = (v_ex_loss + v_in_loss).mean() \n",
    "            entropy = -(log_p.exp() * log_p).sum(dim=1).mean()\n",
    "\n",
    "            # loss\n",
    "            m_loss = -(p_loss - v_loss + ENT_COEF * entropy)\n",
    "            m_losses.append(m_loss)\n",
    "\n",
    "            f_loss = (pred_f - rand_f).pow(2).sum(dim=1).mean()\n",
    "            f_losses.append(f_loss)\n",
    "            \n",
    "            loss = m_loss + f_loss\n",
    "            losses.append(loss)\n",
    "            \n",
    "            net_optim.zero_grad()\n",
    "            pred_optim.zero_grad()\n",
    "            loss.backward()\n",
    "            net_optim.step()\n",
    "            pred_optim.step()\n",
    "\n",
    "\n",
    "def get_action_and_value(obs, net):\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        state = torch.tensor([obs]).to(device).float()\n",
    "        log_p, v_ex, v_in = net(state)\n",
    "        m = Categorical(log_p.exp())\n",
    "        action = m.sample()\n",
    "\n",
    "    return action.item(), v_ex.item(), v_in.item()\n",
    "\n",
    "\n",
    "def compute_adv_with_gae(rews_ex, rews_in, vals_ex, vals_in, roll_memory):\n",
    "    rew_ex = np.array(rews_ex, 'float')\n",
    "    rew_in = np.array(rews_in, 'float')\n",
    "    val_ex = np.array(vals_ex[:-1], 'float')\n",
    "    val_in = np.array(vals_in[:-1], 'float')\n",
    "    _val_ex = np.array(vals_ex[1:], 'float')\n",
    "    _val_in = np.array(vals_in[1:], 'float')\n",
    "    dt_ex = rew_ex + GAMMA_EX * _val_ex - val_ex\n",
    "    dt_in = rew_in + GAMMA_IN * _val_in - val_in\n",
    "    dis_r_ex = np.array([GAMMA_EX**(i) * r for i, r in enumerate(rews_ex)], 'float')\n",
    "    dis_r_in = np.array([GAMMA_IN**(i) * r for i, r in enumerate(rews_in)], 'float')\n",
    "    gae_ex = np.array([(GAMMA_EX * LAMBDA)**(i) * dt for i, dt in enumerate(dt_ex.tolist())], 'float')\n",
    "    gae_in = np.array([(GAMMA_IN * LAMBDA)**(i) * dt for i, dt in enumerate(dt_in.tolist())], 'float')\n",
    "    for i, data in enumerate(roll_memory):\n",
    "        data.append(sum(dis_r_ex[i:] / GAMMA_EX**(i)))\n",
    "        data.append(sum(dis_r_in[i:] / GAMMA_IN**(i)))\n",
    "        data.append(sum(gae_ex[i:] / (GAMMA_EX * LAMBDA)**(i)))\n",
    "        data.append(sum(gae_in[i:] / (GAMMA_IN * LAMBDA)**(i)))\n",
    "\n",
    "    return roll_memory\n",
    "\n",
    "\n",
    "def get_norm_params(obs_memory):\n",
    "    global obs_apace\n",
    "\n",
    "    obses = [[] for _ in range(obs_space)]\n",
    "    for obs in obs_memory:\n",
    "        for j in range(obs_space):\n",
    "            obses[j].append(obs[j])\n",
    "\n",
    "    mean = np.zeros(obs_space, 'float')\n",
    "    std = np.zeros(obs_space, 'float')\n",
    "    for i, obs_ in enumerate(obses):\n",
    "        mean[i] = np.mean(obs_)\n",
    "        std[i] = np.std(obs_)\n",
    "\n",
    "    return mean, np.clip(std, a_min=EPS, a_max=None)\n",
    "\n",
    "\n",
    "def normalize_obs(obs):\n",
    "    global mean, std\n",
    "    norm_obs = (obs - mean) / std\n",
    "#     return np.clip(norm_obs, -5, 5)\n",
    "    return norm_obs\n",
    "\n",
    "\n",
    "def calculate_reward_in(pred_net, rand_net, obs):\n",
    "    norm_obs = normalize_obs(obs)\n",
    "    state = torch.tensor([norm_obs]).to(device).float()\n",
    "    with torch.no_grad():\n",
    "        pred_obs = pred_net(state)\n",
    "        rand_obs = rand_net(state)\n",
    "        reward = (pred_obs - rand_obs).pow(2).sum()\n",
    "        clipped_reward = torch.clamp(reward, -1, 1)\n",
    "\n",
    "    return clipped_reward.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 3377
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 135196,
     "status": "ok",
     "timestamp": 1534482559393,
     "user": {
      "displayName": "윤승제",
      "photoUrl": "//lh5.googleusercontent.com/-EucKC7DmcQI/AAAAAAAAAAI/AAAAAAAAAGA/gQU1NPEmNFA/s50-c-k-no/photo.jpg",
      "userId": "105654037995838004821"
     },
     "user_tz": -540
    },
    "id": "PnifSBJglzHh",
    "outputId": "94177345-918e-4a96-d9a8-d8aba0a4bc9a",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# make an environment\n",
    "# env = gym.make('CartPole-v0')\n",
    "# env = gym.make('CartPole-v1')\n",
    "env = gym.make('MountainCar-v0')\n",
    "# env = gym.make('LunarLander-v2')\n",
    "\n",
    "env.seed(SEED)\n",
    "obs_space = env.observation_space.shape[0]\n",
    "action_space = env.action_space.n\n",
    "\n",
    "# hyperparameter\n",
    "n_episodes = 5000\n",
    "roll_len = 128\n",
    "n_eval = env.spec.trials\n",
    "\n",
    "# global values\n",
    "init_steps = 0\n",
    "steps = 0\n",
    "learn_steps = 0\n",
    "mean = 0.\n",
    "std = 0.\n",
    "ep_rewards = []\n",
    "is_rollout = False\n",
    "is_solved = False\n",
    "\n",
    "# make a rollout memory\n",
    "net_memory = deque(maxlen=2)\n",
    "train_memory = []\n",
    "roll_memory = []\n",
    "obs_memory = []\n",
    "rews_ex = []\n",
    "rews_in = []\n",
    "vals_ex = []\n",
    "vals_in = []\n",
    "\n",
    "# make nerual networks\n",
    "net = ActorCriticNet(obs_space, action_space).to(device)\n",
    "old_net = deepcopy(net)\n",
    "net_memory.appendleft(net.state_dict())\n",
    "pred_net = PredictNet(obs_space).to(device)\n",
    "rand_net = RandomNet(obs_space).to(device)\n",
    "\n",
    "# make optimizer\n",
    "net_optim = torch.optim.Adam(net.parameters(), lr=LR, eps=1e-5)\n",
    "pred_optim = torch.optim.Adam(pred_net.parameters(), lr=LR, eps=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.spec.max_episode_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.spec.trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-110.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.spec.reward_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 3377
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 135196,
     "status": "ok",
     "timestamp": 1534482559393,
     "user": {
      "displayName": "윤승제",
      "photoUrl": "//lh5.googleusercontent.com/-EucKC7DmcQI/AAAAAAAAAAI/AAAAAAAAAGA/gQU1NPEmNFA/s50-c-k-no/photo.jpg",
      "userId": "105654037995838004821"
     },
     "user_tz": -540
    },
    "id": "PnifSBJglzHh",
    "outputId": "94177345-918e-4a96-d9a8-d8aba0a4bc9a"
   },
   "outputs": [],
   "source": [
    "# simulation\n",
    "while True:\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "#         env.render()\n",
    "        action = env.action_space.sample()\n",
    "        _obs, _, done, _ = env.step(action)\n",
    "        obs_memory.append(_obs)\n",
    "        init_steps += 1\n",
    "        if init_steps == roll_len * 50:\n",
    "            mean, std = get_norm_params(obs_memory)\n",
    "            obs_memory.clear()\n",
    "            done = True\n",
    "    if done:\n",
    "        if init_steps == roll_len * 50:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 3377
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 135196,
     "status": "ok",
     "timestamp": 1534482559393,
     "user": {
      "displayName": "윤승제",
      "photoUrl": "//lh5.googleusercontent.com/-EucKC7DmcQI/AAAAAAAAAAI/AAAAAAAAAGA/gQU1NPEmNFA/s50-c-k-no/photo.jpg",
      "userId": "105654037995838004821"
     },
     "user_tz": -540
    },
    "id": "PnifSBJglzHh",
    "outputId": "94177345-918e-4a96-d9a8-d8aba0a4bc9a",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1 Episode in   200 steps, reward_ex -200.00, reward_in 191.60\n",
      "  2 Episode in   400 steps, reward_ex -200.00, reward_in 190.14\n",
      "  3 Episode in   600 steps, reward_ex -200.00, reward_in 181.98\n",
      "  4 Episode in   800 steps, reward_ex -200.00, reward_in 158.49\n",
      "  5 Episode in  1000 steps, reward_ex -200.00, reward_in 150.72\n",
      "  6 Episode in  1200 steps, reward_ex -200.00, reward_in 151.94\n",
      "  7 Episode in  1400 steps, reward_ex -200.00, reward_in 170.89\n",
      "  8 Episode in  1600 steps, reward_ex -200.00, reward_in 98.49\n",
      "  9 Episode in  1800 steps, reward_ex -200.00, reward_in 154.27\n",
      " 10 Episode in  2000 steps, reward_ex -200.00, reward_in 79.74\n",
      " 11 Episode in  2200 steps, reward_ex -200.00, reward_in 104.92\n",
      " 12 Episode in  2400 steps, reward_ex -200.00, reward_in 72.96\n",
      " 13 Episode in  2600 steps, reward_ex -200.00, reward_in 59.66\n",
      " 14 Episode in  2800 steps, reward_ex -200.00, reward_in 107.92\n",
      " 15 Episode in  3000 steps, reward_ex -200.00, reward_in 178.26\n",
      " 16 Episode in  3200 steps, reward_ex -200.00, reward_in 127.89\n",
      " 17 Episode in  3400 steps, reward_ex -200.00, reward_in 105.48\n",
      " 18 Episode in  3600 steps, reward_ex -200.00, reward_in 54.67\n",
      " 19 Episode in  3800 steps, reward_ex -200.00, reward_in 45.68\n",
      " 20 Episode in  4000 steps, reward_ex -200.00, reward_in 56.50\n",
      " 21 Episode in  4200 steps, reward_ex -200.00, reward_in 180.97\n",
      " 22 Episode in  4400 steps, reward_ex -200.00, reward_in 100.84\n",
      " 23 Episode in  4600 steps, reward_ex -200.00, reward_in 116.55\n",
      " 24 Episode in  4800 steps, reward_ex -200.00, reward_in 42.46\n",
      " 25 Episode in  5000 steps, reward_ex -200.00, reward_in 44.93\n",
      " 26 Episode in  5200 steps, reward_ex -200.00, reward_in 30.90\n",
      " 27 Episode in  5400 steps, reward_ex -200.00, reward_in 26.25\n",
      " 28 Episode in  5600 steps, reward_ex -200.00, reward_in 38.91\n",
      " 29 Episode in  5800 steps, reward_ex -200.00, reward_in 102.66\n",
      " 30 Episode in  6000 steps, reward_ex -200.00, reward_in 58.51\n",
      " 31 Episode in  6200 steps, reward_ex -200.00, reward_in 27.14\n",
      " 32 Episode in  6400 steps, reward_ex -200.00, reward_in 44.45\n",
      " 33 Episode in  6600 steps, reward_ex -200.00, reward_in 78.85\n",
      " 34 Episode in  6800 steps, reward_ex -200.00, reward_in 73.49\n",
      " 35 Episode in  7000 steps, reward_ex -200.00, reward_in 45.28\n",
      " 36 Episode in  7200 steps, reward_ex -200.00, reward_in 39.95\n",
      " 37 Episode in  7400 steps, reward_ex -200.00, reward_in 32.53\n",
      " 38 Episode in  7600 steps, reward_ex -200.00, reward_in 32.33\n",
      " 39 Episode in  7800 steps, reward_ex -200.00, reward_in 24.38\n",
      " 40 Episode in  8000 steps, reward_ex -200.00, reward_in 55.28\n",
      " 41 Episode in  8200 steps, reward_ex -200.00, reward_in 24.37\n",
      " 42 Episode in  8400 steps, reward_ex -200.00, reward_in 32.55\n",
      " 43 Episode in  8600 steps, reward_ex -200.00, reward_in 37.24\n",
      " 44 Episode in  8800 steps, reward_ex -200.00, reward_in 20.62\n",
      " 45 Episode in  9000 steps, reward_ex -200.00, reward_in 18.41\n",
      " 46 Episode in  9200 steps, reward_ex -200.00, reward_in 31.27\n",
      " 47 Episode in  9400 steps, reward_ex -200.00, reward_in 17.69\n",
      " 48 Episode in  9600 steps, reward_ex -200.00, reward_in 74.96\n",
      " 49 Episode in  9800 steps, reward_ex -200.00, reward_in 110.21\n",
      " 50 Episode in 10000 steps, reward_ex -200.00, reward_in 22.38\n",
      " 51 Episode in 10200 steps, reward_ex -200.00, reward_in 62.41\n",
      " 52 Episode in 10400 steps, reward_ex -200.00, reward_in 18.67\n",
      " 53 Episode in 10600 steps, reward_ex -200.00, reward_in 32.04\n",
      " 54 Episode in 10800 steps, reward_ex -200.00, reward_in 35.90\n",
      " 55 Episode in 11000 steps, reward_ex -200.00, reward_in 17.17\n",
      " 56 Episode in 11200 steps, reward_ex -200.00, reward_in 14.23\n",
      " 57 Episode in 11400 steps, reward_ex -200.00, reward_in 31.05\n",
      " 58 Episode in 11600 steps, reward_ex -200.00, reward_in 26.07\n",
      " 59 Episode in 11800 steps, reward_ex -200.00, reward_in 38.69\n",
      " 60 Episode in 12000 steps, reward_ex -200.00, reward_in 27.92\n",
      " 61 Episode in 12200 steps, reward_ex -200.00, reward_in 15.01\n",
      " 62 Episode in 12400 steps, reward_ex -200.00, reward_in 13.96\n",
      " 63 Episode in 12600 steps, reward_ex -200.00, reward_in 60.07\n",
      " 64 Episode in 12800 steps, reward_ex -200.00, reward_in 44.99\n",
      " 65 Episode in 13000 steps, reward_ex -200.00, reward_in 45.13\n",
      " 66 Episode in 13200 steps, reward_ex -200.00, reward_in 42.76\n",
      " 67 Episode in 13400 steps, reward_ex -200.00, reward_in 11.75\n",
      " 68 Episode in 13600 steps, reward_ex -200.00, reward_in 15.02\n",
      " 69 Episode in 13800 steps, reward_ex -200.00, reward_in 49.34\n",
      " 70 Episode in 14000 steps, reward_ex -200.00, reward_in 16.66\n",
      " 71 Episode in 14200 steps, reward_ex -200.00, reward_in 12.44\n",
      " 72 Episode in 14400 steps, reward_ex -200.00, reward_in 11.34\n",
      " 73 Episode in 14600 steps, reward_ex -200.00, reward_in 9.35\n",
      " 74 Episode in 14800 steps, reward_ex -200.00, reward_in 15.55\n",
      " 75 Episode in 15000 steps, reward_ex -200.00, reward_in 12.47\n",
      " 76 Episode in 15200 steps, reward_ex -200.00, reward_in 105.84\n",
      " 77 Episode in 15400 steps, reward_ex -200.00, reward_in 44.68\n",
      " 78 Episode in 15600 steps, reward_ex -200.00, reward_in 14.20\n",
      " 79 Episode in 15800 steps, reward_ex -200.00, reward_in 146.71\n",
      " 80 Episode in 16000 steps, reward_ex -200.00, reward_in 76.38\n",
      " 81 Episode in 16200 steps, reward_ex -200.00, reward_in 9.60\n",
      " 82 Episode in 16400 steps, reward_ex -200.00, reward_in 61.97\n",
      " 83 Episode in 16600 steps, reward_ex -200.00, reward_in 15.97\n",
      " 84 Episode in 16800 steps, reward_ex -200.00, reward_in 81.02\n",
      " 85 Episode in 17000 steps, reward_ex -200.00, reward_in 29.07\n",
      " 86 Episode in 17200 steps, reward_ex -200.00, reward_in 9.62\n",
      " 87 Episode in 17400 steps, reward_ex -200.00, reward_in 28.89\n",
      " 88 Episode in 17600 steps, reward_ex -200.00, reward_in 10.64\n",
      " 89 Episode in 17800 steps, reward_ex -200.00, reward_in 18.17\n",
      " 90 Episode in 18000 steps, reward_ex -200.00, reward_in 11.39\n",
      " 91 Episode in 18200 steps, reward_ex -200.00, reward_in 9.52\n",
      " 92 Episode in 18400 steps, reward_ex -200.00, reward_in 12.53\n",
      " 93 Episode in 18600 steps, reward_ex -200.00, reward_in 9.83\n",
      " 94 Episode in 18800 steps, reward_ex -200.00, reward_in 7.84\n",
      " 95 Episode in 19000 steps, reward_ex -200.00, reward_in 20.26\n",
      " 96 Episode in 19200 steps, reward_ex -200.00, reward_in 11.98\n",
      " 97 Episode in 19400 steps, reward_ex -200.00, reward_in 6.61\n",
      " 98 Episode in 19600 steps, reward_ex -200.00, reward_in 83.46\n",
      " 99 Episode in 19800 steps, reward_ex -200.00, reward_in 49.14\n",
      "100 Episode in 20000 steps, reward_ex -200.00, reward_in 14.62\n",
      "101 Episode in 20200 steps, reward_ex -200.00, reward_in 8.06\n",
      "102 Episode in 20400 steps, reward_ex -200.00, reward_in 50.45\n",
      "103 Episode in 20600 steps, reward_ex -200.00, reward_in 21.82\n",
      "104 Episode in 20800 steps, reward_ex -200.00, reward_in 18.35\n",
      "105 Episode in 21000 steps, reward_ex -200.00, reward_in 8.64\n",
      "106 Episode in 21200 steps, reward_ex -200.00, reward_in 8.25\n",
      "107 Episode in 21400 steps, reward_ex -200.00, reward_in 20.73\n",
      "108 Episode in 21600 steps, reward_ex -200.00, reward_in 18.20\n",
      "109 Episode in 21800 steps, reward_ex -200.00, reward_in 9.48\n",
      "110 Episode in 22000 steps, reward_ex -200.00, reward_in 15.10\n",
      "111 Episode in 22200 steps, reward_ex -200.00, reward_in 9.61\n",
      "112 Episode in 22400 steps, reward_ex -200.00, reward_in 7.64\n",
      "113 Episode in 22600 steps, reward_ex -200.00, reward_in 20.60\n",
      "114 Episode in 22800 steps, reward_ex -200.00, reward_in 34.19\n",
      "115 Episode in 23000 steps, reward_ex -200.00, reward_in 13.78\n",
      "116 Episode in 23200 steps, reward_ex -200.00, reward_in 8.07\n",
      "117 Episode in 23400 steps, reward_ex -200.00, reward_in 10.86\n",
      "118 Episode in 23600 steps, reward_ex -200.00, reward_in 12.29\n",
      "119 Episode in 23800 steps, reward_ex -200.00, reward_in 8.18\n",
      "120 Episode in 24000 steps, reward_ex -200.00, reward_in 25.50\n",
      "121 Episode in 24200 steps, reward_ex -200.00, reward_in 34.64\n",
      "122 Episode in 24400 steps, reward_ex -200.00, reward_in 8.21\n",
      "123 Episode in 24600 steps, reward_ex -200.00, reward_in 6.44\n",
      "124 Episode in 24800 steps, reward_ex -200.00, reward_in 12.80\n",
      "125 Episode in 25000 steps, reward_ex -200.00, reward_in 6.93\n",
      "126 Episode in 25200 steps, reward_ex -200.00, reward_in 6.13\n",
      "127 Episode in 25400 steps, reward_ex -200.00, reward_in 19.37\n",
      "128 Episode in 25600 steps, reward_ex -200.00, reward_in 79.97\n"
     ]
    }
   ],
   "source": [
    "# play!\n",
    "for i in range(1, n_episodes + 1):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    ep_rew_ex = 0.\n",
    "    ep_rew_in = 0.\n",
    "    while not done:\n",
    "#         env.render()\n",
    "\n",
    "        action, val_ex, val_in = get_action_and_value(obs, net)\n",
    "        _obs, rew_ex, done, _ = env.step(action)\n",
    "        \n",
    "        rew_in = calculate_reward_in(pred_net, rand_net, _obs)    \n",
    "        \n",
    "        # store\n",
    "        roll_memory.append([obs, action, _obs])\n",
    "        obs_memory.append(_obs)\n",
    "        rews_ex.append(EX_COEF * rew_ex)\n",
    "        rews_in.append(IN_COEF * rew_in)\n",
    "        vals_ex.append(val_ex)\n",
    "        vals_in.append(val_in)\n",
    "        \n",
    "        obs = _obs\n",
    "        steps += 1\n",
    "        ep_rew_ex += rew_ex\n",
    "        ep_rew_in += rew_in\n",
    "        \n",
    "        if done or steps % roll_len == 0:\n",
    "            if done:\n",
    "                _val_ex = 0.\n",
    "                _val_in = 0.\n",
    "            else:\n",
    "                _, _val_ex, _val_in = get_action_and_value(_obs, net)\n",
    "            \n",
    "            vals_ex.append(_val_ex)\n",
    "            vals_in.append(_val_in)\n",
    "            train_memory.extend(\n",
    "                compute_adv_with_gae(rews_ex, rews_in, vals_ex, vals_in, roll_memory)\n",
    "            )\n",
    "            rews_ex.clear()\n",
    "            rews_in.clear()\n",
    "            vals_ex.clear()\n",
    "            vals_in.clear()\n",
    "            roll_memory.clear()\n",
    "            \n",
    "        if steps % roll_len == 0:\n",
    "            net_memory.appendleft(net.state_dict())\n",
    "            old_net.load_state_dict(net_memory.pop())\n",
    "            learn(net, old_net, pred_net, rand_net, net_optim, pred_optim, train_memory)\n",
    "            train_memory.clear()\n",
    "            learn_steps += 1\n",
    "        \n",
    "        if steps % roll_len * 50 == 0:\n",
    "            mean, std = get_norm_params(obs_memory)\n",
    "            obs_memory.clear()\n",
    "    \n",
    "    if done:        \n",
    "        ep_rewards.append(ep_rew_ex)\n",
    "        print('{:3} Episode in {:5} steps, '\n",
    "              'reward_ex {:.2f}, reward_in {:.2f}'.format(i, steps, ep_rew_ex, ep_rew_in))\n",
    "        \n",
    "        if env.spec.id == 'MountainCar-v0' and ep_rew_ex > -200:\n",
    "            print('Wow!')\n",
    "        \n",
    "        if len(ep_rewards) >= n_eval:\n",
    "            if np.mean(list(reversed(ep_rewards))[: n_eval]) >= env.spec.reward_threshold:\n",
    "                print('\\n{} is sloved! '\n",
    "                      '{:3} Episode in {:3} steps'.format(env.spec.id, i, steps))\n",
    "                torch.save(net.state_dict(),\n",
    "                           f'../test/saved_models/{env.spec.id}_ep{i}_clear_model_ppo_r.pt')\n",
    "                break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "plt.title('Reward')\n",
    "plt.plot(ep_rewards)\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.title('Loss')\n",
    "plt.plot(losses)\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.title('m_Loss')\n",
    "plt.plot(m_losses)\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.title('f_Loss')\n",
    "plt.plot(f_losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[\n",
    "    ('CartPole-v0', 161, 32, 256),\n",
    "    ('CartPole-v1', 162, 32, 256),\n",
    "    ('MountainCar-v0', 660),\n",
    "    ('LunarLander-v2', 260)\n",
    "]"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "C51_tensorflow.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
