{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IWnm3qot3o1W"
   },
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "from copy import deepcopy\n",
    "\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 708,
     "status": "ok",
     "timestamp": 1534482400648,
     "user": {
      "displayName": "윤승제",
      "photoUrl": "//lh5.googleusercontent.com/-EucKC7DmcQI/AAAAAAAAAAI/AAAAAAAAAGA/gQU1NPEmNFA/s50-c-k-no/photo.jpg",
      "userId": "105654037995838004821"
     },
     "user_tz": -540
    },
    "id": "maRVADiTlzHD",
    "outputId": "783b7610-95c2-4b54-b2ce-d8e853c484ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda: True\n"
     ]
    }
   ],
   "source": [
    "SEED = 5\n",
    "BATCH_SIZE = 4\n",
    "LR = 0.00025\n",
    "EPOCHS = 4\n",
    "CLIP = 0.1\n",
    "GAMMA = 0.99\n",
    "LAMBDA = 0.95\n",
    "ENT_COEF = 0.01\n",
    "EX_COEF = 0.0\n",
    "IN_COEF = 1.0\n",
    "\n",
    "# set device\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print('cuda:', use_cuda)\n",
    "device = torch.device('cuda' if use_cuda else 'cpu')\n",
    "\n",
    "# random seed\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if use_cuda:\n",
    "    torch.cuda.manual_seed_all(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9Ffkl_5C4R81"
   },
   "outputs": [],
   "source": [
    "class ActorCriticNet(nn.Module):\n",
    "    def __init__(self, obs_space, action_space):\n",
    "        super().__init__()\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(obs_space, 256),\n",
    "            nn.SELU()\n",
    "        )\n",
    "\n",
    "        self.pol = nn.Sequential(\n",
    "            nn.Linear(256, 256),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(256, action_space)\n",
    "        )\n",
    "        \n",
    "        self.val_ex = nn.Sequential(\n",
    "            nn.Linear(256, 256),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "        \n",
    "        self.val_in = nn.Sequential(\n",
    "            nn.Linear(256, 256),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "        \n",
    "        self.log_softmax = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.head(x)\n",
    "        logit = self.pol(out).reshape(out.shape[0], -1)\n",
    "        value_ex = self.val_ex(out).reshape(out.shape[0], 1)\n",
    "        value_in = self.val_in(out).reshape(out.shape[0], 1)\n",
    "        log_probs = self.log_softmax(logit)\n",
    "        \n",
    "        return log_probs, value_ex, value_in\n",
    "    \n",
    "class RandomNet(nn.Module):\n",
    "    def __init__(self, obs_space):\n",
    "        super().__init__()\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(obs_space, 256),\n",
    "            nn.SELU()\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(256, 512)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.head(x)\n",
    "        obs_feature = self.fc(out).reshape(out.shape[0], -1)\n",
    "\n",
    "        return obs_feature\n",
    "\n",
    "\n",
    "class PredictNet(nn.Module):\n",
    "    def __init__(self, obs_space):\n",
    "        super().__init__()\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(obs_space, 256),\n",
    "            nn.SELU()\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(256, 256),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(256, 512)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.head(x)\n",
    "        obs_feature = self.fc(out).reshape(out.shape[0], -1)\n",
    "\n",
    "        return obs_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "m_losses = []\n",
    "f_losses = []\n",
    "\n",
    "\n",
    "def learn(net, old_net, pred_net, rand_net, net_optim, pred_optim, train_memory):\n",
    "    net.train()\n",
    "    old_net.train()\n",
    "    \n",
    "    dataloader = DataLoader(\n",
    "        train_memory,\n",
    "        shuffle=True,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        pin_memory=use_cuda\n",
    "    )\n",
    "\n",
    "    for i in range(EPOCHS):\n",
    "        for (s, a, _s, ret_ex, ret_in, adv) in dataloader:\n",
    "            s = s.to(device).float()\n",
    "            a  = a.to(device).long()\n",
    "            _s = _s.to(device).float()\n",
    "            _s_norm_np = normalize_obs(_s.detach().cpu().numpy())\n",
    "            _s_norm = torch.tensor(_s_norm_np).to(device).float()\n",
    "            ret_ex = ret_ex.to(device).float()\n",
    "            ret_in = ret_in.to(device).float()\n",
    "            adv = adv.to(device).float()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                rand_f = rand_net(_s_norm)\n",
    "                log_p_old, v_ex_old, v_in_old = old_net(s)\n",
    "                log_p_a_old = log_p_old[range(BATCH_SIZE), a]\n",
    "\n",
    "            pred_f = pred_net(_s_norm)\n",
    "            log_p, v_ex, v_in = net(s)\n",
    "            log_p_a = log_p[range(BATCH_SIZE), a]\n",
    "            p_ratio = (log_p_a - log_p_a_old).exp()\n",
    "            p_r_clip = torch.clamp(p_ratio, 1. - CLIP, 1. + CLIP)\n",
    "            p_loss = torch.min(p_ratio * adv, p_r_clip * adv).mean()\n",
    "            v_ex_loss = 0.5 * (ret_ex - v_ex).pow(2)\n",
    "            v_in_loss = 0.5 * (ret_in - v_in).pow(2)\n",
    "            v_loss = (v_ex_loss + v_in_loss).mean() \n",
    "            entropy = -(log_p.exp() * log_p).sum(dim=1).mean()\n",
    "\n",
    "            # loss\n",
    "            m_loss = -(p_loss - v_loss + ENT_COEF * entropy)\n",
    "            m_losses.append(m_loss)\n",
    "\n",
    "            f_loss = (pred_f - rand_f).pow(2).sum(dim=1).mean()\n",
    "            f_losses.append(f_loss)\n",
    "            \n",
    "            loss = m_loss + f_loss\n",
    "            losses.append(loss)\n",
    "            \n",
    "            net_optim.zero_grad()\n",
    "            pred_optim.zero_grad()\n",
    "            loss.backward()\n",
    "            net_optim.step()\n",
    "            pred_optim.step()\n",
    "\n",
    "\n",
    "def get_action_and_value(obs, net):\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        state = torch.tensor([obs]).to(device).float()\n",
    "        log_p, v_ex, v_in = net(state)\n",
    "        m = Categorical(log_p.exp())\n",
    "        action = m.sample()\n",
    "\n",
    "    return action.item(), v_ex.item(), v_in.item()\n",
    "\n",
    "\n",
    "def compute_adv_with_gae(reward_ex, reward_in, values, roll_memory):\n",
    "    rew_ex = np.array(rewards_ex, 'float')\n",
    "    rew_in = np.array(rewards_in, 'float')\n",
    "    rew = rew_ex + rew_in\n",
    "    val = np.array(values[:-1], 'float')\n",
    "    _val = np.array(values[1:], 'float')\n",
    "    delta = rew + GAMMA * _val - val\n",
    "    dis_r_ex = np.array([GAMMA**(i) * r for i, r in enumerate(reward_ex)], 'float')\n",
    "    dis_r_in = np.array([GAMMA**(i) * r for i, r in enumerate(reward_in)], 'float')\n",
    "    gae_dt = np.array([(GAMMA * LAMBDA)**(i) * dt for i, dt in enumerate(delta.tolist())], 'float')\n",
    "    for i, data in enumerate(roll_memory):\n",
    "        data.append(sum(dis_r_ex[i:] / GAMMA**(i)))\n",
    "        data.append(sum(dis_r_in[i:] / GAMMA**(i)))\n",
    "        data.append(sum(gae_dt[i:] / (GAMMA * LAMBDA)**(i)))\n",
    "\n",
    "    return roll_memory\n",
    "\n",
    "\n",
    "def get_norm_params(obs_memory):\n",
    "    global obs_apace\n",
    "\n",
    "    obses = [[] for _ in range(obs_space)]\n",
    "    for obs in obs_memory:\n",
    "        for j in range(obs_space):\n",
    "            obses[j].append(obs[j])\n",
    "\n",
    "    mean = np.zeros(obs_space, 'float')\n",
    "    std = np.zeros(obs_space, 'float')\n",
    "    for i, obs_ in enumerate(obses):\n",
    "        mean[i] = np.mean(obs_)\n",
    "        std[i] = np.std(obs_)\n",
    "\n",
    "    return mean, std\n",
    "\n",
    "\n",
    "def normalize_obs(obs):\n",
    "    global mean, std\n",
    "    norm_obs = (obs - mean) / std\n",
    "#     return np.clip(norm_obs, -5, 5)\n",
    "    return norm_obs\n",
    "\n",
    "\n",
    "def calculate_reward_in(pred_net, rand_net, obs):\n",
    "    norm_obs = normalize_obs(obs)\n",
    "    state = torch.tensor([norm_obs]).to(device).float()\n",
    "    with torch.no_grad():\n",
    "        pred_obs = pred_net(state)\n",
    "        rand_obs = rand_net(state)\n",
    "        reward = (pred_obs - rand_obs).pow(2).sum()\n",
    "        clipped_reward = torch.clamp(reward, -1, 1)\n",
    "\n",
    "    return clipped_reward.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 3377
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 135196,
     "status": "ok",
     "timestamp": 1534482559393,
     "user": {
      "displayName": "윤승제",
      "photoUrl": "//lh5.googleusercontent.com/-EucKC7DmcQI/AAAAAAAAAAI/AAAAAAAAAGA/gQU1NPEmNFA/s50-c-k-no/photo.jpg",
      "userId": "105654037995838004821"
     },
     "user_tz": -540
    },
    "id": "PnifSBJglzHh",
    "outputId": "94177345-918e-4a96-d9a8-d8aba0a4bc9a",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# make an environment\n",
    "# env = gym.make('CartPole-v0')\n",
    "env = gym.make('CartPole-v1')\n",
    "# env = gym.make('MountainCar-v0')\n",
    "# env = gym.make('LunarLander-v2')\n",
    "\n",
    "env.seed(SEED)\n",
    "obs_space = env.observation_space.shape[0]\n",
    "action_space = env.action_space.n\n",
    "\n",
    "# hyperparameter\n",
    "n_episodes = 3000\n",
    "roll_len = 128\n",
    "n_eval = env.spec.trials\n",
    "\n",
    "# global values\n",
    "init_steps = 0\n",
    "steps = 0\n",
    "learn_steps = 0\n",
    "mean = 0.\n",
    "std = 0.\n",
    "ep_rewards = []\n",
    "is_rollout = False\n",
    "is_solved = False\n",
    "is_init_roll = True\n",
    "\n",
    "# make a rollout memory\n",
    "net_memory = deque(maxlen=2)\n",
    "train_memory = []\n",
    "roll_memory = []\n",
    "obs_memory = []\n",
    "rewards_ex = []\n",
    "rewards_in = []\n",
    "values = []\n",
    "\n",
    "# make nerual networks\n",
    "net = ActorCriticNet(obs_space, action_space).to(device)\n",
    "old_net = deepcopy(net)\n",
    "net_memory.appendleft(net.state_dict())\n",
    "pred_net = PredictNet(obs_space).to(device)\n",
    "rand_net = RandomNet(obs_space).to(device)\n",
    "\n",
    "# make optimizer\n",
    "net_optim = torch.optim.Adam(net.parameters(), lr=LR, eps=1e-5)\n",
    "pred_optim = torch.optim.Adam(pred_net.parameters(), lr=LR, eps=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "use_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.spec.max_episode_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.spec.trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "475.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.spec.reward_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 3377
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 135196,
     "status": "ok",
     "timestamp": 1534482559393,
     "user": {
      "displayName": "윤승제",
      "photoUrl": "//lh5.googleusercontent.com/-EucKC7DmcQI/AAAAAAAAAAI/AAAAAAAAAGA/gQU1NPEmNFA/s50-c-k-no/photo.jpg",
      "userId": "105654037995838004821"
     },
     "user_tz": -540
    },
    "id": "PnifSBJglzHh",
    "outputId": "94177345-918e-4a96-d9a8-d8aba0a4bc9a"
   },
   "outputs": [],
   "source": [
    "# simulation\n",
    "while True:\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "#         env.render()\n",
    "        action = env.action_space.sample()\n",
    "        _obs, _, done, _ = env.step(action)\n",
    "        obs_memory.append(_obs)\n",
    "        obs = _obs\n",
    "        init_steps += 1\n",
    "        if init_steps == roll_len * 50:\n",
    "            mean, std = get_norm_params(obs_memory)\n",
    "            obs_memory.clear()\n",
    "            is_init_roll = False\n",
    "            break\n",
    "    if not is_init_roll:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 3377
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 135196,
     "status": "ok",
     "timestamp": 1534482559393,
     "user": {
      "displayName": "윤승제",
      "photoUrl": "//lh5.googleusercontent.com/-EucKC7DmcQI/AAAAAAAAAAI/AAAAAAAAAGA/gQU1NPEmNFA/s50-c-k-no/photo.jpg",
      "userId": "105654037995838004821"
     },
     "user_tz": -540
    },
    "id": "PnifSBJglzHh",
    "outputId": "94177345-918e-4a96-d9a8-d8aba0a4bc9a",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1 Episode in    10 steps, reward_ex 10.00, reward_in 10.00\n",
      "  2 Episode in    35 steps, reward_ex 25.00, reward_in 25.00\n",
      "  3 Episode in    46 steps, reward_ex 11.00, reward_in 11.00\n",
      "  4 Episode in    58 steps, reward_ex 12.00, reward_in 12.00\n",
      "  5 Episode in    81 steps, reward_ex 23.00, reward_in 23.00\n",
      "  6 Episode in   103 steps, reward_ex 22.00, reward_in 22.00\n",
      "  7 Episode in   116 steps, reward_ex 13.00, reward_in 13.00\n",
      "  8 Episode in   128 steps, reward_ex 12.00, reward_in 12.00\n",
      "  9 Episode in   140 steps, reward_ex 12.00, reward_in 12.00\n",
      " 10 Episode in   157 steps, reward_ex 17.00, reward_in 17.00\n",
      " 11 Episode in   211 steps, reward_ex 54.00, reward_in 54.00\n",
      " 12 Episode in   236 steps, reward_ex 25.00, reward_in 24.08\n",
      " 13 Episode in   255 steps, reward_ex 19.00, reward_in 19.00\n",
      " 14 Episode in   268 steps, reward_ex 13.00, reward_in 12.84\n",
      " 15 Episode in   278 steps, reward_ex 10.00, reward_in 9.81\n",
      " 16 Episode in   291 steps, reward_ex 13.00, reward_in 11.69\n",
      " 17 Episode in   301 steps, reward_ex 10.00, reward_in 10.00\n",
      " 18 Episode in   312 steps, reward_ex 11.00, reward_in 11.00\n",
      " 19 Episode in   338 steps, reward_ex 26.00, reward_in 21.82\n",
      " 20 Episode in   355 steps, reward_ex 17.00, reward_in 14.84\n",
      " 21 Episode in   370 steps, reward_ex 15.00, reward_in 13.22\n",
      " 22 Episode in   390 steps, reward_ex 20.00, reward_in 15.85\n",
      " 23 Episode in   401 steps, reward_ex 11.00, reward_in 10.71\n",
      " 24 Episode in   415 steps, reward_ex 14.00, reward_in 11.90\n",
      " 25 Episode in   429 steps, reward_ex 14.00, reward_in 9.10\n",
      " 26 Episode in   442 steps, reward_ex 13.00, reward_in 8.98\n",
      " 27 Episode in   456 steps, reward_ex 14.00, reward_in 13.60\n",
      " 28 Episode in   466 steps, reward_ex 10.00, reward_in 7.29\n",
      " 29 Episode in   482 steps, reward_ex 16.00, reward_in 10.78\n",
      " 30 Episode in   498 steps, reward_ex 16.00, reward_in 15.98\n",
      " 31 Episode in   513 steps, reward_ex 15.00, reward_in 14.41\n",
      " 32 Episode in   536 steps, reward_ex 23.00, reward_in 17.06\n",
      " 33 Episode in   545 steps, reward_ex 9.00, reward_in 4.18\n",
      " 34 Episode in   563 steps, reward_ex 18.00, reward_in 17.62\n",
      " 35 Episode in   572 steps, reward_ex 9.00, reward_in 3.22\n",
      " 36 Episode in   609 steps, reward_ex 37.00, reward_in 28.78\n",
      " 37 Episode in   632 steps, reward_ex 23.00, reward_in 9.61\n",
      " 38 Episode in   642 steps, reward_ex 10.00, reward_in 6.98\n",
      " 39 Episode in   675 steps, reward_ex 33.00, reward_in 21.68\n",
      " 40 Episode in   701 steps, reward_ex 26.00, reward_in 14.99\n",
      " 41 Episode in   737 steps, reward_ex 36.00, reward_in 30.52\n",
      " 42 Episode in   755 steps, reward_ex 18.00, reward_in 7.59\n",
      " 43 Episode in   766 steps, reward_ex 11.00, reward_in 7.25\n",
      " 44 Episode in   793 steps, reward_ex 27.00, reward_in 15.26\n",
      " 45 Episode in   823 steps, reward_ex 30.00, reward_in 23.58\n",
      " 46 Episode in   854 steps, reward_ex 31.00, reward_in 15.44\n",
      " 47 Episode in   864 steps, reward_ex 10.00, reward_in 6.44\n",
      " 48 Episode in   887 steps, reward_ex 23.00, reward_in 22.12\n",
      " 49 Episode in   930 steps, reward_ex 43.00, reward_in 41.07\n",
      " 50 Episode in   950 steps, reward_ex 20.00, reward_in 12.23\n",
      " 51 Episode in   991 steps, reward_ex 41.00, reward_in 18.55\n",
      " 52 Episode in  1027 steps, reward_ex 36.00, reward_in 32.15\n",
      " 53 Episode in  1086 steps, reward_ex 59.00, reward_in 29.35\n",
      " 54 Episode in  1119 steps, reward_ex 33.00, reward_in 19.77\n",
      " 55 Episode in  1141 steps, reward_ex 22.00, reward_in 10.18\n",
      " 56 Episode in  1164 steps, reward_ex 23.00, reward_in 13.76\n",
      " 57 Episode in  1194 steps, reward_ex 30.00, reward_in 22.47\n",
      " 58 Episode in  1235 steps, reward_ex 41.00, reward_in 22.69\n",
      " 59 Episode in  1266 steps, reward_ex 31.00, reward_in 23.12\n",
      " 60 Episode in  1321 steps, reward_ex 55.00, reward_in 18.54\n",
      " 61 Episode in  1356 steps, reward_ex 35.00, reward_in 26.73\n",
      " 62 Episode in  1382 steps, reward_ex 26.00, reward_in 11.79\n",
      " 63 Episode in  1430 steps, reward_ex 48.00, reward_in 21.89\n",
      " 64 Episode in  1447 steps, reward_ex 17.00, reward_in 7.83\n",
      " 65 Episode in  1480 steps, reward_ex 33.00, reward_in 23.76\n",
      " 66 Episode in  1524 steps, reward_ex 44.00, reward_in 19.88\n",
      " 67 Episode in  1617 steps, reward_ex 93.00, reward_in 64.46\n",
      " 68 Episode in  1686 steps, reward_ex 69.00, reward_in 65.40\n",
      " 69 Episode in  1841 steps, reward_ex 155.00, reward_in 128.69\n",
      " 70 Episode in  1898 steps, reward_ex 57.00, reward_in 31.66\n",
      " 71 Episode in  1943 steps, reward_ex 45.00, reward_in 22.49\n",
      " 72 Episode in  2164 steps, reward_ex 221.00, reward_in 143.40\n",
      " 73 Episode in  2433 steps, reward_ex 269.00, reward_in 260.25\n",
      " 74 Episode in  2649 steps, reward_ex 216.00, reward_in 216.00\n",
      " 75 Episode in  2854 steps, reward_ex 205.00, reward_in 156.14\n",
      " 76 Episode in  3082 steps, reward_ex 228.00, reward_in 148.80\n",
      " 77 Episode in  3328 steps, reward_ex 246.00, reward_in 223.01\n",
      " 78 Episode in  3559 steps, reward_ex 231.00, reward_in 211.37\n",
      " 79 Episode in  3790 steps, reward_ex 231.00, reward_in 149.28\n",
      " 80 Episode in  4197 steps, reward_ex 407.00, reward_in 361.08\n",
      " 81 Episode in  4494 steps, reward_ex 297.00, reward_in 245.28\n",
      " 82 Episode in  4782 steps, reward_ex 288.00, reward_in 285.83\n",
      " 83 Episode in  5040 steps, reward_ex 258.00, reward_in 226.83\n",
      " 84 Episode in  5321 steps, reward_ex 281.00, reward_in 234.02\n",
      " 85 Episode in  5584 steps, reward_ex 263.00, reward_in 220.80\n",
      " 86 Episode in  5789 steps, reward_ex 205.00, reward_in 141.19\n",
      " 87 Episode in  6026 steps, reward_ex 237.00, reward_in 159.29\n",
      " 88 Episode in  6229 steps, reward_ex 203.00, reward_in 137.04\n",
      " 89 Episode in  6463 steps, reward_ex 234.00, reward_in 98.78\n",
      " 90 Episode in  6651 steps, reward_ex 188.00, reward_in 104.49\n",
      " 91 Episode in  6815 steps, reward_ex 164.00, reward_in 70.27\n",
      " 92 Episode in  7046 steps, reward_ex 231.00, reward_in 165.27\n",
      " 93 Episode in  7229 steps, reward_ex 183.00, reward_in 133.53\n",
      " 94 Episode in  7570 steps, reward_ex 341.00, reward_in 219.41\n",
      " 95 Episode in  7801 steps, reward_ex 231.00, reward_in 185.47\n",
      " 96 Episode in  7998 steps, reward_ex 197.00, reward_in 105.74\n",
      " 97 Episode in  8149 steps, reward_ex 151.00, reward_in 57.10\n",
      " 98 Episode in  8306 steps, reward_ex 157.00, reward_in 35.08\n",
      " 99 Episode in  8453 steps, reward_ex 147.00, reward_in 16.83\n",
      "100 Episode in  8659 steps, reward_ex 206.00, reward_in 48.70\n",
      "101 Episode in  8781 steps, reward_ex 122.00, reward_in 43.22\n",
      "102 Episode in  8941 steps, reward_ex 160.00, reward_in 30.97\n",
      "103 Episode in  9074 steps, reward_ex 133.00, reward_in 14.13\n",
      "104 Episode in  9237 steps, reward_ex 163.00, reward_in 28.98\n",
      "105 Episode in  9439 steps, reward_ex 202.00, reward_in 54.85\n",
      "106 Episode in  9584 steps, reward_ex 145.00, reward_in 16.06\n",
      "107 Episode in  9790 steps, reward_ex 206.00, reward_in 75.30\n",
      "108 Episode in  9915 steps, reward_ex 125.00, reward_in 77.91\n",
      "109 Episode in 10069 steps, reward_ex 154.00, reward_in 74.74\n",
      "110 Episode in 10266 steps, reward_ex 197.00, reward_in 82.23\n",
      "111 Episode in 10460 steps, reward_ex 194.00, reward_in 168.73\n",
      "112 Episode in 10596 steps, reward_ex 136.00, reward_in 43.24\n",
      "113 Episode in 10744 steps, reward_ex 148.00, reward_in 34.10\n",
      "114 Episode in 10907 steps, reward_ex 163.00, reward_in 28.38\n",
      "115 Episode in 11046 steps, reward_ex 139.00, reward_in 16.25\n",
      "116 Episode in 11197 steps, reward_ex 151.00, reward_in 15.47\n",
      "117 Episode in 11365 steps, reward_ex 168.00, reward_in 23.97\n",
      "118 Episode in 11501 steps, reward_ex 136.00, reward_in 17.19\n",
      "119 Episode in 11682 steps, reward_ex 181.00, reward_in 113.50\n",
      "120 Episode in 11807 steps, reward_ex 125.00, reward_in 63.84\n",
      "121 Episode in 11929 steps, reward_ex 122.00, reward_in 31.06\n",
      "122 Episode in 12040 steps, reward_ex 111.00, reward_in 17.20\n",
      "123 Episode in 12162 steps, reward_ex 122.00, reward_in 22.95\n",
      "124 Episode in 12291 steps, reward_ex 129.00, reward_in 39.25\n",
      "125 Episode in 12434 steps, reward_ex 143.00, reward_in 14.75\n",
      "126 Episode in 12570 steps, reward_ex 136.00, reward_in 20.22\n",
      "127 Episode in 12700 steps, reward_ex 130.00, reward_in 28.32\n",
      "128 Episode in 12836 steps, reward_ex 136.00, reward_in 20.62\n",
      "129 Episode in 12965 steps, reward_ex 129.00, reward_in 49.63\n",
      "130 Episode in 13138 steps, reward_ex 173.00, reward_in 53.80\n",
      "131 Episode in 13334 steps, reward_ex 196.00, reward_in 52.25\n",
      "132 Episode in 13477 steps, reward_ex 143.00, reward_in 100.02\n",
      "133 Episode in 13627 steps, reward_ex 150.00, reward_in 54.76\n",
      "134 Episode in 13766 steps, reward_ex 139.00, reward_in 57.32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135 Episode in 13940 steps, reward_ex 174.00, reward_in 58.66\n",
      "136 Episode in 14099 steps, reward_ex 159.00, reward_in 39.20\n",
      "137 Episode in 14240 steps, reward_ex 141.00, reward_in 53.28\n",
      "138 Episode in 14395 steps, reward_ex 155.00, reward_in 45.39\n",
      "139 Episode in 14565 steps, reward_ex 170.00, reward_in 19.31\n",
      "140 Episode in 14708 steps, reward_ex 143.00, reward_in 18.79\n",
      "141 Episode in 14878 steps, reward_ex 170.00, reward_in 51.32\n",
      "142 Episode in 15019 steps, reward_ex 141.00, reward_in 21.22\n",
      "143 Episode in 15182 steps, reward_ex 163.00, reward_in 30.44\n",
      "144 Episode in 15352 steps, reward_ex 170.00, reward_in 84.85\n",
      "145 Episode in 15487 steps, reward_ex 135.00, reward_in 55.20\n",
      "146 Episode in 15620 steps, reward_ex 133.00, reward_in 42.82\n",
      "147 Episode in 15772 steps, reward_ex 152.00, reward_in 30.58\n",
      "148 Episode in 15940 steps, reward_ex 168.00, reward_in 37.70\n",
      "149 Episode in 16112 steps, reward_ex 172.00, reward_in 28.59\n",
      "150 Episode in 16261 steps, reward_ex 149.00, reward_in 25.54\n",
      "151 Episode in 16423 steps, reward_ex 162.00, reward_in 52.98\n",
      "152 Episode in 16594 steps, reward_ex 171.00, reward_in 52.53\n",
      "153 Episode in 16750 steps, reward_ex 156.00, reward_in 16.33\n",
      "154 Episode in 16932 steps, reward_ex 182.00, reward_in 63.12\n",
      "155 Episode in 17095 steps, reward_ex 163.00, reward_in 48.68\n",
      "156 Episode in 17274 steps, reward_ex 179.00, reward_in 23.24\n",
      "157 Episode in 17425 steps, reward_ex 151.00, reward_in 27.54\n",
      "158 Episode in 17580 steps, reward_ex 155.00, reward_in 45.25\n",
      "159 Episode in 17724 steps, reward_ex 144.00, reward_in 20.29\n",
      "160 Episode in 17863 steps, reward_ex 139.00, reward_in 34.15\n",
      "161 Episode in 18034 steps, reward_ex 171.00, reward_in 33.72\n",
      "162 Episode in 18215 steps, reward_ex 181.00, reward_in 75.49\n",
      "163 Episode in 18360 steps, reward_ex 145.00, reward_in 33.18\n",
      "164 Episode in 18495 steps, reward_ex 135.00, reward_in 14.29\n",
      "165 Episode in 18621 steps, reward_ex 126.00, reward_in 17.22\n",
      "166 Episode in 18765 steps, reward_ex 144.00, reward_in 27.01\n",
      "167 Episode in 18903 steps, reward_ex 138.00, reward_in 14.31\n",
      "168 Episode in 19067 steps, reward_ex 164.00, reward_in 20.12\n",
      "169 Episode in 19226 steps, reward_ex 159.00, reward_in 62.45\n",
      "170 Episode in 19362 steps, reward_ex 136.00, reward_in 21.39\n",
      "171 Episode in 19499 steps, reward_ex 137.00, reward_in 16.37\n",
      "172 Episode in 19627 steps, reward_ex 128.00, reward_in 12.59\n",
      "173 Episode in 19747 steps, reward_ex 120.00, reward_in 9.91\n",
      "174 Episode in 19875 steps, reward_ex 128.00, reward_in 21.35\n",
      "175 Episode in 20005 steps, reward_ex 130.00, reward_in 21.89\n",
      "176 Episode in 20135 steps, reward_ex 130.00, reward_in 15.12\n",
      "177 Episode in 20259 steps, reward_ex 124.00, reward_in 6.87\n",
      "178 Episode in 20430 steps, reward_ex 171.00, reward_in 53.60\n",
      "179 Episode in 20578 steps, reward_ex 148.00, reward_in 31.19\n",
      "180 Episode in 20727 steps, reward_ex 149.00, reward_in 22.82\n",
      "181 Episode in 20873 steps, reward_ex 146.00, reward_in 37.11\n",
      "182 Episode in 21011 steps, reward_ex 138.00, reward_in 21.11\n",
      "183 Episode in 21172 steps, reward_ex 161.00, reward_in 20.77\n",
      "184 Episode in 21322 steps, reward_ex 150.00, reward_in 13.95\n",
      "185 Episode in 21474 steps, reward_ex 152.00, reward_in 11.11\n",
      "186 Episode in 21629 steps, reward_ex 155.00, reward_in 8.67\n",
      "187 Episode in 21784 steps, reward_ex 155.00, reward_in 38.16\n",
      "188 Episode in 21951 steps, reward_ex 167.00, reward_in 62.58\n",
      "189 Episode in 22087 steps, reward_ex 136.00, reward_in 28.05\n",
      "190 Episode in 22248 steps, reward_ex 161.00, reward_in 42.41\n",
      "191 Episode in 22406 steps, reward_ex 158.00, reward_in 25.17\n",
      "192 Episode in 22581 steps, reward_ex 175.00, reward_in 48.37\n",
      "193 Episode in 22865 steps, reward_ex 284.00, reward_in 116.25\n",
      "194 Episode in 23015 steps, reward_ex 150.00, reward_in 100.73\n",
      "195 Episode in 23189 steps, reward_ex 174.00, reward_in 48.10\n",
      "196 Episode in 23343 steps, reward_ex 154.00, reward_in 15.96\n",
      "197 Episode in 23586 steps, reward_ex 243.00, reward_in 74.66\n",
      "198 Episode in 23731 steps, reward_ex 145.00, reward_in 140.16\n",
      "199 Episode in 23926 steps, reward_ex 195.00, reward_in 98.99\n",
      "200 Episode in 24141 steps, reward_ex 215.00, reward_in 113.63\n",
      "201 Episode in 24331 steps, reward_ex 190.00, reward_in 143.14\n",
      "202 Episode in 24500 steps, reward_ex 169.00, reward_in 68.25\n",
      "203 Episode in 24869 steps, reward_ex 369.00, reward_in 226.60\n",
      "204 Episode in 25042 steps, reward_ex 173.00, reward_in 132.49\n",
      "205 Episode in 25300 steps, reward_ex 258.00, reward_in 132.98\n",
      "206 Episode in 25761 steps, reward_ex 461.00, reward_in 387.62\n",
      "207 Episode in 26018 steps, reward_ex 257.00, reward_in 245.04\n",
      "208 Episode in 26241 steps, reward_ex 223.00, reward_in 114.84\n",
      "209 Episode in 26473 steps, reward_ex 232.00, reward_in 181.28\n",
      "210 Episode in 26885 steps, reward_ex 412.00, reward_in 273.08\n",
      "211 Episode in 27381 steps, reward_ex 496.00, reward_in 455.15\n",
      "212 Episode in 27672 steps, reward_ex 291.00, reward_in 251.48\n",
      "213 Episode in 28136 steps, reward_ex 464.00, reward_in 325.44\n",
      "214 Episode in 28471 steps, reward_ex 335.00, reward_in 168.04\n",
      "215 Episode in 28811 steps, reward_ex 340.00, reward_in 200.54\n",
      "216 Episode in 29192 steps, reward_ex 381.00, reward_in 340.25\n",
      "217 Episode in 29631 steps, reward_ex 439.00, reward_in 406.83\n",
      "218 Episode in 30040 steps, reward_ex 409.00, reward_in 351.14\n",
      "219 Episode in 30540 steps, reward_ex 500.00, reward_in 472.31\n",
      "220 Episode in 31040 steps, reward_ex 500.00, reward_in 339.89\n",
      "221 Episode in 31540 steps, reward_ex 500.00, reward_in 379.40\n",
      "222 Episode in 32040 steps, reward_ex 500.00, reward_in 228.85\n",
      "223 Episode in 32540 steps, reward_ex 500.00, reward_in 362.13\n",
      "224 Episode in 33040 steps, reward_ex 500.00, reward_in 93.48\n",
      "225 Episode in 33540 steps, reward_ex 500.00, reward_in 352.34\n",
      "226 Episode in 34040 steps, reward_ex 500.00, reward_in 282.94\n",
      "227 Episode in 34540 steps, reward_ex 500.00, reward_in 179.56\n",
      "228 Episode in 35040 steps, reward_ex 500.00, reward_in 298.36\n",
      "229 Episode in 35540 steps, reward_ex 500.00, reward_in 197.15\n",
      "230 Episode in 36040 steps, reward_ex 500.00, reward_in 376.42\n",
      "231 Episode in 36540 steps, reward_ex 500.00, reward_in 318.53\n",
      "232 Episode in 37040 steps, reward_ex 500.00, reward_in 300.99\n",
      "233 Episode in 37540 steps, reward_ex 500.00, reward_in 365.56\n",
      "234 Episode in 38040 steps, reward_ex 500.00, reward_in 256.89\n",
      "235 Episode in 38540 steps, reward_ex 500.00, reward_in 281.30\n",
      "236 Episode in 39040 steps, reward_ex 500.00, reward_in 307.27\n",
      "237 Episode in 39540 steps, reward_ex 500.00, reward_in 444.04\n",
      "238 Episode in 40040 steps, reward_ex 500.00, reward_in 285.34\n",
      "239 Episode in 40540 steps, reward_ex 500.00, reward_in 259.73\n",
      "240 Episode in 41040 steps, reward_ex 500.00, reward_in 117.87\n",
      "241 Episode in 41540 steps, reward_ex 500.00, reward_in 183.74\n",
      "242 Episode in 42040 steps, reward_ex 500.00, reward_in 174.40\n",
      "243 Episode in 42540 steps, reward_ex 500.00, reward_in 136.77\n",
      "244 Episode in 43040 steps, reward_ex 500.00, reward_in 275.27\n",
      "245 Episode in 43540 steps, reward_ex 500.00, reward_in 333.77\n",
      "246 Episode in 44040 steps, reward_ex 500.00, reward_in 204.65\n",
      "247 Episode in 44540 steps, reward_ex 500.00, reward_in 214.88\n",
      "248 Episode in 45040 steps, reward_ex 500.00, reward_in 260.44\n",
      "249 Episode in 45385 steps, reward_ex 345.00, reward_in 293.26\n",
      "250 Episode in 45780 steps, reward_ex 395.00, reward_in 359.32\n",
      "251 Episode in 46124 steps, reward_ex 344.00, reward_in 250.83\n",
      "252 Episode in 46604 steps, reward_ex 480.00, reward_in 326.81\n",
      "253 Episode in 46962 steps, reward_ex 358.00, reward_in 234.63\n",
      "254 Episode in 47270 steps, reward_ex 308.00, reward_in 195.36\n",
      "255 Episode in 47721 steps, reward_ex 451.00, reward_in 266.34\n",
      "256 Episode in 48061 steps, reward_ex 340.00, reward_in 157.39\n",
      "257 Episode in 48560 steps, reward_ex 499.00, reward_in 387.20\n",
      "258 Episode in 48980 steps, reward_ex 420.00, reward_in 359.87\n",
      "259 Episode in 49423 steps, reward_ex 443.00, reward_in 280.30\n",
      "260 Episode in 49923 steps, reward_ex 500.00, reward_in 366.16\n",
      "261 Episode in 50423 steps, reward_ex 500.00, reward_in 236.69\n",
      "262 Episode in 50923 steps, reward_ex 500.00, reward_in 289.38\n",
      "263 Episode in 51423 steps, reward_ex 500.00, reward_in 335.13\n",
      "264 Episode in 51923 steps, reward_ex 500.00, reward_in 478.90\n",
      "265 Episode in 52423 steps, reward_ex 500.00, reward_in 381.45\n",
      "266 Episode in 52923 steps, reward_ex 500.00, reward_in 340.82\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "267 Episode in 53423 steps, reward_ex 500.00, reward_in 280.96\n",
      "268 Episode in 53923 steps, reward_ex 500.00, reward_in 262.81\n",
      "269 Episode in 54423 steps, reward_ex 500.00, reward_in 263.40\n",
      "270 Episode in 54923 steps, reward_ex 500.00, reward_in 255.61\n",
      "271 Episode in 55423 steps, reward_ex 500.00, reward_in 308.09\n",
      "272 Episode in 55923 steps, reward_ex 500.00, reward_in 369.12\n",
      "273 Episode in 56423 steps, reward_ex 500.00, reward_in 301.21\n",
      "274 Episode in 56923 steps, reward_ex 500.00, reward_in 205.92\n",
      "275 Episode in 57423 steps, reward_ex 500.00, reward_in 241.82\n",
      "276 Episode in 57923 steps, reward_ex 500.00, reward_in 320.17\n",
      "277 Episode in 58423 steps, reward_ex 500.00, reward_in 186.18\n",
      "278 Episode in 58923 steps, reward_ex 500.00, reward_in 183.53\n",
      "279 Episode in 59423 steps, reward_ex 500.00, reward_in 213.89\n",
      "280 Episode in 59923 steps, reward_ex 500.00, reward_in 367.98\n",
      "281 Episode in 60423 steps, reward_ex 500.00, reward_in 213.67\n",
      "282 Episode in 60923 steps, reward_ex 500.00, reward_in 428.97\n"
     ]
    }
   ],
   "source": [
    "# play!\n",
    "for i in range(1, n_episodes + 1):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    ep_reward_ex = 0.\n",
    "    ep_reward_in = 0.\n",
    "    while not done:\n",
    "        env.render()\n",
    "\n",
    "        action, val_ex, val_in = get_action_and_value(obs, net)\n",
    "        _obs, rew_ex, done, _ = env.step(action)\n",
    "        \n",
    "        rew_in = calculate_reward_in(pred_net, rand_net, _obs)    \n",
    "        value = 0.5 * (val_ex + val_in)\n",
    "        \n",
    "        # store\n",
    "        roll_memory.append([obs, action, _obs])\n",
    "        obs_memory.append(_obs)\n",
    "        rewards_ex.append(EX_COEF*rew_ex)\n",
    "        rewards_in.append(IN_COEF*rew_in)\n",
    "        values.append(value)\n",
    "        \n",
    "        obs = _obs\n",
    "        steps += 1\n",
    "        ep_reward_ex += rew_ex\n",
    "        ep_reward_in += rew_in\n",
    "        \n",
    "        if done or steps % roll_len == 0:\n",
    "            if done:\n",
    "                _value = 0.\n",
    "            else:\n",
    "                _, _val_ex, _val_in = get_action_and_value(_obs, net)\n",
    "                _value = 0.5*(_val_ex + _val_in)\n",
    "            \n",
    "            values.append(_value)\n",
    "            train_memory.extend(compute_adv_with_gae(rewards_ex, rewards_in, values, roll_memory))\n",
    "            rewards_ex.clear()\n",
    "            rewards_in.clear()\n",
    "            values.clear()\n",
    "            roll_memory.clear()\n",
    "            \n",
    "        if steps % roll_len == 0:\n",
    "            net_memory.appendleft(net.state_dict())\n",
    "            old_net.load_state_dict(net_memory.pop())\n",
    "            learn(net, old_net, pred_net, rand_net, net_optim, pred_optim, train_memory)\n",
    "            train_memory.clear()\n",
    "            learn_steps += 1\n",
    "        \n",
    "        if steps % roll_len*50 == 0:\n",
    "            mean, std = get_norm_params(obs_memory)\n",
    "            obs_memory.clear()\n",
    "    \n",
    "    if done:        \n",
    "        ep_rewards.append(ep_reward_ex)\n",
    "        print('{:3} Episode in {:5} steps, reward_ex {:.2f}, reward_in {:.2f}'.format(\n",
    "            i, steps, ep_reward_ex, ep_reward_in))\n",
    "\n",
    "        if len(ep_rewards) >= n_eval:\n",
    "            if np.mean(list(reversed(ep_rewards))[: n_eval]) >= env.spec.reward_threshold:\n",
    "                print('\\n{} is sloved! {:3} Episode in {:3} steps'.format(\n",
    "                    env.spec.id, i, steps))\n",
    "                torch.save(old_net.state_dict(),\n",
    "                           f'./test/saved_models/{env.spec.id}_ep{i}_clear_model_ppo_r.pt')\n",
    "                break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "plt.title('Reward')\n",
    "plt.plot(ep_rewards)\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.title('Loss')\n",
    "plt.plot(losses)\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.title('m_Loss')\n",
    "plt.plot(m_losses)\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.title('f_Loss')\n",
    "plt.plot(f_losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[\n",
    "    ('CartPole-v0', 161, 32, 256),\n",
    "    ('CartPole-v1', 162, 32, 256),\n",
    "    ('MountainCar-v0', 660),\n",
    "    ('LunarLander-v2', 260)\n",
    "]"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "C51_tensorflow.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
