{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IWnm3qot3o1W"
   },
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "from copy import deepcopy\n",
    "\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 708,
     "status": "ok",
     "timestamp": 1534482400648,
     "user": {
      "displayName": "윤승제",
      "photoUrl": "//lh5.googleusercontent.com/-EucKC7DmcQI/AAAAAAAAAAI/AAAAAAAAAGA/gQU1NPEmNFA/s50-c-k-no/photo.jpg",
      "userId": "105654037995838004821"
     },
     "user_tz": -540
    },
    "id": "maRVADiTlzHD",
    "outputId": "783b7610-95c2-4b54-b2ce-d8e853c484ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda: True\n"
     ]
    }
   ],
   "source": [
    "SEED = 5\n",
    "BATCH_SIZE = 4\n",
    "LR = 0.00025\n",
    "EPOCHS = 4\n",
    "CLIP = 0.1\n",
    "GAMMA = 0.99\n",
    "LAMBDA = 0.95\n",
    "ENT_COEF = 0.01\n",
    "EX_COEF = 0.0\n",
    "IN_COEF = 1.0\n",
    "\n",
    "# set device\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print('cuda:', use_cuda)\n",
    "device = torch.device('cuda' if use_cuda else 'cpu')\n",
    "\n",
    "# random seed\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if use_cuda:\n",
    "    torch.cuda.manual_seed_all(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9Ffkl_5C4R81"
   },
   "outputs": [],
   "source": [
    "class ActorCriticNet(nn.Module):\n",
    "    def __init__(self, obs_space, action_space):\n",
    "        super().__init__()\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(obs_space, 256),\n",
    "            nn.SELU()\n",
    "        )\n",
    "\n",
    "        self.pol = nn.Sequential(\n",
    "            nn.Linear(256, 256),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(256, action_space)\n",
    "        )\n",
    "        \n",
    "        self.val_ex = nn.Sequential(\n",
    "            nn.Linear(256, 256),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "        \n",
    "        self.val_in = nn.Sequential(\n",
    "            nn.Linear(256, 256),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "        \n",
    "        self.log_softmax = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.head(x)\n",
    "        logit = self.pol(out).reshape(out.shape[0], -1)\n",
    "        value_ex = self.val_ex(out).reshape(out.shape[0], 1)\n",
    "        value_in = self.val_in(out).reshape(out.shape[0], 1)\n",
    "        log_probs = self.log_softmax(logit)\n",
    "        \n",
    "        return log_probs, value_ex, value_in\n",
    "    \n",
    "class RandomNet(nn.Module):\n",
    "    def __init__(self, obs_space):\n",
    "        super().__init__()\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(obs_space, 256),\n",
    "            nn.SELU()\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(256, 512)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.head(x)\n",
    "        obs_feature = self.fc(out).reshape(out.shape[0], -1)\n",
    "\n",
    "        return obs_feature\n",
    "\n",
    "\n",
    "class PredictNet(nn.Module):\n",
    "    def __init__(self, obs_space):\n",
    "        super().__init__()\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(obs_space, 256),\n",
    "            nn.SELU()\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(256, 256),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(256, 512)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.head(x)\n",
    "        obs_feature = self.fc(out).reshape(out.shape[0], -1)\n",
    "\n",
    "        return obs_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "m_losses = []\n",
    "f_losses = []\n",
    "\n",
    "\n",
    "def learn(net, old_net, pred_net, rand_net, net_optim, pred_optim, train_memory):\n",
    "    net.train()\n",
    "    old_net.train()\n",
    "    \n",
    "    dataloader = DataLoader(\n",
    "        train_memory,\n",
    "        shuffle=True,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        pin_memory=use_cuda\n",
    "    )\n",
    "\n",
    "    for i in range(EPOCHS):\n",
    "        for (s, a, _s, ret_ex, ret_in, adv) in dataloader:\n",
    "            s = s.to(device).float()\n",
    "            a  = a.to(device).long()\n",
    "            _s = _s.to(device).float()\n",
    "            _s_norm_np = normalize_obs(_s.detach().cpu().numpy())\n",
    "            _s_norm = torch.tensor(_s_norm_np).to(device).float()\n",
    "            ret_ex = ret_ex.to(device).float()\n",
    "            ret_in = ret_in.to(device).float()\n",
    "            adv = adv.to(device).float()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                rand_f = rand_net(_s_norm)\n",
    "                log_p_old, v_ex_old, v_in_old = old_net(s)\n",
    "                log_p_a_old = log_p_old[range(BATCH_SIZE), a]\n",
    "\n",
    "            pred_f = pred_net(_s_norm)\n",
    "            log_p, v_ex, v_in = net(s)\n",
    "            log_p_a = log_p[range(BATCH_SIZE), a]\n",
    "            p_ratio = (log_p_a - log_p_a_old).exp()\n",
    "            p_r_clip = torch.clamp(p_ratio, 1. - CLIP, 1. + CLIP)\n",
    "            p_loss = torch.min(p_ratio * adv, p_r_clip * adv).mean()\n",
    "            v_ex_loss = 0.5 * (ret_ex - v_ex).pow(2)\n",
    "            v_in_loss = 0.5 * (ret_in - v_in).pow(2)\n",
    "            v_loss = (v_ex_loss + v_in_loss).mean() \n",
    "            entropy = -(log_p.exp() * log_p).sum(dim=1).mean()\n",
    "\n",
    "            # loss\n",
    "            m_loss = -(p_loss - v_loss + ENT_COEF * entropy)\n",
    "            m_losses.append(m_loss)\n",
    "\n",
    "            f_loss = (pred_f - rand_f).pow(2).sum(dim=1).mean()\n",
    "            f_losses.append(f_loss)\n",
    "            \n",
    "            loss = m_loss + f_loss\n",
    "            losses.append(loss)\n",
    "            \n",
    "            net_optim.zero_grad()\n",
    "            pred_optim.zero_grad()\n",
    "            loss.backward()\n",
    "            net_optim.step()\n",
    "            pred_optim.step()\n",
    "\n",
    "\n",
    "def get_action_and_value(obs, net):\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        state = torch.tensor([obs]).to(device).float()\n",
    "        log_p, v_ex, v_in = net(state)\n",
    "        m = Categorical(log_p.exp())\n",
    "        action = m.sample()\n",
    "\n",
    "    return action.item(), v_ex.item(), v_in.item()\n",
    "\n",
    "\n",
    "def compute_adv_with_gae(reward_ex, reward_in, values, roll_memory):\n",
    "    rew_ex = np.array(rewards_ex, 'float')\n",
    "    rew_in = np.array(rewards_in, 'float')\n",
    "    rew = rew_ex + rew_in\n",
    "    val = np.array(values[:-1], 'float')\n",
    "    _val = np.array(values[1:], 'float')\n",
    "    delta = rew + GAMMA * _val - val\n",
    "    dis_r_ex = np.array([GAMMA**(i) * r for i, r in enumerate(reward_ex)], 'float')\n",
    "    dis_r_in = np.array([GAMMA**(i) * r for i, r in enumerate(reward_in)], 'float')\n",
    "    gae_dt = np.array([(GAMMA * LAMBDA)**(i) * dt for i, dt in enumerate(delta.tolist())], 'float')\n",
    "    for i, data in enumerate(roll_memory):\n",
    "        data.append(sum(dis_r_ex[i:] / GAMMA**(i)))\n",
    "        data.append(sum(dis_r_in[i:] / GAMMA**(i)))\n",
    "        data.append(sum(gae_dt[i:] / (GAMMA * LAMBDA)**(i)))\n",
    "\n",
    "    return roll_memory\n",
    "\n",
    "\n",
    "def get_norm_params(obs_memory):\n",
    "    global obs_apace\n",
    "\n",
    "    obses = [[] for _ in range(obs_space)]\n",
    "    for obs in obs_memory:\n",
    "        for j in range(obs_space):\n",
    "            obses[j].append(obs[j])\n",
    "\n",
    "    mean = np.zeros(obs_space, 'float')\n",
    "    std = np.zeros(obs_space, 'float')\n",
    "    for i, obs_ in enumerate(obses):\n",
    "        mean[i] = np.mean(obs_)\n",
    "        std[i] = np.std(obs_)\n",
    "\n",
    "    return mean, std\n",
    "\n",
    "\n",
    "def normalize_obs(obs):\n",
    "    global mean, std\n",
    "    norm_obs = (obs - mean) / std\n",
    "#     return np.clip(norm_obs, -5, 5)\n",
    "    return norm_obs\n",
    "\n",
    "\n",
    "def calculate_reward_in(pred_net, rand_net, obs):\n",
    "    norm_obs = normalize_obs(obs)\n",
    "    state = torch.tensor([norm_obs]).to(device).float()\n",
    "    with torch.no_grad():\n",
    "        pred_obs = pred_net(state)\n",
    "        rand_obs = rand_net(state)\n",
    "        reward = (pred_obs - rand_obs).pow(2).sum()\n",
    "        clipped_reward = torch.clamp(reward, -1, 1)\n",
    "\n",
    "    return clipped_reward.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 3377
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 135196,
     "status": "ok",
     "timestamp": 1534482559393,
     "user": {
      "displayName": "윤승제",
      "photoUrl": "//lh5.googleusercontent.com/-EucKC7DmcQI/AAAAAAAAAAI/AAAAAAAAAGA/gQU1NPEmNFA/s50-c-k-no/photo.jpg",
      "userId": "105654037995838004821"
     },
     "user_tz": -540
    },
    "id": "PnifSBJglzHh",
    "outputId": "94177345-918e-4a96-d9a8-d8aba0a4bc9a",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# make an environment\n",
    "env = gym.make('CartPole-v0')\n",
    "# env = gym.make('CartPole-v1')\n",
    "# env = gym.make('MountainCar-v0')\n",
    "# env = gym.make('LunarLander-v2')\n",
    "\n",
    "env.seed(SEED)\n",
    "obs_space = env.observation_space.shape[0]\n",
    "action_space = env.action_space.n\n",
    "\n",
    "# hyperparameter\n",
    "n_episodes = 3000\n",
    "roll_len = 128\n",
    "n_eval = env.spec.trials\n",
    "\n",
    "# global values\n",
    "init_steps = 0\n",
    "steps = 0\n",
    "learn_steps = 0\n",
    "mean = 0.\n",
    "std = 0.\n",
    "ep_rewards = []\n",
    "is_rollout = False\n",
    "is_solved = False\n",
    "is_init_roll = True\n",
    "\n",
    "# make a rollout memory\n",
    "net_memory = deque(maxlen=2)\n",
    "train_memory = []\n",
    "roll_memory = []\n",
    "obs_memory = []\n",
    "rewards_ex = []\n",
    "rewards_in = []\n",
    "values = []\n",
    "\n",
    "# make nerual networks\n",
    "net = ActorCriticNet(obs_space, action_space).to(device)\n",
    "old_net = deepcopy(net)\n",
    "net_memory.appendleft(net.state_dict())\n",
    "pred_net = PredictNet(obs_space).to(device)\n",
    "rand_net = RandomNet(obs_space).to(device)\n",
    "\n",
    "# make optimizer\n",
    "net_optim = torch.optim.Adam(net.parameters(), lr=LR, eps=1e-5)\n",
    "pred_optim = torch.optim.Adam(pred_net.parameters(), lr=LR, eps=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "use_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.spec.max_episode_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.spec.trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "195.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.spec.reward_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 3377
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 135196,
     "status": "ok",
     "timestamp": 1534482559393,
     "user": {
      "displayName": "윤승제",
      "photoUrl": "//lh5.googleusercontent.com/-EucKC7DmcQI/AAAAAAAAAAI/AAAAAAAAAGA/gQU1NPEmNFA/s50-c-k-no/photo.jpg",
      "userId": "105654037995838004821"
     },
     "user_tz": -540
    },
    "id": "PnifSBJglzHh",
    "outputId": "94177345-918e-4a96-d9a8-d8aba0a4bc9a"
   },
   "outputs": [],
   "source": [
    "# simulation\n",
    "while True:\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "#         env.render()\n",
    "        action = env.action_space.sample()\n",
    "        _obs, _, done, _ = env.step(action)\n",
    "        obs_memory.append(_obs)\n",
    "        obs = _obs\n",
    "        init_steps += 1\n",
    "        if init_steps == roll_len * 50:\n",
    "            mean, std = get_norm_params(obs_memory)\n",
    "            obs_memory.clear()\n",
    "            is_init_roll = False\n",
    "            break\n",
    "    if not is_init_roll:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 3377
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 135196,
     "status": "ok",
     "timestamp": 1534482559393,
     "user": {
      "displayName": "윤승제",
      "photoUrl": "//lh5.googleusercontent.com/-EucKC7DmcQI/AAAAAAAAAAI/AAAAAAAAAGA/gQU1NPEmNFA/s50-c-k-no/photo.jpg",
      "userId": "105654037995838004821"
     },
     "user_tz": -540
    },
    "id": "PnifSBJglzHh",
    "outputId": "94177345-918e-4a96-d9a8-d8aba0a4bc9a",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1 Episode in    10 steps, reward_ex 10.00, reward_in 10.00\n",
      "  2 Episode in    35 steps, reward_ex 25.00, reward_in 25.00\n",
      "  3 Episode in    46 steps, reward_ex 11.00, reward_in 11.00\n",
      "  4 Episode in    58 steps, reward_ex 12.00, reward_in 12.00\n",
      "  5 Episode in    81 steps, reward_ex 23.00, reward_in 23.00\n",
      "  6 Episode in   103 steps, reward_ex 22.00, reward_in 22.00\n",
      "  7 Episode in   116 steps, reward_ex 13.00, reward_in 13.00\n",
      "  8 Episode in   128 steps, reward_ex 12.00, reward_in 12.00\n",
      "  9 Episode in   140 steps, reward_ex 12.00, reward_in 12.00\n",
      " 10 Episode in   157 steps, reward_ex 17.00, reward_in 17.00\n",
      " 11 Episode in   211 steps, reward_ex 54.00, reward_in 54.00\n",
      " 12 Episode in   236 steps, reward_ex 25.00, reward_in 24.08\n",
      " 13 Episode in   255 steps, reward_ex 19.00, reward_in 19.00\n",
      " 14 Episode in   268 steps, reward_ex 13.00, reward_in 12.84\n",
      " 15 Episode in   278 steps, reward_ex 10.00, reward_in 9.81\n",
      " 16 Episode in   291 steps, reward_ex 13.00, reward_in 11.69\n",
      " 17 Episode in   301 steps, reward_ex 10.00, reward_in 10.00\n",
      " 18 Episode in   312 steps, reward_ex 11.00, reward_in 11.00\n",
      " 19 Episode in   338 steps, reward_ex 26.00, reward_in 21.82\n",
      " 20 Episode in   355 steps, reward_ex 17.00, reward_in 14.84\n",
      " 21 Episode in   370 steps, reward_ex 15.00, reward_in 13.22\n",
      " 22 Episode in   390 steps, reward_ex 20.00, reward_in 15.85\n",
      " 23 Episode in   401 steps, reward_ex 11.00, reward_in 10.71\n",
      " 24 Episode in   415 steps, reward_ex 14.00, reward_in 11.90\n",
      " 25 Episode in   429 steps, reward_ex 14.00, reward_in 9.10\n",
      " 26 Episode in   442 steps, reward_ex 13.00, reward_in 8.98\n",
      " 27 Episode in   456 steps, reward_ex 14.00, reward_in 13.60\n",
      " 28 Episode in   466 steps, reward_ex 10.00, reward_in 7.29\n",
      " 29 Episode in   482 steps, reward_ex 16.00, reward_in 10.78\n",
      " 30 Episode in   498 steps, reward_ex 16.00, reward_in 15.98\n",
      " 31 Episode in   513 steps, reward_ex 15.00, reward_in 14.41\n",
      " 32 Episode in   536 steps, reward_ex 23.00, reward_in 17.06\n",
      " 33 Episode in   545 steps, reward_ex 9.00, reward_in 4.18\n",
      " 34 Episode in   563 steps, reward_ex 18.00, reward_in 17.62\n",
      " 35 Episode in   572 steps, reward_ex 9.00, reward_in 3.22\n",
      " 36 Episode in   609 steps, reward_ex 37.00, reward_in 28.78\n",
      " 37 Episode in   632 steps, reward_ex 23.00, reward_in 9.61\n",
      " 38 Episode in   642 steps, reward_ex 10.00, reward_in 6.98\n",
      " 39 Episode in   675 steps, reward_ex 33.00, reward_in 21.68\n",
      " 40 Episode in   701 steps, reward_ex 26.00, reward_in 14.99\n",
      " 41 Episode in   737 steps, reward_ex 36.00, reward_in 30.52\n",
      " 42 Episode in   755 steps, reward_ex 18.00, reward_in 7.59\n",
      " 43 Episode in   766 steps, reward_ex 11.00, reward_in 7.25\n",
      " 44 Episode in   793 steps, reward_ex 27.00, reward_in 15.26\n",
      " 45 Episode in   823 steps, reward_ex 30.00, reward_in 23.58\n",
      " 46 Episode in   854 steps, reward_ex 31.00, reward_in 15.44\n",
      " 47 Episode in   864 steps, reward_ex 10.00, reward_in 6.44\n",
      " 48 Episode in   887 steps, reward_ex 23.00, reward_in 22.12\n",
      " 49 Episode in   930 steps, reward_ex 43.00, reward_in 41.07\n",
      " 50 Episode in   950 steps, reward_ex 20.00, reward_in 12.23\n",
      " 51 Episode in   991 steps, reward_ex 41.00, reward_in 18.55\n",
      " 52 Episode in  1027 steps, reward_ex 36.00, reward_in 32.15\n",
      " 53 Episode in  1086 steps, reward_ex 59.00, reward_in 29.35\n",
      " 54 Episode in  1119 steps, reward_ex 33.00, reward_in 19.77\n",
      " 55 Episode in  1141 steps, reward_ex 22.00, reward_in 10.18\n",
      " 56 Episode in  1164 steps, reward_ex 23.00, reward_in 13.76\n",
      " 57 Episode in  1194 steps, reward_ex 30.00, reward_in 22.47\n",
      " 58 Episode in  1235 steps, reward_ex 41.00, reward_in 22.69\n",
      " 59 Episode in  1266 steps, reward_ex 31.00, reward_in 23.12\n",
      " 60 Episode in  1321 steps, reward_ex 55.00, reward_in 18.54\n",
      " 61 Episode in  1356 steps, reward_ex 35.00, reward_in 26.73\n",
      " 62 Episode in  1382 steps, reward_ex 26.00, reward_in 11.79\n",
      " 63 Episode in  1430 steps, reward_ex 48.00, reward_in 21.89\n",
      " 64 Episode in  1447 steps, reward_ex 17.00, reward_in 7.83\n",
      " 65 Episode in  1480 steps, reward_ex 33.00, reward_in 23.76\n",
      " 66 Episode in  1524 steps, reward_ex 44.00, reward_in 19.88\n",
      " 67 Episode in  1617 steps, reward_ex 93.00, reward_in 64.46\n",
      " 68 Episode in  1686 steps, reward_ex 69.00, reward_in 65.40\n",
      " 69 Episode in  1841 steps, reward_ex 155.00, reward_in 128.69\n",
      " 70 Episode in  1898 steps, reward_ex 57.00, reward_in 31.66\n",
      " 71 Episode in  1943 steps, reward_ex 45.00, reward_in 22.49\n",
      " 72 Episode in  2143 steps, reward_ex 200.00, reward_in 122.40\n",
      " 73 Episode in  2343 steps, reward_ex 200.00, reward_in 163.36\n",
      " 74 Episode in  2543 steps, reward_ex 200.00, reward_in 133.53\n",
      " 75 Episode in  2698 steps, reward_ex 155.00, reward_in 79.05\n",
      " 76 Episode in  2865 steps, reward_ex 167.00, reward_in 80.62\n",
      " 77 Episode in  3035 steps, reward_ex 170.00, reward_in 82.07\n",
      " 78 Episode in  3220 steps, reward_ex 185.00, reward_in 83.70\n",
      " 79 Episode in  3380 steps, reward_ex 160.00, reward_in 88.41\n",
      " 80 Episode in  3568 steps, reward_ex 188.00, reward_in 61.98\n",
      " 81 Episode in  3740 steps, reward_ex 172.00, reward_in 59.08\n",
      " 82 Episode in  3916 steps, reward_ex 176.00, reward_in 87.30\n",
      " 83 Episode in  4062 steps, reward_ex 146.00, reward_in 40.90\n",
      " 84 Episode in  4232 steps, reward_ex 170.00, reward_in 50.41\n",
      " 85 Episode in  4382 steps, reward_ex 150.00, reward_in 45.87\n",
      " 86 Episode in  4528 steps, reward_ex 146.00, reward_in 40.74\n",
      " 87 Episode in  4690 steps, reward_ex 162.00, reward_in 50.93\n",
      " 88 Episode in  4851 steps, reward_ex 161.00, reward_in 42.89\n",
      " 89 Episode in  5033 steps, reward_ex 182.00, reward_in 76.64\n",
      " 90 Episode in  5208 steps, reward_ex 175.00, reward_in 114.47\n",
      " 91 Episode in  5371 steps, reward_ex 163.00, reward_in 31.93\n",
      " 92 Episode in  5571 steps, reward_ex 200.00, reward_in 109.53\n",
      " 93 Episode in  5749 steps, reward_ex 178.00, reward_in 139.94\n",
      " 94 Episode in  5949 steps, reward_ex 200.00, reward_in 160.18\n",
      " 95 Episode in  6149 steps, reward_ex 200.00, reward_in 77.91\n",
      " 96 Episode in  6349 steps, reward_ex 200.00, reward_in 121.16\n",
      " 97 Episode in  6549 steps, reward_ex 200.00, reward_in 160.71\n",
      " 98 Episode in  6749 steps, reward_ex 200.00, reward_in 158.26\n"
     ]
    }
   ],
   "source": [
    "# play!\n",
    "for i in range(1, n_episodes + 1):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    ep_reward_ex = 0.\n",
    "    ep_reward_in = 0.\n",
    "    while not done:\n",
    "        env.render()\n",
    "\n",
    "        action, val_ex, val_in = get_action_and_value(obs, net)\n",
    "        _obs, rew_ex, done, _ = env.step(action)\n",
    "        \n",
    "        rew_in = calculate_reward_in(pred_net, rand_net, _obs)    \n",
    "        value = 0.5 * (val_ex + val_in)\n",
    "        \n",
    "        # store\n",
    "        roll_memory.append([obs, action, _obs])\n",
    "        obs_memory.append(_obs)\n",
    "        rewards_ex.append(EX_COEF*rew_ex)\n",
    "        rewards_in.append(IN_COEF*rew_in)\n",
    "        values.append(value)\n",
    "        \n",
    "        obs = _obs\n",
    "        steps += 1\n",
    "        ep_reward_ex += rew_ex\n",
    "        ep_reward_in += rew_in\n",
    "        \n",
    "        if done or steps % roll_len == 0:\n",
    "            if done:\n",
    "                _value = 0.\n",
    "            else:\n",
    "                _, _val_ex, _val_in = get_action_and_value(_obs, net)\n",
    "                _value = 0.5*(_val_ex + _val_in)\n",
    "            \n",
    "            values.append(_value)\n",
    "            train_memory.extend(compute_adv_with_gae(rewards_ex, rewards_in, values, roll_memory))\n",
    "            rewards_ex.clear()\n",
    "            rewards_in.clear()\n",
    "            values.clear()\n",
    "            roll_memory.clear()\n",
    "            \n",
    "        if steps % roll_len == 0:\n",
    "            net_memory.appendleft(net.state_dict())\n",
    "            old_net.load_state_dict(net_memory.pop())\n",
    "            learn(net, old_net, pred_net, rand_net, net_optim, pred_optim, train_memory)\n",
    "            train_memory.clear()\n",
    "            learn_steps += 1\n",
    "        \n",
    "        if steps % roll_len*50 == 0:\n",
    "            mean, std = get_norm_params(obs_memory)\n",
    "            obs_memory.clear()\n",
    "    \n",
    "    if done:        \n",
    "        ep_rewards.append(ep_reward_ex)\n",
    "        print('{:3} Episode in {:5} steps, reward_ex {:.2f}, reward_in {:.2f}'.format(\n",
    "            i, steps, ep_reward_ex, ep_reward_in))\n",
    "\n",
    "        if len(ep_rewards) >= n_eval:\n",
    "            if np.mean(list(reversed(ep_rewards))[: n_eval]) >= env.spec.reward_threshold:\n",
    "                print('\\n{} is sloved! {:3} Episode in {:3} steps'.format(\n",
    "                    env.spec.id, i, steps))\n",
    "                torch.save(old_net.state_dict(),\n",
    "                           f'../test/saved_models/{env.spec.id}_ep{i}_clear_model_ppo_r.pt')\n",
    "                break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "plt.title('Reward')\n",
    "plt.plot(ep_rewards)\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.title('Loss')\n",
    "plt.plot(losses)\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.title('m_Loss')\n",
    "plt.plot(m_losses)\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.title('f_Loss')\n",
    "plt.plot(f_losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[\n",
    "    ('CartPole-v0', 161, 32, 256),\n",
    "    ('CartPole-v1', 162, 32, 256),\n",
    "    ('MountainCar-v0', 660),\n",
    "    ('LunarLander-v2', 260)\n",
    "]"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "C51_tensorflow.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
