{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IWnm3qot3o1W"
   },
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "from copy import deepcopy\n",
    "\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 708,
     "status": "ok",
     "timestamp": 1534482400648,
     "user": {
      "displayName": "윤승제",
      "photoUrl": "//lh5.googleusercontent.com/-EucKC7DmcQI/AAAAAAAAAAI/AAAAAAAAAGA/gQU1NPEmNFA/s50-c-k-no/photo.jpg",
      "userId": "105654037995838004821"
     },
     "user_tz": -540
    },
    "id": "maRVADiTlzHD",
    "outputId": "783b7610-95c2-4b54-b2ce-d8e853c484ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda: True\n"
     ]
    }
   ],
   "source": [
    "SEED = 1\n",
    "BATCH_SIZE = 4\n",
    "LR = 0.0001\n",
    "EPOCHS = 4\n",
    "CLIP = 0.1\n",
    "GAMMA = 0.99\n",
    "LAMBDA = 0.95\n",
    "ENT_COEF = 0.01\n",
    "EX_COEF = 2.0\n",
    "IN_COEF = 1.0\n",
    "\n",
    "# set device\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print('cuda:', use_cuda)\n",
    "device = torch.device('cuda' if use_cuda else 'cpu')\n",
    "\n",
    "# random seed\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if use_cuda:\n",
    "    torch.cuda.manual_seed_all(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9Ffkl_5C4R81"
   },
   "outputs": [],
   "source": [
    "class ActorCriticNet(nn.Module):\n",
    "    def __init__(self, obs_space, action_space):\n",
    "        super().__init__()\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(obs_space, 256),\n",
    "            nn.SELU()\n",
    "        )\n",
    "\n",
    "        self.pol = nn.Sequential(\n",
    "            nn.Linear(256, 256),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(256, action_space)\n",
    "        )\n",
    "        \n",
    "        self.val_ex = nn.Sequential(\n",
    "            nn.Linear(256, 256),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "        \n",
    "        self.val_in = nn.Sequential(\n",
    "            nn.Linear(256, 256),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "        \n",
    "        self.log_softmax = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.head(x)\n",
    "        logit = self.pol(out).reshape(out.shape[0], -1)\n",
    "        value_ex = self.val_ex(out).reshape(out.shape[0], 1)\n",
    "        value_in = self.val_in(out).reshape(out.shape[0], 1)\n",
    "        log_probs = self.log_softmax(logit)\n",
    "        \n",
    "        return log_probs, value_ex, value_in\n",
    "    \n",
    "class RandomNet(nn.Module):\n",
    "    def __init__(self, obs_space):\n",
    "        super().__init__()\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(obs_space, 256),\n",
    "            nn.SELU()\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(256, 512)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.head(x)\n",
    "        obs_feature = self.fc(out).reshape(out.shape[0], -1)\n",
    "\n",
    "        return obs_feature\n",
    "\n",
    "\n",
    "class PredictNet(nn.Module):\n",
    "    def __init__(self, obs_space):\n",
    "        super().__init__()\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(obs_space, 256),\n",
    "            nn.SELU()\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(256, 256),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(256, 512)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.head(x)\n",
    "        obs_feature = self.fc(out).reshape(out.shape[0], -1)\n",
    "\n",
    "        return obs_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "m_losses = []\n",
    "f_losses = []\n",
    "\n",
    "\n",
    "def learn(net, old_net, pred_net, rand_net, net_optim, pred_optim, train_memory):\n",
    "    global action_space\n",
    "\n",
    "    net.train()\n",
    "    old_net.train()\n",
    "    dataloader = DataLoader(train_memory,\n",
    "                        batch_size=BATCH_SIZE,\n",
    "                        shuffle=True,\n",
    "                        pin_memory=use_cuda)\n",
    "    for i in range(EPOCHS):\n",
    "        for (s, a, r_ex, r_in, _s, d, adv) in dataloader:\n",
    "            s_batch = s.to(device).float()\n",
    "            a_batch = a.detach().to(device).long()\n",
    "            _s_batch = _s.to(device).float()\n",
    "            _s_norm = normalize_obs(_s.detach().cpu().numpy())\n",
    "            _s_norm_batch = torch.tensor(_s_norm).to(device).float()\n",
    "            r_ex_batch = r_ex.to(device).float()\n",
    "            r_in_batch = r_in.to(device).float()\n",
    "            adv_batch = adv.to(device).float()\n",
    "            done_mask = 1. - d.to(device).float()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                rand_feature = rand_net(_s_norm_batch)\n",
    "                log_p_batch_old, _, _ = old_net(s_batch)\n",
    "                log_p_acting_old = log_p_batch_old[range(BATCH_SIZE), a_batch]\n",
    "\n",
    "            pred_feature = pred_net(_s_norm_batch)\n",
    "            log_p_batch, v_ex, v_in = net(s_batch)\n",
    "            _, _v_ex, _v_in = net(_s_batch)\n",
    "            log_p_acting = log_p_batch[range(BATCH_SIZE), a_batch]\n",
    "            p_ratio = (log_p_acting - log_p_acting_old).exp()\n",
    "            p_ratio_clip = torch.clamp(p_ratio, 1. - CLIP, 1. + CLIP)\n",
    "            clip_loss = torch.min(p_ratio * adv_batch,\n",
    "                                  p_ratio_clip * adv_batch)\n",
    "            \n",
    "            v_ex_loss = 0.5 * (r_ex_batch + GAMMA * done_mask * _v_ex - v_ex).pow(2)\n",
    "            v_in_loss = 0.5 * (r_in_batch + GAMMA * done_mask * _v_in - v_in).pow(2)\n",
    "            v_loss = v_ex_loss + v_in_loss \n",
    "            entropy = -(log_p_batch.exp() * log_p_batch).sum(dim=1)\n",
    "\n",
    "            # loss\n",
    "            m_loss = -(clip_loss - v_loss + ENT_COEF * entropy).mean()\n",
    "            m_losses.append(m_loss)\n",
    "\n",
    "            f_loss = (pred_feature - rand_feature).pow(2).sum(dim=1).mean()\n",
    "            f_losses.append(f_loss)\n",
    "            \n",
    "            loss = m_loss + f_loss\n",
    "            losses.append(loss)\n",
    "            \n",
    "            net_optim.zero_grad()\n",
    "            pred_optim.zero_grad()\n",
    "            loss.backward()\n",
    "            net_optim.step()\n",
    "            pred_optim.step()\n",
    "\n",
    "\n",
    "def get_action_and_value(obs, old_net):\n",
    "    old_net.eval()\n",
    "    with torch.no_grad():\n",
    "        state = torch.tensor([obs]).to(device).float()\n",
    "        log_p, v_ex, v_in = old_net(state)\n",
    "        m = Categorical(log_p.exp())\n",
    "        action = m.sample()\n",
    "\n",
    "    return action.item(), v_ex.item(), v_in.item()\n",
    "\n",
    "\n",
    "def compute_adv(rewards, values, dones, roll_memory):\n",
    "    rew = np.array(rewards, 'float')\n",
    "    val = np.array(values[:-1], 'float')\n",
    "    _val = np.array(values[1:], 'float')\n",
    "    done_mask = 1. - np.array(dones, 'float')\n",
    "    delta = rew + 0.99 * done_mask * _val - val\n",
    "    disc_dt = np.array([\n",
    "        (GAMMA * LAMBDA)**(i) * dt for i, dt in enumerate(delta.tolist())],\n",
    "        'float')\n",
    "    for i, data in enumerate(roll_memory):\n",
    "        data.append(sum(disc_dt[i:] / (GAMMA * LAMBDA)**(i)))\n",
    "\n",
    "    return roll_memory\n",
    "\n",
    "\n",
    "def get_norm_params(obs_memory):\n",
    "    global obs_apace\n",
    "\n",
    "    obses = [[] for _ in range(obs_space)]\n",
    "    for obs in obs_memory:\n",
    "        for j in range(obs_space):\n",
    "            obses[j].append(obs[j])\n",
    "\n",
    "    mean = np.zeros(obs_space, 'float')\n",
    "    std = np.zeros(obs_space, 'float')\n",
    "    for i, obs_ in enumerate(obses):\n",
    "        mean[i] = np.mean(obs_)\n",
    "        std[i] = np.std(obs_)\n",
    "\n",
    "    return mean, std\n",
    "\n",
    "\n",
    "def normalize_obs(obs):\n",
    "    global mean, std\n",
    "#     means = [mean for _ in range(BATCH_SIZE)]\n",
    "#     stds = [std for _ in range(BATCH_SIZE)]\n",
    "#     mean_np = np.stack(means)\n",
    "#     std_np = np.stack(stds)\n",
    "    norm_obs = (obs - mean) / std\n",
    "\n",
    "#     return np.clip(norm_obs, -5, 5)\n",
    "    return norm_obs\n",
    "\n",
    "\n",
    "def calculate_reward_in(pred_net, rand_net, obs):\n",
    "    norm_obs = normalize_obs(obs)\n",
    "    state = torch.tensor([norm_obs]).to(device).float()\n",
    "    with torch.no_grad():\n",
    "        pred_obs = pred_net(state)\n",
    "        rand_obs = rand_net(state)\n",
    "        reward = (pred_obs - rand_obs).pow(2).sum()\n",
    "        clipped_reward = torch.clamp(reward, -1, 1)\n",
    "\n",
    "    return clipped_reward.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 3377
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 135196,
     "status": "ok",
     "timestamp": 1534482559393,
     "user": {
      "displayName": "윤승제",
      "photoUrl": "//lh5.googleusercontent.com/-EucKC7DmcQI/AAAAAAAAAAI/AAAAAAAAAGA/gQU1NPEmNFA/s50-c-k-no/photo.jpg",
      "userId": "105654037995838004821"
     },
     "user_tz": -540
    },
    "id": "PnifSBJglzHh",
    "outputId": "94177345-918e-4a96-d9a8-d8aba0a4bc9a",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# make an environment\n",
    "# env = gym.make('CartPole-v0')\n",
    "# env = gym.make('CartPole-v1')\n",
    "env = gym.make('MountainCar-v0')\n",
    "# env = gym.make('LunarLander-v2')\n",
    "\n",
    "env.seed(SEED)\n",
    "obs_space = env.observation_space.shape[0]\n",
    "action_space = env.action_space.n\n",
    "\n",
    "# hyperparameter\n",
    "n_episodes = 3000\n",
    "roll_len = 128\n",
    "n_eval = env.spec.trials\n",
    "\n",
    "# global values\n",
    "init_steps = 0\n",
    "steps = 0\n",
    "learn_steps = 0\n",
    "mean = 0.\n",
    "std = 0.\n",
    "ep_rewards = []\n",
    "is_rollout = False\n",
    "is_solved = False\n",
    "is_init_roll = True\n",
    "\n",
    "# make nerual networks\n",
    "net = ActorCriticNet(obs_space, action_space).to(device)\n",
    "old_net = deepcopy(net)\n",
    "pred_net = PredictNet(obs_space).to(device)\n",
    "rand_net = RandomNet(obs_space).to(device)\n",
    "\n",
    "# make optimizer\n",
    "net_optim = torch.optim.Adam(net.parameters(), lr=LR, eps=1e-5)\n",
    "pred_optim = torch.optim.Adam(pred_net.parameters(), lr=LR, eps=1e-5)\n",
    "\n",
    "# make a rollout memory\n",
    "train_memory = []\n",
    "roll_memory = []\n",
    "obs_memory = []\n",
    "rewards = []\n",
    "values = []\n",
    "dones = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "use_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.spec.max_episode_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.spec.trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-110.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.spec.reward_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 3377
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 135196,
     "status": "ok",
     "timestamp": 1534482559393,
     "user": {
      "displayName": "윤승제",
      "photoUrl": "//lh5.googleusercontent.com/-EucKC7DmcQI/AAAAAAAAAAI/AAAAAAAAAGA/gQU1NPEmNFA/s50-c-k-no/photo.jpg",
      "userId": "105654037995838004821"
     },
     "user_tz": -540
    },
    "id": "PnifSBJglzHh",
    "outputId": "94177345-918e-4a96-d9a8-d8aba0a4bc9a"
   },
   "outputs": [],
   "source": [
    "# simulation\n",
    "while True:\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        env.render()\n",
    "        action = env.action_space.sample()\n",
    "        _obs, _, done, _ = env.step(action)\n",
    "        obs_memory.append(_obs)\n",
    "        obs = _obs\n",
    "        init_steps += 1\n",
    "        if init_steps == roll_len * 50:\n",
    "            mean, std = get_norm_params(obs_memory)\n",
    "            obs_memory.clear()\n",
    "            is_init_roll = False\n",
    "            break\n",
    "    if not is_init_roll:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 3377
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 135196,
     "status": "ok",
     "timestamp": 1534482559393,
     "user": {
      "displayName": "윤승제",
      "photoUrl": "//lh5.googleusercontent.com/-EucKC7DmcQI/AAAAAAAAAAI/AAAAAAAAAGA/gQU1NPEmNFA/s50-c-k-no/photo.jpg",
      "userId": "105654037995838004821"
     },
     "user_tz": -540
    },
    "id": "PnifSBJglzHh",
    "outputId": "94177345-918e-4a96-d9a8-d8aba0a4bc9a",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1 Episode in   200 steps, reward_ex -200.00, reward_in 188.02\n",
      "  2 Episode in   400 steps, reward_ex -200.00, reward_in 150.89\n",
      "  3 Episode in   600 steps, reward_ex -200.00, reward_in 193.58\n",
      "  4 Episode in   800 steps, reward_ex -200.00, reward_in 182.59\n",
      "  5 Episode in  1000 steps, reward_ex -200.00, reward_in 197.42\n",
      "  6 Episode in  1200 steps, reward_ex -200.00, reward_in 145.04\n",
      "  7 Episode in  1400 steps, reward_ex -200.00, reward_in 107.68\n",
      "  8 Episode in  1600 steps, reward_ex -200.00, reward_in 153.32\n",
      "  9 Episode in  1800 steps, reward_ex -200.00, reward_in 73.19\n",
      " 10 Episode in  2000 steps, reward_ex -200.00, reward_in 120.36\n",
      " 11 Episode in  2200 steps, reward_ex -200.00, reward_in 81.53\n",
      " 12 Episode in  2400 steps, reward_ex -200.00, reward_in 109.04\n",
      " 13 Episode in  2600 steps, reward_ex -200.00, reward_in 51.10\n",
      " 14 Episode in  2800 steps, reward_ex -200.00, reward_in 44.77\n",
      " 15 Episode in  3000 steps, reward_ex -200.00, reward_in 75.20\n",
      " 16 Episode in  3200 steps, reward_ex -200.00, reward_in 122.29\n",
      " 17 Episode in  3400 steps, reward_ex -200.00, reward_in 143.05\n",
      " 18 Episode in  3600 steps, reward_ex -200.00, reward_in 135.95\n",
      " 19 Episode in  3800 steps, reward_ex -200.00, reward_in 55.08\n",
      " 20 Episode in  4000 steps, reward_ex -200.00, reward_in 51.32\n",
      " 21 Episode in  4200 steps, reward_ex -200.00, reward_in 54.09\n",
      " 22 Episode in  4400 steps, reward_ex -200.00, reward_in 53.37\n",
      " 23 Episode in  4600 steps, reward_ex -200.00, reward_in 72.92\n",
      " 24 Episode in  4800 steps, reward_ex -200.00, reward_in 47.02\n",
      " 25 Episode in  5000 steps, reward_ex -200.00, reward_in 44.29\n",
      " 26 Episode in  5200 steps, reward_ex -200.00, reward_in 63.30\n",
      " 27 Episode in  5400 steps, reward_ex -200.00, reward_in 43.40\n",
      " 28 Episode in  5600 steps, reward_ex -200.00, reward_in 24.08\n",
      " 29 Episode in  5800 steps, reward_ex -200.00, reward_in 30.19\n",
      " 30 Episode in  6000 steps, reward_ex -200.00, reward_in 113.91\n",
      " 31 Episode in  6200 steps, reward_ex -200.00, reward_in 37.39\n",
      " 32 Episode in  6400 steps, reward_ex -200.00, reward_in 86.25\n",
      " 33 Episode in  6600 steps, reward_ex -200.00, reward_in 24.98\n",
      " 34 Episode in  6800 steps, reward_ex -200.00, reward_in 19.25\n",
      " 35 Episode in  7000 steps, reward_ex -200.00, reward_in 15.51\n",
      " 36 Episode in  7200 steps, reward_ex -200.00, reward_in 118.93\n",
      " 37 Episode in  7400 steps, reward_ex -200.00, reward_in 53.42\n",
      " 38 Episode in  7600 steps, reward_ex -200.00, reward_in 20.05\n",
      " 39 Episode in  7800 steps, reward_ex -200.00, reward_in 88.00\n",
      " 40 Episode in  8000 steps, reward_ex -200.00, reward_in 56.96\n",
      " 41 Episode in  8200 steps, reward_ex -200.00, reward_in 13.95\n",
      " 42 Episode in  8400 steps, reward_ex -200.00, reward_in 29.82\n",
      " 43 Episode in  8600 steps, reward_ex -200.00, reward_in 117.69\n",
      " 44 Episode in  8800 steps, reward_ex -200.00, reward_in 125.06\n",
      " 45 Episode in  9000 steps, reward_ex -200.00, reward_in 91.50\n",
      " 46 Episode in  9200 steps, reward_ex -200.00, reward_in 27.88\n",
      " 47 Episode in  9400 steps, reward_ex -200.00, reward_in 27.47\n",
      " 48 Episode in  9600 steps, reward_ex -200.00, reward_in 20.42\n",
      " 49 Episode in  9800 steps, reward_ex -200.00, reward_in 20.15\n",
      " 50 Episode in 10000 steps, reward_ex -200.00, reward_in 13.83\n",
      " 51 Episode in 10200 steps, reward_ex -200.00, reward_in 20.69\n",
      " 52 Episode in 10400 steps, reward_ex -200.00, reward_in 13.59\n",
      " 53 Episode in 10600 steps, reward_ex -200.00, reward_in 14.18\n",
      " 54 Episode in 10800 steps, reward_ex -200.00, reward_in 20.06\n",
      " 55 Episode in 11000 steps, reward_ex -200.00, reward_in 13.06\n",
      " 56 Episode in 11200 steps, reward_ex -200.00, reward_in 13.79\n",
      " 57 Episode in 11400 steps, reward_ex -200.00, reward_in 10.20\n",
      " 58 Episode in 11600 steps, reward_ex -200.00, reward_in 133.81\n",
      " 59 Episode in 11800 steps, reward_ex -200.00, reward_in 29.48\n",
      " 60 Episode in 12000 steps, reward_ex -200.00, reward_in 50.14\n",
      " 61 Episode in 12200 steps, reward_ex -200.00, reward_in 15.45\n",
      " 62 Episode in 12400 steps, reward_ex -200.00, reward_in 11.42\n",
      " 63 Episode in 12600 steps, reward_ex -200.00, reward_in 22.22\n",
      " 64 Episode in 12800 steps, reward_ex -200.00, reward_in 15.75\n",
      " 65 Episode in 13000 steps, reward_ex -200.00, reward_in 19.58\n",
      " 66 Episode in 13200 steps, reward_ex -200.00, reward_in 11.90\n",
      " 67 Episode in 13400 steps, reward_ex -200.00, reward_in 10.31\n",
      " 68 Episode in 13600 steps, reward_ex -200.00, reward_in 10.58\n",
      " 69 Episode in 13800 steps, reward_ex -200.00, reward_in 19.03\n",
      " 70 Episode in 14000 steps, reward_ex -200.00, reward_in 20.36\n",
      " 71 Episode in 14200 steps, reward_ex -200.00, reward_in 16.87\n",
      " 72 Episode in 14400 steps, reward_ex -200.00, reward_in 17.12\n",
      " 73 Episode in 14600 steps, reward_ex -200.00, reward_in 12.72\n",
      " 74 Episode in 14800 steps, reward_ex -200.00, reward_in 31.27\n",
      " 75 Episode in 15000 steps, reward_ex -200.00, reward_in 10.60\n",
      " 76 Episode in 15200 steps, reward_ex -200.00, reward_in 7.68\n",
      " 77 Episode in 15400 steps, reward_ex -200.00, reward_in 23.88\n",
      " 78 Episode in 15600 steps, reward_ex -200.00, reward_in 21.77\n",
      " 79 Episode in 15800 steps, reward_ex -200.00, reward_in 24.56\n",
      " 80 Episode in 16000 steps, reward_ex -200.00, reward_in 16.75\n",
      " 81 Episode in 16200 steps, reward_ex -200.00, reward_in 7.66\n",
      " 82 Episode in 16400 steps, reward_ex -200.00, reward_in 36.11\n",
      " 83 Episode in 16600 steps, reward_ex -200.00, reward_in 18.95\n",
      " 84 Episode in 16800 steps, reward_ex -200.00, reward_in 9.02\n",
      " 85 Episode in 17000 steps, reward_ex -200.00, reward_in 9.68\n",
      " 86 Episode in 17200 steps, reward_ex -200.00, reward_in 13.24\n",
      " 87 Episode in 17400 steps, reward_ex -200.00, reward_in 9.26\n",
      " 88 Episode in 17600 steps, reward_ex -200.00, reward_in 28.21\n",
      " 89 Episode in 17800 steps, reward_ex -200.00, reward_in 14.14\n",
      " 90 Episode in 18000 steps, reward_ex -200.00, reward_in 11.74\n",
      " 91 Episode in 18200 steps, reward_ex -200.00, reward_in 17.79\n",
      " 92 Episode in 18400 steps, reward_ex -200.00, reward_in 17.29\n",
      " 93 Episode in 18600 steps, reward_ex -200.00, reward_in 17.77\n",
      " 94 Episode in 18800 steps, reward_ex -200.00, reward_in 9.16\n",
      " 95 Episode in 19000 steps, reward_ex -200.00, reward_in 11.06\n",
      " 96 Episode in 19200 steps, reward_ex -200.00, reward_in 7.50\n",
      " 97 Episode in 19400 steps, reward_ex -200.00, reward_in 7.28\n",
      " 98 Episode in 19600 steps, reward_ex -200.00, reward_in 38.72\n",
      " 99 Episode in 19800 steps, reward_ex -200.00, reward_in 26.59\n",
      "100 Episode in 20000 steps, reward_ex -200.00, reward_in 8.28\n",
      "101 Episode in 20200 steps, reward_ex -200.00, reward_in 6.49\n",
      "102 Episode in 20400 steps, reward_ex -200.00, reward_in 50.31\n",
      "103 Episode in 20600 steps, reward_ex -200.00, reward_in 33.84\n",
      "104 Episode in 20800 steps, reward_ex -200.00, reward_in 29.24\n",
      "105 Episode in 21000 steps, reward_ex -200.00, reward_in 7.49\n",
      "106 Episode in 21200 steps, reward_ex -200.00, reward_in 30.35\n",
      "107 Episode in 21400 steps, reward_ex -200.00, reward_in 6.51\n",
      "108 Episode in 21600 steps, reward_ex -200.00, reward_in 25.23\n",
      "109 Episode in 21800 steps, reward_ex -200.00, reward_in 20.00\n",
      "110 Episode in 22000 steps, reward_ex -200.00, reward_in 5.75\n",
      "111 Episode in 22200 steps, reward_ex -200.00, reward_in 8.26\n",
      "112 Episode in 22400 steps, reward_ex -200.00, reward_in 5.63\n",
      "113 Episode in 22600 steps, reward_ex -200.00, reward_in 5.70\n",
      "114 Episode in 22800 steps, reward_ex -200.00, reward_in 17.64\n",
      "115 Episode in 23000 steps, reward_ex -200.00, reward_in 47.78\n",
      "116 Episode in 23200 steps, reward_ex -200.00, reward_in 27.03\n",
      "117 Episode in 23400 steps, reward_ex -200.00, reward_in 9.44\n",
      "118 Episode in 23600 steps, reward_ex -200.00, reward_in 9.63\n",
      "119 Episode in 23800 steps, reward_ex -200.00, reward_in 13.13\n",
      "120 Episode in 24000 steps, reward_ex -200.00, reward_in 8.45\n",
      "121 Episode in 24200 steps, reward_ex -200.00, reward_in 5.86\n",
      "122 Episode in 24400 steps, reward_ex -200.00, reward_in 6.75\n",
      "123 Episode in 24600 steps, reward_ex -200.00, reward_in 14.55\n",
      "124 Episode in 24800 steps, reward_ex -200.00, reward_in 12.54\n",
      "125 Episode in 25000 steps, reward_ex -200.00, reward_in 8.63\n",
      "126 Episode in 25200 steps, reward_ex -200.00, reward_in 13.99\n",
      "127 Episode in 25400 steps, reward_ex -200.00, reward_in 9.89\n",
      "128 Episode in 25600 steps, reward_ex -200.00, reward_in 35.79\n",
      "129 Episode in 25800 steps, reward_ex -200.00, reward_in 80.47\n",
      "130 Episode in 26000 steps, reward_ex -200.00, reward_in 10.79\n",
      "131 Episode in 26200 steps, reward_ex -200.00, reward_in 11.24\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132 Episode in 26400 steps, reward_ex -200.00, reward_in 11.35\n",
      "133 Episode in 26600 steps, reward_ex -200.00, reward_in 9.07\n",
      "134 Episode in 26800 steps, reward_ex -200.00, reward_in 10.05\n",
      "135 Episode in 27000 steps, reward_ex -200.00, reward_in 10.54\n",
      "136 Episode in 27200 steps, reward_ex -200.00, reward_in 12.63\n",
      "137 Episode in 27400 steps, reward_ex -200.00, reward_in 7.31\n",
      "138 Episode in 27600 steps, reward_ex -200.00, reward_in 7.41\n",
      "139 Episode in 27800 steps, reward_ex -200.00, reward_in 5.37\n",
      "140 Episode in 28000 steps, reward_ex -200.00, reward_in 8.94\n",
      "141 Episode in 28200 steps, reward_ex -200.00, reward_in 6.25\n",
      "142 Episode in 28400 steps, reward_ex -200.00, reward_in 8.24\n",
      "143 Episode in 28600 steps, reward_ex -200.00, reward_in 46.84\n",
      "144 Episode in 28800 steps, reward_ex -200.00, reward_in 136.63\n",
      "145 Episode in 29000 steps, reward_ex -200.00, reward_in 40.44\n",
      "146 Episode in 29200 steps, reward_ex -200.00, reward_in 5.59\n",
      "147 Episode in 29400 steps, reward_ex -200.00, reward_in 5.58\n",
      "148 Episode in 29600 steps, reward_ex -200.00, reward_in 6.93\n",
      "149 Episode in 29800 steps, reward_ex -200.00, reward_in 5.94\n",
      "150 Episode in 30000 steps, reward_ex -200.00, reward_in 31.26\n",
      "151 Episode in 30200 steps, reward_ex -200.00, reward_in 29.14\n",
      "152 Episode in 30400 steps, reward_ex -200.00, reward_in 41.58\n",
      "153 Episode in 30600 steps, reward_ex -200.00, reward_in 37.07\n",
      "154 Episode in 30800 steps, reward_ex -200.00, reward_in 8.95\n",
      "155 Episode in 31000 steps, reward_ex -200.00, reward_in 6.97\n",
      "156 Episode in 31200 steps, reward_ex -200.00, reward_in 8.09\n",
      "157 Episode in 31400 steps, reward_ex -200.00, reward_in 4.64\n",
      "158 Episode in 31600 steps, reward_ex -200.00, reward_in 53.84\n",
      "159 Episode in 31800 steps, reward_ex -200.00, reward_in 9.63\n",
      "160 Episode in 32000 steps, reward_ex -200.00, reward_in 11.86\n",
      "161 Episode in 32200 steps, reward_ex -200.00, reward_in 11.88\n",
      "162 Episode in 32400 steps, reward_ex -200.00, reward_in 5.37\n",
      "163 Episode in 32600 steps, reward_ex -200.00, reward_in 7.04\n",
      "164 Episode in 32800 steps, reward_ex -200.00, reward_in 5.59\n",
      "165 Episode in 33000 steps, reward_ex -200.00, reward_in 68.00\n",
      "166 Episode in 33200 steps, reward_ex -200.00, reward_in 14.71\n",
      "167 Episode in 33400 steps, reward_ex -200.00, reward_in 20.63\n",
      "168 Episode in 33600 steps, reward_ex -200.00, reward_in 6.53\n",
      "169 Episode in 33800 steps, reward_ex -200.00, reward_in 5.81\n",
      "170 Episode in 34000 steps, reward_ex -200.00, reward_in 4.30\n",
      "171 Episode in 34200 steps, reward_ex -200.00, reward_in 3.79\n",
      "172 Episode in 34400 steps, reward_ex -200.00, reward_in 5.44\n",
      "173 Episode in 34600 steps, reward_ex -200.00, reward_in 5.04\n",
      "174 Episode in 34800 steps, reward_ex -200.00, reward_in 8.53\n",
      "175 Episode in 35000 steps, reward_ex -200.00, reward_in 5.02\n",
      "176 Episode in 35200 steps, reward_ex -200.00, reward_in 5.19\n",
      "177 Episode in 35400 steps, reward_ex -200.00, reward_in 4.07\n",
      "178 Episode in 35600 steps, reward_ex -200.00, reward_in 8.65\n",
      "179 Episode in 35800 steps, reward_ex -200.00, reward_in 12.98\n",
      "180 Episode in 36000 steps, reward_ex -200.00, reward_in 11.16\n",
      "181 Episode in 36200 steps, reward_ex -200.00, reward_in 24.01\n",
      "182 Episode in 36400 steps, reward_ex -200.00, reward_in 35.77\n",
      "183 Episode in 36600 steps, reward_ex -200.00, reward_in 6.29\n",
      "184 Episode in 36800 steps, reward_ex -200.00, reward_in 8.17\n",
      "185 Episode in 37000 steps, reward_ex -200.00, reward_in 5.49\n",
      "186 Episode in 37200 steps, reward_ex -200.00, reward_in 4.99\n",
      "187 Episode in 37400 steps, reward_ex -200.00, reward_in 12.14\n",
      "188 Episode in 37600 steps, reward_ex -200.00, reward_in 8.37\n",
      "189 Episode in 37800 steps, reward_ex -200.00, reward_in 5.58\n",
      "190 Episode in 38000 steps, reward_ex -200.00, reward_in 15.50\n",
      "191 Episode in 38200 steps, reward_ex -200.00, reward_in 9.25\n",
      "192 Episode in 38400 steps, reward_ex -200.00, reward_in 36.87\n",
      "193 Episode in 38600 steps, reward_ex -200.00, reward_in 11.25\n",
      "194 Episode in 38800 steps, reward_ex -200.00, reward_in 5.36\n",
      "195 Episode in 39000 steps, reward_ex -200.00, reward_in 10.77\n",
      "196 Episode in 39200 steps, reward_ex -200.00, reward_in 4.64\n",
      "197 Episode in 39400 steps, reward_ex -200.00, reward_in 5.39\n",
      "198 Episode in 39600 steps, reward_ex -200.00, reward_in 7.66\n",
      "199 Episode in 39800 steps, reward_ex -200.00, reward_in 4.33\n",
      "200 Episode in 40000 steps, reward_ex -200.00, reward_in 7.71\n",
      "201 Episode in 40200 steps, reward_ex -200.00, reward_in 5.25\n",
      "202 Episode in 40400 steps, reward_ex -200.00, reward_in 6.23\n",
      "203 Episode in 40600 steps, reward_ex -200.00, reward_in 5.98\n",
      "204 Episode in 40800 steps, reward_ex -200.00, reward_in 48.42\n",
      "205 Episode in 41000 steps, reward_ex -200.00, reward_in 13.02\n",
      "206 Episode in 41200 steps, reward_ex -200.00, reward_in 6.61\n",
      "207 Episode in 41400 steps, reward_ex -200.00, reward_in 12.15\n",
      "208 Episode in 41600 steps, reward_ex -200.00, reward_in 6.93\n",
      "209 Episode in 41800 steps, reward_ex -200.00, reward_in 55.86\n",
      "210 Episode in 42000 steps, reward_ex -200.00, reward_in 54.97\n",
      "211 Episode in 42200 steps, reward_ex -200.00, reward_in 7.44\n",
      "212 Episode in 42400 steps, reward_ex -200.00, reward_in 5.13\n",
      "213 Episode in 42600 steps, reward_ex -200.00, reward_in 32.39\n",
      "214 Episode in 42800 steps, reward_ex -200.00, reward_in 8.38\n",
      "215 Episode in 43000 steps, reward_ex -200.00, reward_in 3.75\n",
      "216 Episode in 43200 steps, reward_ex -200.00, reward_in 9.65\n",
      "217 Episode in 43400 steps, reward_ex -200.00, reward_in 4.79\n",
      "218 Episode in 43600 steps, reward_ex -200.00, reward_in 4.19\n",
      "219 Episode in 43800 steps, reward_ex -200.00, reward_in 30.26\n",
      "220 Episode in 44000 steps, reward_ex -200.00, reward_in 11.91\n",
      "221 Episode in 44200 steps, reward_ex -200.00, reward_in 5.07\n",
      "222 Episode in 44400 steps, reward_ex -200.00, reward_in 4.18\n",
      "223 Episode in 44600 steps, reward_ex -200.00, reward_in 4.08\n",
      "224 Episode in 44800 steps, reward_ex -200.00, reward_in 6.34\n",
      "225 Episode in 45000 steps, reward_ex -200.00, reward_in 4.11\n",
      "226 Episode in 45200 steps, reward_ex -200.00, reward_in 7.07\n",
      "227 Episode in 45400 steps, reward_ex -200.00, reward_in 6.51\n",
      "228 Episode in 45600 steps, reward_ex -200.00, reward_in 42.98\n",
      "229 Episode in 45800 steps, reward_ex -200.00, reward_in 35.83\n",
      "230 Episode in 46000 steps, reward_ex -200.00, reward_in 14.32\n",
      "231 Episode in 46200 steps, reward_ex -200.00, reward_in 7.08\n",
      "232 Episode in 46400 steps, reward_ex -200.00, reward_in 4.18\n",
      "233 Episode in 46600 steps, reward_ex -200.00, reward_in 3.76\n",
      "234 Episode in 46800 steps, reward_ex -200.00, reward_in 4.23\n",
      "235 Episode in 47000 steps, reward_ex -200.00, reward_in 7.53\n",
      "236 Episode in 47200 steps, reward_ex -200.00, reward_in 4.94\n",
      "237 Episode in 47400 steps, reward_ex -200.00, reward_in 16.97\n",
      "238 Episode in 47600 steps, reward_ex -200.00, reward_in 13.52\n",
      "239 Episode in 47800 steps, reward_ex -200.00, reward_in 5.20\n",
      "240 Episode in 48000 steps, reward_ex -200.00, reward_in 9.11\n",
      "241 Episode in 48200 steps, reward_ex -200.00, reward_in 6.65\n",
      "242 Episode in 48400 steps, reward_ex -200.00, reward_in 3.96\n",
      "243 Episode in 48600 steps, reward_ex -200.00, reward_in 3.28\n",
      "244 Episode in 48800 steps, reward_ex -200.00, reward_in 16.89\n",
      "245 Episode in 49000 steps, reward_ex -200.00, reward_in 43.68\n",
      "246 Episode in 49200 steps, reward_ex -200.00, reward_in 84.50\n",
      "247 Episode in 49400 steps, reward_ex -200.00, reward_in 14.99\n",
      "248 Episode in 49600 steps, reward_ex -200.00, reward_in 4.36\n",
      "249 Episode in 49800 steps, reward_ex -200.00, reward_in 15.59\n",
      "250 Episode in 50000 steps, reward_ex -200.00, reward_in 5.71\n",
      "251 Episode in 50200 steps, reward_ex -200.00, reward_in 3.36\n",
      "252 Episode in 50400 steps, reward_ex -200.00, reward_in 4.35\n",
      "253 Episode in 50600 steps, reward_ex -200.00, reward_in 5.71\n",
      "254 Episode in 50800 steps, reward_ex -200.00, reward_in 4.71\n",
      "255 Episode in 51000 steps, reward_ex -200.00, reward_in 3.04\n",
      "256 Episode in 51200 steps, reward_ex -200.00, reward_in 56.00\n",
      "257 Episode in 51400 steps, reward_ex -200.00, reward_in 6.81\n",
      "258 Episode in 51600 steps, reward_ex -200.00, reward_in 3.86\n",
      "259 Episode in 51800 steps, reward_ex -200.00, reward_in 4.00\n",
      "260 Episode in 52000 steps, reward_ex -200.00, reward_in 4.63\n",
      "261 Episode in 52200 steps, reward_ex -200.00, reward_in 3.77\n",
      "262 Episode in 52400 steps, reward_ex -200.00, reward_in 3.36\n",
      "263 Episode in 52600 steps, reward_ex -200.00, reward_in 7.74\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "264 Episode in 52800 steps, reward_ex -200.00, reward_in 11.73\n",
      "265 Episode in 53000 steps, reward_ex -200.00, reward_in 3.57\n",
      "266 Episode in 53200 steps, reward_ex -200.00, reward_in 3.20\n",
      "267 Episode in 53400 steps, reward_ex -200.00, reward_in 4.14\n",
      "268 Episode in 53600 steps, reward_ex -200.00, reward_in 68.84\n",
      "269 Episode in 53800 steps, reward_ex -200.00, reward_in 16.07\n",
      "270 Episode in 54000 steps, reward_ex -200.00, reward_in 3.41\n",
      "271 Episode in 54200 steps, reward_ex -200.00, reward_in 4.56\n",
      "272 Episode in 54400 steps, reward_ex -200.00, reward_in 6.61\n",
      "273 Episode in 54600 steps, reward_ex -200.00, reward_in 3.60\n",
      "274 Episode in 54800 steps, reward_ex -200.00, reward_in 2.82\n",
      "275 Episode in 55000 steps, reward_ex -200.00, reward_in 4.50\n",
      "276 Episode in 55200 steps, reward_ex -200.00, reward_in 7.89\n",
      "277 Episode in 55400 steps, reward_ex -200.00, reward_in 3.80\n",
      "278 Episode in 55600 steps, reward_ex -200.00, reward_in 3.87\n",
      "279 Episode in 55800 steps, reward_ex -200.00, reward_in 6.74\n",
      "280 Episode in 56000 steps, reward_ex -200.00, reward_in 13.99\n",
      "281 Episode in 56200 steps, reward_ex -200.00, reward_in 84.17\n",
      "282 Episode in 56400 steps, reward_ex -200.00, reward_in 7.36\n",
      "283 Episode in 56600 steps, reward_ex -200.00, reward_in 7.09\n",
      "284 Episode in 56800 steps, reward_ex -200.00, reward_in 4.20\n",
      "285 Episode in 57000 steps, reward_ex -200.00, reward_in 3.61\n",
      "286 Episode in 57200 steps, reward_ex -200.00, reward_in 43.97\n",
      "287 Episode in 57400 steps, reward_ex -200.00, reward_in 38.62\n",
      "288 Episode in 57600 steps, reward_ex -200.00, reward_in 3.88\n",
      "289 Episode in 57800 steps, reward_ex -200.00, reward_in 9.95\n",
      "290 Episode in 58000 steps, reward_ex -200.00, reward_in 3.16\n",
      "291 Episode in 58200 steps, reward_ex -200.00, reward_in 5.23\n",
      "292 Episode in 58400 steps, reward_ex -200.00, reward_in 6.00\n",
      "293 Episode in 58600 steps, reward_ex -200.00, reward_in 3.73\n",
      "294 Episode in 58800 steps, reward_ex -200.00, reward_in 3.20\n",
      "295 Episode in 59000 steps, reward_ex -200.00, reward_in 3.56\n",
      "296 Episode in 59200 steps, reward_ex -200.00, reward_in 2.97\n",
      "297 Episode in 59400 steps, reward_ex -200.00, reward_in 6.97\n",
      "298 Episode in 59600 steps, reward_ex -200.00, reward_in 12.85\n",
      "299 Episode in 59800 steps, reward_ex -200.00, reward_in 6.75\n",
      "300 Episode in 60000 steps, reward_ex -200.00, reward_in 4.75\n",
      "301 Episode in 60200 steps, reward_ex -200.00, reward_in 4.95\n",
      "302 Episode in 60400 steps, reward_ex -200.00, reward_in 10.72\n",
      "303 Episode in 60600 steps, reward_ex -200.00, reward_in 4.68\n",
      "304 Episode in 60800 steps, reward_ex -200.00, reward_in 3.29\n",
      "305 Episode in 61000 steps, reward_ex -200.00, reward_in 27.92\n",
      "306 Episode in 61200 steps, reward_ex -200.00, reward_in 9.55\n",
      "307 Episode in 61400 steps, reward_ex -200.00, reward_in 3.28\n",
      "308 Episode in 61600 steps, reward_ex -200.00, reward_in 28.23\n",
      "309 Episode in 61800 steps, reward_ex -200.00, reward_in 11.71\n",
      "310 Episode in 62000 steps, reward_ex -200.00, reward_in 4.02\n",
      "311 Episode in 62200 steps, reward_ex -200.00, reward_in 34.15\n",
      "312 Episode in 62400 steps, reward_ex -200.00, reward_in 5.11\n",
      "313 Episode in 62600 steps, reward_ex -200.00, reward_in 4.21\n",
      "314 Episode in 62800 steps, reward_ex -200.00, reward_in 3.28\n",
      "315 Episode in 63000 steps, reward_ex -200.00, reward_in 2.53\n",
      "316 Episode in 63200 steps, reward_ex -200.00, reward_in 3.59\n",
      "317 Episode in 63400 steps, reward_ex -200.00, reward_in 7.26\n",
      "318 Episode in 63600 steps, reward_ex -200.00, reward_in 7.42\n",
      "319 Episode in 63800 steps, reward_ex -200.00, reward_in 21.96\n",
      "320 Episode in 64000 steps, reward_ex -200.00, reward_in 6.44\n",
      "321 Episode in 64200 steps, reward_ex -200.00, reward_in 3.78\n",
      "322 Episode in 64400 steps, reward_ex -200.00, reward_in 3.89\n",
      "323 Episode in 64600 steps, reward_ex -200.00, reward_in 3.01\n",
      "324 Episode in 64800 steps, reward_ex -200.00, reward_in 7.15\n",
      "325 Episode in 65000 steps, reward_ex -200.00, reward_in 12.80\n",
      "326 Episode in 65200 steps, reward_ex -200.00, reward_in 4.80\n",
      "327 Episode in 65400 steps, reward_ex -200.00, reward_in 3.01\n",
      "328 Episode in 65600 steps, reward_ex -200.00, reward_in 15.47\n",
      "329 Episode in 65800 steps, reward_ex -200.00, reward_in 8.98\n",
      "330 Episode in 66000 steps, reward_ex -200.00, reward_in 7.25\n",
      "331 Episode in 66200 steps, reward_ex -200.00, reward_in 15.47\n",
      "332 Episode in 66400 steps, reward_ex -200.00, reward_in 5.01\n",
      "333 Episode in 66600 steps, reward_ex -200.00, reward_in 3.88\n",
      "334 Episode in 66800 steps, reward_ex -200.00, reward_in 3.32\n",
      "335 Episode in 67000 steps, reward_ex -200.00, reward_in 5.86\n",
      "336 Episode in 67200 steps, reward_ex -200.00, reward_in 8.03\n",
      "337 Episode in 67400 steps, reward_ex -200.00, reward_in 6.78\n",
      "338 Episode in 67600 steps, reward_ex -200.00, reward_in 17.05\n",
      "339 Episode in 67800 steps, reward_ex -200.00, reward_in 8.39\n",
      "340 Episode in 68000 steps, reward_ex -200.00, reward_in 13.99\n",
      "341 Episode in 68200 steps, reward_ex -200.00, reward_in 6.74\n",
      "342 Episode in 68400 steps, reward_ex -200.00, reward_in 3.64\n",
      "343 Episode in 68600 steps, reward_ex -200.00, reward_in 4.41\n",
      "344 Episode in 68800 steps, reward_ex -200.00, reward_in 4.51\n",
      "345 Episode in 69000 steps, reward_ex -200.00, reward_in 3.75\n",
      "346 Episode in 69200 steps, reward_ex -200.00, reward_in 31.79\n",
      "347 Episode in 69400 steps, reward_ex -200.00, reward_in 14.29\n",
      "348 Episode in 69600 steps, reward_ex -200.00, reward_in 3.77\n",
      "349 Episode in 69800 steps, reward_ex -200.00, reward_in 5.46\n",
      "350 Episode in 70000 steps, reward_ex -200.00, reward_in 3.55\n",
      "351 Episode in 70200 steps, reward_ex -200.00, reward_in 4.19\n",
      "352 Episode in 70400 steps, reward_ex -200.00, reward_in 4.62\n",
      "353 Episode in 70600 steps, reward_ex -200.00, reward_in 2.40\n",
      "354 Episode in 70800 steps, reward_ex -200.00, reward_in 4.35\n",
      "355 Episode in 71000 steps, reward_ex -200.00, reward_in 42.26\n",
      "356 Episode in 71200 steps, reward_ex -200.00, reward_in 4.80\n",
      "357 Episode in 71400 steps, reward_ex -200.00, reward_in 8.67\n",
      "358 Episode in 71600 steps, reward_ex -200.00, reward_in 3.61\n",
      "359 Episode in 71800 steps, reward_ex -200.00, reward_in 3.98\n",
      "360 Episode in 72000 steps, reward_ex -200.00, reward_in 2.83\n",
      "361 Episode in 72200 steps, reward_ex -200.00, reward_in 10.93\n",
      "362 Episode in 72400 steps, reward_ex -200.00, reward_in 17.17\n",
      "363 Episode in 72600 steps, reward_ex -200.00, reward_in 8.08\n",
      "364 Episode in 72800 steps, reward_ex -200.00, reward_in 3.31\n",
      "365 Episode in 73000 steps, reward_ex -200.00, reward_in 4.87\n",
      "366 Episode in 73200 steps, reward_ex -200.00, reward_in 2.68\n",
      "367 Episode in 73400 steps, reward_ex -200.00, reward_in 8.29\n",
      "368 Episode in 73600 steps, reward_ex -200.00, reward_in 5.25\n",
      "369 Episode in 73800 steps, reward_ex -200.00, reward_in 4.01\n",
      "370 Episode in 74000 steps, reward_ex -200.00, reward_in 2.97\n",
      "371 Episode in 74200 steps, reward_ex -200.00, reward_in 9.51\n",
      "372 Episode in 74400 steps, reward_ex -200.00, reward_in 39.62\n",
      "373 Episode in 74600 steps, reward_ex -200.00, reward_in 9.21\n",
      "374 Episode in 74800 steps, reward_ex -200.00, reward_in 3.67\n",
      "375 Episode in 75000 steps, reward_ex -200.00, reward_in 3.02\n",
      "376 Episode in 75200 steps, reward_ex -200.00, reward_in 2.52\n",
      "377 Episode in 75400 steps, reward_ex -200.00, reward_in 2.49\n",
      "378 Episode in 75600 steps, reward_ex -200.00, reward_in 131.82\n",
      "379 Episode in 75800 steps, reward_ex -200.00, reward_in 26.06\n",
      "380 Episode in 76000 steps, reward_ex -200.00, reward_in 3.78\n",
      "381 Episode in 76200 steps, reward_ex -200.00, reward_in 13.81\n",
      "382 Episode in 76400 steps, reward_ex -200.00, reward_in 43.56\n",
      "383 Episode in 76600 steps, reward_ex -200.00, reward_in 8.42\n",
      "384 Episode in 76800 steps, reward_ex -200.00, reward_in 18.54\n",
      "385 Episode in 77000 steps, reward_ex -200.00, reward_in 3.30\n",
      "386 Episode in 77200 steps, reward_ex -200.00, reward_in 2.73\n",
      "387 Episode in 77400 steps, reward_ex -200.00, reward_in 3.79\n",
      "388 Episode in 77600 steps, reward_ex -200.00, reward_in 2.46\n",
      "389 Episode in 77800 steps, reward_ex -200.00, reward_in 40.73\n",
      "390 Episode in 78000 steps, reward_ex -200.00, reward_in 3.35\n",
      "391 Episode in 78200 steps, reward_ex -200.00, reward_in 2.75\n",
      "392 Episode in 78400 steps, reward_ex -200.00, reward_in 2.87\n",
      "393 Episode in 78600 steps, reward_ex -200.00, reward_in 5.10\n",
      "394 Episode in 78800 steps, reward_ex -200.00, reward_in 8.96\n",
      "395 Episode in 79000 steps, reward_ex -200.00, reward_in 3.74\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "396 Episode in 79200 steps, reward_ex -200.00, reward_in 2.51\n",
      "397 Episode in 79400 steps, reward_ex -200.00, reward_in 2.53\n",
      "398 Episode in 79600 steps, reward_ex -200.00, reward_in 3.18\n",
      "399 Episode in 79800 steps, reward_ex -200.00, reward_in 3.42\n",
      "400 Episode in 80000 steps, reward_ex -200.00, reward_in 2.50\n",
      "401 Episode in 80200 steps, reward_ex -200.00, reward_in 2.32\n",
      "402 Episode in 80400 steps, reward_ex -200.00, reward_in 5.13\n",
      "403 Episode in 80600 steps, reward_ex -200.00, reward_in 3.28\n",
      "404 Episode in 80800 steps, reward_ex -200.00, reward_in 2.73\n",
      "405 Episode in 81000 steps, reward_ex -200.00, reward_in 3.02\n",
      "406 Episode in 81200 steps, reward_ex -200.00, reward_in 6.66\n",
      "407 Episode in 81400 steps, reward_ex -200.00, reward_in 11.88\n",
      "408 Episode in 81600 steps, reward_ex -200.00, reward_in 4.25\n",
      "409 Episode in 81800 steps, reward_ex -200.00, reward_in 3.74\n",
      "410 Episode in 82000 steps, reward_ex -200.00, reward_in 3.31\n",
      "411 Episode in 82200 steps, reward_ex -200.00, reward_in 2.68\n",
      "412 Episode in 82400 steps, reward_ex -200.00, reward_in 2.85\n",
      "413 Episode in 82600 steps, reward_ex -200.00, reward_in 5.30\n",
      "414 Episode in 82800 steps, reward_ex -200.00, reward_in 45.73\n",
      "415 Episode in 83000 steps, reward_ex -200.00, reward_in 11.84\n",
      "416 Episode in 83200 steps, reward_ex -200.00, reward_in 16.04\n",
      "417 Episode in 83400 steps, reward_ex -200.00, reward_in 5.66\n",
      "418 Episode in 83600 steps, reward_ex -200.00, reward_in 3.25\n",
      "419 Episode in 83800 steps, reward_ex -200.00, reward_in 2.53\n",
      "420 Episode in 84000 steps, reward_ex -200.00, reward_in 2.50\n",
      "421 Episode in 84200 steps, reward_ex -200.00, reward_in 7.09\n",
      "422 Episode in 84400 steps, reward_ex -200.00, reward_in 14.03\n",
      "423 Episode in 84600 steps, reward_ex -200.00, reward_in 10.67\n",
      "424 Episode in 84800 steps, reward_ex -200.00, reward_in 3.83\n",
      "425 Episode in 85000 steps, reward_ex -200.00, reward_in 2.32\n",
      "426 Episode in 85200 steps, reward_ex -200.00, reward_in 11.29\n",
      "427 Episode in 85400 steps, reward_ex -200.00, reward_in 5.12\n",
      "428 Episode in 85600 steps, reward_ex -200.00, reward_in 2.65\n",
      "429 Episode in 85800 steps, reward_ex -200.00, reward_in 9.41\n",
      "430 Episode in 86000 steps, reward_ex -200.00, reward_in 13.09\n",
      "431 Episode in 86200 steps, reward_ex -200.00, reward_in 34.15\n",
      "432 Episode in 86400 steps, reward_ex -200.00, reward_in 8.20\n",
      "433 Episode in 86600 steps, reward_ex -200.00, reward_in 5.56\n",
      "434 Episode in 86800 steps, reward_ex -200.00, reward_in 2.98\n",
      "435 Episode in 87000 steps, reward_ex -200.00, reward_in 5.77\n",
      "436 Episode in 87200 steps, reward_ex -200.00, reward_in 2.39\n",
      "437 Episode in 87400 steps, reward_ex -200.00, reward_in 7.20\n",
      "438 Episode in 87600 steps, reward_ex -200.00, reward_in 2.88\n",
      "439 Episode in 87800 steps, reward_ex -200.00, reward_in 2.19\n",
      "440 Episode in 88000 steps, reward_ex -200.00, reward_in 21.81\n",
      "441 Episode in 88200 steps, reward_ex -200.00, reward_in 4.79\n",
      "442 Episode in 88400 steps, reward_ex -200.00, reward_in 4.11\n",
      "443 Episode in 88600 steps, reward_ex -200.00, reward_in 2.40\n",
      "444 Episode in 88800 steps, reward_ex -200.00, reward_in 2.22\n",
      "445 Episode in 89000 steps, reward_ex -200.00, reward_in 39.88\n",
      "446 Episode in 89200 steps, reward_ex -200.00, reward_in 9.18\n",
      "447 Episode in 89400 steps, reward_ex -200.00, reward_in 2.07\n",
      "448 Episode in 89600 steps, reward_ex -200.00, reward_in 1.79\n",
      "449 Episode in 89800 steps, reward_ex -200.00, reward_in 4.47\n",
      "450 Episode in 90000 steps, reward_ex -200.00, reward_in 2.83\n",
      "451 Episode in 90200 steps, reward_ex -200.00, reward_in 10.24\n",
      "452 Episode in 90400 steps, reward_ex -200.00, reward_in 6.65\n",
      "453 Episode in 90600 steps, reward_ex -200.00, reward_in 2.68\n",
      "454 Episode in 90800 steps, reward_ex -200.00, reward_in 3.32\n",
      "455 Episode in 91000 steps, reward_ex -200.00, reward_in 2.42\n",
      "456 Episode in 91200 steps, reward_ex -200.00, reward_in 42.26\n",
      "457 Episode in 91400 steps, reward_ex -200.00, reward_in 7.25\n",
      "458 Episode in 91600 steps, reward_ex -200.00, reward_in 2.70\n",
      "459 Episode in 91800 steps, reward_ex -200.00, reward_in 40.66\n",
      "460 Episode in 92000 steps, reward_ex -200.00, reward_in 9.50\n",
      "461 Episode in 92200 steps, reward_ex -200.00, reward_in 3.11\n",
      "462 Episode in 92400 steps, reward_ex -200.00, reward_in 33.53\n",
      "463 Episode in 92600 steps, reward_ex -200.00, reward_in 5.05\n",
      "464 Episode in 92800 steps, reward_ex -200.00, reward_in 3.94\n",
      "465 Episode in 93000 steps, reward_ex -200.00, reward_in 2.73\n",
      "466 Episode in 93200 steps, reward_ex -200.00, reward_in 1.91\n",
      "467 Episode in 93400 steps, reward_ex -200.00, reward_in 5.99\n",
      "468 Episode in 93600 steps, reward_ex -200.00, reward_in 2.75\n",
      "469 Episode in 93800 steps, reward_ex -200.00, reward_in 1.94\n",
      "470 Episode in 94000 steps, reward_ex -200.00, reward_in 4.30\n",
      "471 Episode in 94200 steps, reward_ex -200.00, reward_in 18.56\n",
      "472 Episode in 94400 steps, reward_ex -200.00, reward_in 3.37\n",
      "473 Episode in 94600 steps, reward_ex -200.00, reward_in 4.06\n",
      "474 Episode in 94800 steps, reward_ex -200.00, reward_in 2.66\n",
      "475 Episode in 95000 steps, reward_ex -200.00, reward_in 2.54\n",
      "476 Episode in 95200 steps, reward_ex -200.00, reward_in 33.85\n",
      "477 Episode in 95400 steps, reward_ex -200.00, reward_in 9.90\n",
      "478 Episode in 95600 steps, reward_ex -200.00, reward_in 8.28\n",
      "479 Episode in 95800 steps, reward_ex -200.00, reward_in 4.98\n",
      "480 Episode in 96000 steps, reward_ex -200.00, reward_in 2.57\n",
      "481 Episode in 96200 steps, reward_ex -200.00, reward_in 2.36\n",
      "482 Episode in 96400 steps, reward_ex -200.00, reward_in 1.99\n",
      "483 Episode in 96600 steps, reward_ex -200.00, reward_in 3.88\n",
      "484 Episode in 96800 steps, reward_ex -200.00, reward_in 3.72\n",
      "485 Episode in 97000 steps, reward_ex -200.00, reward_in 13.74\n",
      "486 Episode in 97200 steps, reward_ex -200.00, reward_in 9.57\n",
      "487 Episode in 97400 steps, reward_ex -200.00, reward_in 7.47\n",
      "488 Episode in 97600 steps, reward_ex -200.00, reward_in 3.05\n",
      "489 Episode in 97800 steps, reward_ex -200.00, reward_in 10.77\n",
      "490 Episode in 98000 steps, reward_ex -200.00, reward_in 5.43\n",
      "491 Episode in 98200 steps, reward_ex -200.00, reward_in 2.00\n",
      "492 Episode in 98400 steps, reward_ex -200.00, reward_in 5.09\n",
      "493 Episode in 98600 steps, reward_ex -200.00, reward_in 2.58\n",
      "494 Episode in 98800 steps, reward_ex -200.00, reward_in 2.25\n",
      "495 Episode in 99000 steps, reward_ex -200.00, reward_in 3.56\n",
      "496 Episode in 99200 steps, reward_ex -200.00, reward_in 2.34\n",
      "497 Episode in 99400 steps, reward_ex -200.00, reward_in 17.74\n",
      "498 Episode in 99600 steps, reward_ex -200.00, reward_in 7.39\n",
      "499 Episode in 99800 steps, reward_ex -200.00, reward_in 15.96\n",
      "500 Episode in 100000 steps, reward_ex -200.00, reward_in 5.84\n",
      "501 Episode in 100200 steps, reward_ex -200.00, reward_in 4.05\n",
      "502 Episode in 100400 steps, reward_ex -200.00, reward_in 6.45\n",
      "503 Episode in 100600 steps, reward_ex -200.00, reward_in 15.53\n",
      "504 Episode in 100800 steps, reward_ex -200.00, reward_in 2.97\n",
      "505 Episode in 101000 steps, reward_ex -200.00, reward_in 2.35\n",
      "506 Episode in 101200 steps, reward_ex -200.00, reward_in 1.96\n",
      "507 Episode in 101400 steps, reward_ex -200.00, reward_in 2.37\n",
      "508 Episode in 101600 steps, reward_ex -200.00, reward_in 4.92\n",
      "509 Episode in 101800 steps, reward_ex -200.00, reward_in 6.28\n",
      "510 Episode in 102000 steps, reward_ex -200.00, reward_in 2.03\n"
     ]
    }
   ],
   "source": [
    "# play!\n",
    "for i in range(1, n_episodes + 1):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    ep_reward_ex = 0.\n",
    "    ep_reward_in = 0.\n",
    "    while not done:\n",
    "        env.render()\n",
    "\n",
    "        action, val_ex, val_in = get_action_and_value(obs, old_net)\n",
    "        _obs, rew_ex, done, _ = env.step(action)\n",
    "        \n",
    "        rew_in = calculate_reward_in(pred_net, rand_net, _obs)    \n",
    "        \n",
    "        reward = EX_COEF * rew_ex + IN_COEF * rew_in\n",
    "        value = val_ex + val_in\n",
    "        \n",
    "        # store\n",
    "        roll_memory.append([obs, action, EX_COEF * rew_ex, IN_COEF * rew_in, _obs, done])\n",
    "        obs_memory.append(_obs)\n",
    "        rewards.append(reward)\n",
    "        values.append(value)\n",
    "        dones.append(done)\n",
    "        \n",
    "        obs = _obs\n",
    "        steps += 1\n",
    "        ep_reward_ex += rew_ex\n",
    "        ep_reward_in += rew_in\n",
    "        \n",
    "        if done or steps % roll_len == 0:\n",
    "            _, _val_ex, _val_in = get_action_and_value(_obs, old_net)\n",
    "            _value = _val_ex + _val_in\n",
    "            values.append(_value)\n",
    "            train_memory.extend(compute_adv(rewards, values, dones, roll_memory))\n",
    "            rewards.clear()\n",
    "            values.clear()\n",
    "            dones.clear()\n",
    "            roll_memory.clear()\n",
    "            \n",
    "        if steps % roll_len == 0:\n",
    "#             print('\\n============  Start Learning  ============\\n')\n",
    "            learn(net, old_net, pred_net, rand_net, net_optim, pred_optim, train_memory)\n",
    "            learn_steps += 1\n",
    "            train_memory.clear()\n",
    "\n",
    "        if learn_steps > 1:\n",
    "            old_net.load_state_dict(net.state_dict())\n",
    "            learn_steps = 1\n",
    "        \n",
    "        if steps % roll_len*50 == 0:\n",
    "            mean, std = get_norm_params(obs_memory)\n",
    "            obs_memory.clear()\n",
    "    \n",
    "    if done:        \n",
    "        ep_rewards.append(ep_reward_ex)\n",
    "        print('{:3} Episode in {:5} steps, reward_ex {:.2f}, reward_in {:.2f}'.format(\n",
    "            i, steps, ep_reward_ex, ep_reward_in))\n",
    "\n",
    "        if len(ep_rewards) >= n_eval:\n",
    "            if np.mean(list(reversed(ep_rewards))[: n_eval]) >= env.spec.reward_threshold:\n",
    "                print('\\n{} is sloved! {:3} Episode in {:3} steps'.format(\n",
    "                    env.spec.id, i, steps))\n",
    "                torch.save(old_net.state_dict(),\n",
    "                           f'./test/saved_models/{env.spec.id}_ep{i}_clear_model_ppo.pt')\n",
    "                break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "plt.title('Reward')\n",
    "plt.plot(ep_rewards)\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.title('Loss')\n",
    "plt.plot(losses)\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.title('m_Loss')\n",
    "plt.plot(m_losses)\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.title('f_Loss')\n",
    "plt.plot(f_losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[\n",
    "    ('CartPole-v0', 161, 32, 256),\n",
    "    ('CartPole-v1', 162, 32, 256),\n",
    "    ('MountainCar-v0', 660),\n",
    "    ('LunarLander-v2', 260)\n",
    "]"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "C51_tensorflow.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
