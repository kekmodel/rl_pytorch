{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IWnm3qot3o1W"
   },
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "from copy import deepcopy\n",
    "\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 708,
     "status": "ok",
     "timestamp": 1534482400648,
     "user": {
      "displayName": "윤승제",
      "photoUrl": "//lh5.googleusercontent.com/-EucKC7DmcQI/AAAAAAAAAAI/AAAAAAAAAGA/gQU1NPEmNFA/s50-c-k-no/photo.jpg",
      "userId": "105654037995838004821"
     },
     "user_tz": -540
    },
    "id": "maRVADiTlzHD",
    "outputId": "783b7610-95c2-4b54-b2ce-d8e853c484ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda: True\n"
     ]
    }
   ],
   "source": [
    "SEED = 1\n",
    "BATCH_SIZE = 16\n",
    "LR = 0.0003\n",
    "EPOCHS = 4\n",
    "ALPHA = 0.0001\n",
    "CLIP = 0.1\n",
    "GAMMA = 0.99\n",
    "LAMBDA = 0.95\n",
    "ENT_COEF = 0.01\n",
    "\n",
    "# set device\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print('cuda:', use_cuda)\n",
    "device = torch.device('cuda' if use_cuda else 'cpu')\n",
    "\n",
    "# random seed\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if use_cuda:\n",
    "    torch.cuda.manual_seed_all(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9Ffkl_5C4R81"
   },
   "outputs": [],
   "source": [
    "class ActorCriticNet(nn.Module):\n",
    "    def __init__(self, obs_space, action_space):\n",
    "        super().__init__()\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(obs_space, 256),\n",
    "            nn.SELU()\n",
    "        )\n",
    "\n",
    "        self.pol = nn.Sequential(\n",
    "            nn.Linear(256, 256),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(256, action_space)\n",
    "        )\n",
    "        \n",
    "        self.val_in = nn.Sequential(\n",
    "            nn.Linear(256, 256),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "        \n",
    "        self.val_ex = nn.Sequential(\n",
    "            nn.Linear(256, 256),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "        \n",
    "        self.log_softmax = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.head(x)\n",
    "        logit = self.pol(out).reshape(out.shape[0], -1)\n",
    "        value_in = self.val_in(out).reshape(out.shape[0], 1)\n",
    "        value_ex = self.val_ex(out).reshape(out.shape[0], 1)\n",
    "        log_probs = self.log_softmax(logit)\n",
    "        \n",
    "        return log_probs, value_in, value_ex\n",
    "    \n",
    "class RandomNet(nn.Module):\n",
    "    def __init__(self, obs_space):\n",
    "        super().__init__()\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(obs_space, 256),\n",
    "            nn.SELU()\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(256, 256),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(256, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.head(x)\n",
    "        obs_feature = self.fc(out).reshape(out.shape[0], -1)\n",
    "\n",
    "        return obs_feature\n",
    "\n",
    "\n",
    "class PredictNet(nn.Module):\n",
    "    def __init__(self, obs_space):\n",
    "        super().__init__()\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(obs_space, 256),\n",
    "            nn.SELU()\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(256, 512),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(512, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.head(x)\n",
    "        obs_feature = self.fc(out).reshape(out.shape[0], -1)\n",
    "\n",
    "        return obs_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "\n",
    "\n",
    "def learn(net, old_net, optimizer, train_memory):\n",
    "    global action_space\n",
    "\n",
    "    net.train()\n",
    "    old_net.train()\n",
    "    for i in range(EPOCHS):\n",
    "        dataloader = DataLoader(train_memory,\n",
    "                                batch_size=BATCH_SIZE,\n",
    "                                shuffle=True,\n",
    "                                pin_memory=use_cuda)\n",
    "        for (s, a, adv) in dataloader:\n",
    "            s_batch = s.to(device).float()\n",
    "            a_batch = a.detach().to(device).long()\n",
    "            adv_batch = adv.to(device).float()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                log_p_batch_old, v_batch_old = old_net(s_batch)\n",
    "                log_p_acting_old = log_p_batch_old[range(BATCH_SIZE), a_batch]\n",
    "\n",
    "            log_p_batch, v_batch = net(s_batch)\n",
    "            log_p_acting = log_p_batch[range(BATCH_SIZE), a_batch]\n",
    "\n",
    "            p_ratio = (log_p_acting - log_p_acting_old).exp()\n",
    "            p_ratio_clip = torch.clamp(p_ratio, 1 - CLIP, 1 + CLIP)\n",
    "            clip_loss = torch.min(p_ratio * adv_batch,\n",
    "                                  p_ratio_clip * adv_batch)\n",
    "\n",
    "            v_loss = (v_batch - v_batch_old).pow(2)\n",
    "            entropy = -(log_p_batch.exp() * log_p_batch).sum()\n",
    "\n",
    "            # loss\n",
    "            loss = -(clip_loss - v_loss + ENT_COEF * entropy).mean()\n",
    "            losses.append(loss)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "def get_action_and_value(obs, old_net):\n",
    "    old_net.eval()\n",
    "    with torch.no_grad():\n",
    "        state = torch.tensor([obs]).to(device).float()\n",
    "        log_p, v_in, v_ex = old_net(state)\n",
    "        m = Categorical(log_p.exp())\n",
    "        action = m.sample()\n",
    "\n",
    "    return action.item(), v_in.item(), v_ex.item()\n",
    "\n",
    "\n",
    "def compute_adv(rewards, values, roll_memory):\n",
    "    rew = np.array(rewards, 'float')\n",
    "    val = np.array(values[:-1], 'float')\n",
    "    _val = np.array(values[1:], 'float')\n",
    "    delta = rew + 0.99 * _val - val\n",
    "    disc_dt = np.array(\n",
    "        [(GAMMA * LAMBDA)**(i) * dt for i, dt in enumerate(delta.tolist())],\n",
    "        'float')\n",
    "    for i, data in enumerate(roll_memory):\n",
    "        data.append(sum(disc_dt[i:]/(GAMMA * LAMBDA)**(i)))\n",
    "\n",
    "    return roll_memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 3377
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 135196,
     "status": "ok",
     "timestamp": 1534482559393,
     "user": {
      "displayName": "윤승제",
      "photoUrl": "//lh5.googleusercontent.com/-EucKC7DmcQI/AAAAAAAAAAI/AAAAAAAAAGA/gQU1NPEmNFA/s50-c-k-no/photo.jpg",
      "userId": "105654037995838004821"
     },
     "user_tz": -540
    },
    "id": "PnifSBJglzHh",
    "outputId": "94177345-918e-4a96-d9a8-d8aba0a4bc9a",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# make an environment\n",
    "# env = gym.make('CartPole-v0')\n",
    "# env = gym.make('CartPole-v1')\n",
    "env = gym.make('MountainCar-v0')\n",
    "# env = gym.make('LunarLander-v2')\n",
    "\n",
    "env.seed(SEED)\n",
    "obs_space = env.observation_space.shape[0]\n",
    "action_space = env.action_space.n\n",
    "\n",
    "# hyperparameter\n",
    "n_episodes = 1000\n",
    "roll_len = 128\n",
    "n_init = 3000\n",
    "n_eval = env.spec.trials\n",
    "\n",
    "# global values\n",
    "steps = 0\n",
    "learn_steps = 0\n",
    "ep_rewards = []\n",
    "reward_eval = deque(maxlen=n_eval)\n",
    "is_rollout = False\n",
    "is_solved = False\n",
    "\n",
    "# make nerual networks\n",
    "net = ActorCriticNet(obs_space, action_space).to(device)\n",
    "old_net = deepcopy(net)\n",
    "pred_net = PredictNet(obs_space).to(device)\n",
    "rand_net = RandomNet(obs_space).to(device)\n",
    "\n",
    "# make a optimizer\n",
    "net_optim = optim.Adam(net.parameters(), lr=LR)\n",
    "pred_optim = optim.Adam(pred_net.parameters(), lr=LR)\n",
    "\n",
    "# make a rollout memory\n",
    "train_memory = []\n",
    "roll_memory = []\n",
    "obs_memory = []\n",
    "rewards = []\n",
    "values = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "use_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.spec.max_episode_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.spec.trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-110.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.spec.reward_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 3377
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 135196,
     "status": "ok",
     "timestamp": 1534482559393,
     "user": {
      "displayName": "윤승제",
      "photoUrl": "//lh5.googleusercontent.com/-EucKC7DmcQI/AAAAAAAAAAI/AAAAAAAAAGA/gQU1NPEmNFA/s50-c-k-no/photo.jpg",
      "userId": "105654037995838004821"
     },
     "user_tz": -540
    },
    "id": "PnifSBJglzHh",
    "outputId": "94177345-918e-4a96-d9a8-d8aba0a4bc9a",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1 Episode in   200 steps, reward -200.00\n",
      "  2 Episode in   400 steps, reward -200.00\n",
      "  3 Episode in   600 steps, reward -200.00\n",
      "  4 Episode in   800 steps, reward -200.00\n",
      "  5 Episode in  1000 steps, reward -200.00\n",
      "  6 Episode in  1200 steps, reward -200.00\n",
      "  7 Episode in  1400 steps, reward -200.00\n",
      "  8 Episode in  1600 steps, reward -200.00\n",
      "  9 Episode in  1800 steps, reward -200.00\n",
      " 10 Episode in  2000 steps, reward -200.00\n",
      " 11 Episode in  2200 steps, reward -200.00\n",
      " 12 Episode in  2400 steps, reward -200.00\n",
      " 13 Episode in  2600 steps, reward -200.00\n",
      " 14 Episode in  2800 steps, reward -200.00\n",
      " 15 Episode in  3000 steps, reward -200.00\n",
      " 16 Episode in  3200 steps, reward -200.00\n",
      " 17 Episode in  3400 steps, reward -200.00\n",
      " 18 Episode in  3600 steps, reward -200.00\n",
      " 19 Episode in  3800 steps, reward -200.00\n",
      " 20 Episode in  4000 steps, reward -200.00\n",
      " 21 Episode in  4200 steps, reward -200.00\n",
      " 22 Episode in  4400 steps, reward -200.00\n",
      " 23 Episode in  4600 steps, reward -200.00\n",
      " 24 Episode in  4800 steps, reward -200.00\n",
      " 25 Episode in  5000 steps, reward -200.00\n",
      " 26 Episode in  5200 steps, reward -200.00\n",
      " 27 Episode in  5400 steps, reward -200.00\n",
      " 28 Episode in  5600 steps, reward -200.00\n",
      " 29 Episode in  5800 steps, reward -200.00\n",
      " 30 Episode in  6000 steps, reward -200.00\n",
      " 31 Episode in  6200 steps, reward -200.00\n",
      " 32 Episode in  6400 steps, reward -200.00\n",
      " 33 Episode in  6600 steps, reward -200.00\n",
      " 34 Episode in  6800 steps, reward -200.00\n",
      " 35 Episode in  7000 steps, reward -200.00\n",
      " 36 Episode in  7200 steps, reward -200.00\n",
      " 37 Episode in  7400 steps, reward -200.00\n",
      " 38 Episode in  7600 steps, reward -200.00\n",
      " 39 Episode in  7800 steps, reward -200.00\n",
      " 40 Episode in  8000 steps, reward -200.00\n",
      " 41 Episode in  8200 steps, reward -200.00\n",
      " 42 Episode in  8400 steps, reward -200.00\n",
      " 43 Episode in  8600 steps, reward -200.00\n",
      " 44 Episode in  8800 steps, reward -200.00\n",
      " 45 Episode in  9000 steps, reward -200.00\n",
      " 46 Episode in  9200 steps, reward -200.00\n",
      " 47 Episode in  9400 steps, reward -200.00\n",
      " 48 Episode in  9600 steps, reward -200.00\n",
      " 49 Episode in  9800 steps, reward -200.00\n",
      " 50 Episode in 10000 steps, reward -200.00\n",
      " 51 Episode in 10200 steps, reward -200.00\n",
      " 52 Episode in 10400 steps, reward -200.00\n",
      " 53 Episode in 10600 steps, reward -200.00\n",
      " 54 Episode in 10800 steps, reward -200.00\n",
      " 55 Episode in 11000 steps, reward -200.00\n",
      " 56 Episode in 11200 steps, reward -200.00\n",
      " 57 Episode in 11400 steps, reward -200.00\n",
      " 58 Episode in 11600 steps, reward -200.00\n",
      " 59 Episode in 11800 steps, reward -200.00\n",
      " 60 Episode in 12000 steps, reward -200.00\n",
      " 61 Episode in 12200 steps, reward -200.00\n",
      " 62 Episode in 12400 steps, reward -200.00\n",
      " 63 Episode in 12600 steps, reward -200.00\n",
      " 64 Episode in 12800 steps, reward -200.00\n",
      " 65 Episode in 13000 steps, reward -200.00\n",
      " 66 Episode in 13200 steps, reward -200.00\n",
      " 67 Episode in 13400 steps, reward -200.00\n",
      " 68 Episode in 13600 steps, reward -200.00\n",
      " 69 Episode in 13800 steps, reward -200.00\n",
      " 70 Episode in 14000 steps, reward -200.00\n",
      " 71 Episode in 14200 steps, reward -200.00\n",
      " 72 Episode in 14400 steps, reward -200.00\n",
      " 73 Episode in 14600 steps, reward -200.00\n",
      " 74 Episode in 14800 steps, reward -200.00\n"
     ]
    }
   ],
   "source": [
    "for i in range(n_init):\n",
    "    \n",
    "    \n",
    "# play!\n",
    "for i in range(1, n_episodes + 1):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    ep_reward = 0\n",
    "    while not done:\n",
    "        env.render()\n",
    "\n",
    "        action, value = get_action_and_value(obs, old_net)\n",
    "        _obs, reward, done, _ = env.step(action)\n",
    "        \n",
    "        # store\n",
    "        roll_memory.append([obs,action])\n",
    "        rewards.append(reward)\n",
    "        values.append(value)\n",
    "        \n",
    "        obs = _obs\n",
    "        steps += 1\n",
    "        ep_reward += reward\n",
    "        \n",
    "        if done or steps % roll_len == 0:\n",
    "            _, _value = get_action_and_value(_obs, old_net)\n",
    "            values.append(_value)\n",
    "            train_memory.extend(compute_adv(rewards, values, roll_memory))\n",
    "            rewards.clear()\n",
    "            values.clear()\n",
    "            roll_memory.clear()\n",
    "            \n",
    "        if steps % roll_len == 0:\n",
    "#             print('\\n============  Start Learning  ============\\n')\n",
    "            learn(net, old_net, optimizer, train_memory)\n",
    "            learn_steps += 1\n",
    "            train_memory.clear()\n",
    "\n",
    "        if learn_steps > 1:\n",
    "            old_net.load_state_dict(net.state_dict())\n",
    "            learn_steps = 1\n",
    "    \n",
    "    if done:        \n",
    "        ep_rewards.append(ep_reward)\n",
    "        print('{:3} Episode in {:5} steps, reward {:.2f}'.format(\n",
    "            i, steps, ep_reward))\n",
    "\n",
    "        if len(ep_rewards) >= n_eval:\n",
    "            if np.mean(list(reversed(ep_rewards))[: n_eval]) >= env.spec.reward_threshold:\n",
    "                print('\\n{} is sloved! {:3} Episode in {:3} steps'.format(\n",
    "                    env.spec.id, i, steps))\n",
    "                torch.save(old_net.state_dict(),\n",
    "                           f'./test/saved_models/{env.spec.id}_ep{i}_clear_model_ppo.pt')\n",
    "                break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "plt.title('Reward')\n",
    "plt.plot(ep_rewards)\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.title('Loss')\n",
    "plt.plot(losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[\n",
    "    ('CartPole-v0', 126, 16, 128),\n",
    "    ('CartPole-v1', 225, 64, 256),\n",
    "    ('MountainCar-v0', 660),\n",
    "    ('LunarLander-v2', 260)\n",
    "]"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "C51_tensorflow.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
