{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IWnm3qot3o1W"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import deque\n",
    "from copy import deepcopy\n",
    "\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 708,
     "status": "ok",
     "timestamp": 1534482400648,
     "user": {
      "displayName": "윤승제",
      "photoUrl": "//lh5.googleusercontent.com/-EucKC7DmcQI/AAAAAAAAAAI/AAAAAAAAAGA/gQU1NPEmNFA/s50-c-k-no/photo.jpg",
      "userId": "105654037995838004821"
     },
     "user_tz": -540
    },
    "id": "maRVADiTlzHD",
    "outputId": "783b7610-95c2-4b54-b2ce-d8e853c484ba"
   },
   "outputs": [],
   "source": [
    "SEED = 1\n",
    "BATCH_SIZE = 256\n",
    "LR_DQN = 0.0003\n",
    "LR_RND = 0.0003\n",
    "UP_COEF = 0.1\n",
    "EX_COEF = 0.5\n",
    "IN_COEF = 0.5\n",
    "GAMMA = 0.99\n",
    "EPS = 1e-8\n",
    "\n",
    "# set device\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device('cuda' if use_cuda else 'cpu')\n",
    "\n",
    "# random seed\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if use_cuda:\n",
    "    torch.cuda.manual_seed_all(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9Ffkl_5C4R81"
   },
   "outputs": [],
   "source": [
    "class DuelingDQN(nn.Module):\n",
    "    def __init__(self, obs_space, action_space):\n",
    "        super().__init__()\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(obs_space, 256),\n",
    "            nn.SELU(),\n",
    "#             nn.Linear(256, 256),\n",
    "#             nn.SELU()\n",
    "        )\n",
    "\n",
    "        self.val = nn.Sequential(\n",
    "            nn.Linear(256, 256),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "\n",
    "        self.adv = nn.Sequential(\n",
    "            nn.Linear(256, 256),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(256, action_space)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.head(x)\n",
    "        val_out = self.val(out).reshape(out.shape[0], 1)\n",
    "        adv_out = self.adv(out).reshape(out.shape[0], -1)\n",
    "        adv_mean = adv_out.mean(dim=1, keepdim=True)\n",
    "        q = val_out + adv_out - adv_mean\n",
    "\n",
    "        return q\n",
    "\n",
    "\n",
    "class RandomNet(nn.Module):\n",
    "    def __init__(self, obs_space):\n",
    "        super().__init__()\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(obs_space, 256),\n",
    "            nn.SELU()\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(256, 256),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(256, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.head(x)\n",
    "        obs_feature = self.fc(out).reshape(out.shape[0], -1)\n",
    "\n",
    "        return obs_feature\n",
    "\n",
    "\n",
    "class PredictNet(nn.Module):\n",
    "    def __init__(self, obs_space):\n",
    "        super().__init__()\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(obs_space, 256),\n",
    "            nn.SELU()\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(256, 512),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(512, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.head(x)\n",
    "        obs_feature = self.fc(out).reshape(out.shape[0], -1)\n",
    "\n",
    "        return obs_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "f_losses = []\n",
    "\n",
    "\n",
    "def learn(net, tgt_net, pred_net, rand_net, net_optim, pred_optim, rep_memory):\n",
    "    global mean\n",
    "    global std\n",
    "\n",
    "    net.train()\n",
    "    tgt_net.train()\n",
    "    pred_net.train()\n",
    "    rand_net.train()\n",
    "\n",
    "    train_data = []\n",
    "    train_data.extend(random.sample(rep_memory, BATCH_SIZE))\n",
    "\n",
    "    dataloader = DataLoader(\n",
    "        train_data,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        pin_memory=use_cuda\n",
    "    )\n",
    "\n",
    "    # double DQN\n",
    "    for i, (s, a, r_ex, r_in, _s, d) in enumerate(dataloader):\n",
    "        s_batch = s.to(device).float()\n",
    "        a_batch = a.detach().to(device).long()\n",
    "        _s_batch = _s.to(device).float()\n",
    "\n",
    "        _s_norm = normalize_obs(_s.detach().cpu().numpy(), mean, std)\n",
    "        _s_norm_batch = torch.tensor(_s_norm).to(device).float()\n",
    "        r_ex_batch = r_ex.detach().to(device).float()\n",
    "        r_in_batch = r_in.detach().to(device).float()\n",
    "        r_batch = EX_COEF * r_ex_batch + IN_COEF * r_in_batch\n",
    "        is_done = 1. - d.detach().reshape(BATCH_SIZE, 1).to(device).float()\n",
    "\n",
    "        _q_batch = net(_s_batch)\n",
    "        _a_batch = torch.argmax(_q_batch, dim=1)\n",
    "        pred_features = pred_net(_s_norm_batch)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            _q_batch_tgt = tgt_net(_s_batch)\n",
    "            action_space = _q_batch_tgt.shape[1]\n",
    "            done_mask = torch.cat(\n",
    "                tuple(is_done for _ in range(action_space)), dim=1)\n",
    "            _q_batch_tgt_masked = _q_batch_tgt * done_mask\n",
    "            _q_best_tgt = _q_batch_tgt_masked[range(BATCH_SIZE), _a_batch]\n",
    "            rand_features = rand_net(_s_norm_batch)\n",
    "\n",
    "        q_batch = net(s_batch)\n",
    "        q_acting = q_batch[range(BATCH_SIZE), a_batch]\n",
    "\n",
    "        # loss\n",
    "        loss = ((r_batch + GAMMA*_q_best_tgt) - q_acting).pow(2).mean()\n",
    "        losses.append(loss)\n",
    "\n",
    "        f_loss = (pred_features - rand_features).pow(2).sum(dim=1).mean()\n",
    "        f_losses.append(f_loss)\n",
    "\n",
    "        net_optim.zero_grad()\n",
    "        loss.backward()\n",
    "        net_optim.step()\n",
    "\n",
    "        pred_optim.zero_grad()\n",
    "        f_loss.backward()\n",
    "        pred_optim.step()\n",
    "\n",
    "\n",
    "def select_action(obs, tgt_net):\n",
    "    tgt_net.eval()\n",
    "    with torch.no_grad():\n",
    "        state = torch.tensor([obs]).to(device).float()\n",
    "        q = target_net(state)\n",
    "        action = torch.argmax(q)\n",
    "\n",
    "    return action.item()\n",
    "\n",
    "\n",
    "def get_norm_params(obs_memory):\n",
    "    global obs_apace\n",
    "\n",
    "    obses = [[] for _ in range(obs_space)]\n",
    "    for obs in obs_memory:\n",
    "        for j in range(obs_space):\n",
    "            obses[j].append(obs[j])\n",
    "\n",
    "    mean = np.zeros(obs_space, np.float32)\n",
    "    std = np.zeros(obs_space, np.float32)\n",
    "    for i, obs_ in enumerate(obses):\n",
    "        mean[i] = np.mean(obs_)\n",
    "        std[i] = np.std(obs_)\n",
    "    return mean, std\n",
    "\n",
    "\n",
    "def normalize_obs(obs, mean, std):\n",
    "    means = [mean for _ in range(BATCH_SIZE)]\n",
    "    stds = [std for _ in range(BATCH_SIZE)]\n",
    "    mean = np.stack(means)\n",
    "    std = np.stack(stds)\n",
    "    norm_obs = (obs - mean) / std\n",
    "\n",
    "#     return np.clip(norm_obs, -5, 5)\n",
    "    return norm_obs\n",
    "\n",
    "\n",
    "def calculate_reward_in(pred_net, rand_net, obs):\n",
    "    global mean\n",
    "    global std\n",
    "\n",
    "    norm_obs = normalize_obs(obs, mean, std)\n",
    "    state = torch.tensor([norm_obs]).to(device).float()\n",
    "    with torch.no_grad():\n",
    "        pred_obs = pred_net(state)\n",
    "        rand_obs = rand_net(state)\n",
    "        reward = (pred_obs - rand_obs).pow(2).sum()\n",
    "        clipped_reward = torch.clamp(reward, -1, 1)\n",
    "\n",
    "    return clipped_reward.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 3377
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 135196,
     "status": "ok",
     "timestamp": 1534482559393,
     "user": {
      "displayName": "윤승제",
      "photoUrl": "//lh5.googleusercontent.com/-EucKC7DmcQI/AAAAAAAAAAI/AAAAAAAAAGA/gQU1NPEmNFA/s50-c-k-no/photo.jpg",
      "userId": "105654037995838004821"
     },
     "user_tz": -540
    },
    "id": "PnifSBJglzHh",
    "outputId": "94177345-918e-4a96-d9a8-d8aba0a4bc9a",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# make an environment\n",
    "# env = gym.make('CartPole-v0')\n",
    "# env = gym.make('CartPole-v1')\n",
    "env = gym.make('MountainCar-v0')\n",
    "# env = gym.make('LunarLander-v2')\n",
    "\n",
    "env.seed(SEED)\n",
    "obs_space = env.observation_space.shape[0]\n",
    "action_space = env.action_space.n\n",
    "\n",
    "# hyperparameter\n",
    "n_episodes = 1000\n",
    "learn_start = 1500\n",
    "memory_size = 50000\n",
    "update_frq = 1\n",
    "use_eps_decay = False\n",
    "epsilon = 0.001\n",
    "eps_min = 0.001\n",
    "decay_rate = 0.0001\n",
    "n_eval = env.spec.trials\n",
    "\n",
    "# global values\n",
    "total_steps = 0\n",
    "learn_steps = 0\n",
    "rewards = []\n",
    "reward_eval = deque(maxlen=n_eval)\n",
    "is_learned = False\n",
    "is_solved = False\n",
    "\n",
    "# make four nerual networks\n",
    "net = DuelingDQN(obs_space, action_space).to(device)\n",
    "target_net = deepcopy(net)\n",
    "pred_net = PredictNet(obs_space).to(device)\n",
    "rand_net = RandomNet(obs_space).to(device)\n",
    "\n",
    "# make optimizer\n",
    "net_optim = torch.optim.Adam(net.parameters(), lr=LR_DQN, eps=EPS)\n",
    "pred_optim = torch.optim.Adam(pred_net.parameters(), lr=LR_RND, eps=EPS)\n",
    "\n",
    "# make memory\n",
    "rep_memory = deque(maxlen=memory_size)\n",
    "obs_memory = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "use_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.spec.max_episode_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.spec.trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-110.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.spec.reward_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 3377
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 135196,
     "status": "ok",
     "timestamp": 1534482559393,
     "user": {
      "displayName": "윤승제",
      "photoUrl": "//lh5.googleusercontent.com/-EucKC7DmcQI/AAAAAAAAAAI/AAAAAAAAAGA/gQU1NPEmNFA/s50-c-k-no/photo.jpg",
      "userId": "105654037995838004821"
     },
     "user_tz": -540
    },
    "id": "PnifSBJglzHh",
    "outputId": "94177345-918e-4a96-d9a8-d8aba0a4bc9a",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1 Episode in   200 steps, reward -200.00, reward_in 0.00\n",
      "  2 Episode in   400 steps, reward -200.00, reward_in 0.00\n",
      "  3 Episode in   600 steps, reward -200.00, reward_in 0.00\n",
      "  4 Episode in   800 steps, reward -200.00, reward_in 0.00\n",
      "  5 Episode in  1000 steps, reward -200.00, reward_in 0.00\n",
      "  6 Episode in  1200 steps, reward -200.00, reward_in 0.00\n",
      "  7 Episode in  1400 steps, reward -200.00, reward_in 0.00\n",
      "\n",
      "====================  Start Learning  ====================\n",
      "\n",
      "  8 Episode in  1600 steps, reward -200.00, reward_in 79.07\n",
      "  9 Episode in  1800 steps, reward -200.00, reward_in 31.09\n",
      " 10 Episode in  2000 steps, reward -200.00, reward_in 6.10\n",
      " 11 Episode in  2200 steps, reward -200.00, reward_in 4.44\n",
      " 12 Episode in  2400 steps, reward -200.00, reward_in 7.49\n",
      " 13 Episode in  2600 steps, reward -200.00, reward_in 5.20\n",
      " 14 Episode in  2800 steps, reward -200.00, reward_in 5.66\n",
      " 15 Episode in  3000 steps, reward -200.00, reward_in 13.48\n",
      " 16 Episode in  3200 steps, reward -200.00, reward_in 0.97\n",
      " 17 Episode in  3400 steps, reward -200.00, reward_in 29.77\n",
      " 18 Episode in  3600 steps, reward -200.00, reward_in 1.76\n",
      " 19 Episode in  3800 steps, reward -200.00, reward_in 1.12\n",
      " 20 Episode in  4000 steps, reward -200.00, reward_in 1.72\n",
      " 21 Episode in  4200 steps, reward -200.00, reward_in 10.07\n",
      " 22 Episode in  4400 steps, reward -200.00, reward_in 24.77\n",
      " 23 Episode in  4600 steps, reward -200.00, reward_in 2.05\n",
      " 24 Episode in  4800 steps, reward -200.00, reward_in 18.42\n",
      " 25 Episode in  5000 steps, reward -200.00, reward_in 2.22\n",
      " 26 Episode in  5200 steps, reward -200.00, reward_in 27.07\n",
      " 27 Episode in  5400 steps, reward -200.00, reward_in 0.70\n",
      " 28 Episode in  5600 steps, reward -200.00, reward_in 6.07\n",
      " 29 Episode in  5800 steps, reward -200.00, reward_in 5.44\n",
      " 30 Episode in  6000 steps, reward -200.00, reward_in 6.67\n",
      " 31 Episode in  6200 steps, reward -200.00, reward_in 21.01\n",
      " 32 Episode in  6400 steps, reward -200.00, reward_in 6.30\n",
      " 33 Episode in  6600 steps, reward -200.00, reward_in 2.85\n",
      " 34 Episode in  6800 steps, reward -200.00, reward_in 24.51\n",
      " 35 Episode in  7000 steps, reward -200.00, reward_in 23.75\n",
      " 36 Episode in  7200 steps, reward -200.00, reward_in 0.88\n",
      " 37 Episode in  7400 steps, reward -200.00, reward_in 1.51\n",
      " 38 Episode in  7600 steps, reward -200.00, reward_in 12.33\n",
      " 39 Episode in  7800 steps, reward -200.00, reward_in 10.46\n",
      " 40 Episode in  8000 steps, reward -200.00, reward_in 7.32\n",
      " 41 Episode in  8200 steps, reward -200.00, reward_in 66.24\n",
      " 42 Episode in  8400 steps, reward -200.00, reward_in 26.13\n",
      " 43 Episode in  8600 steps, reward -200.00, reward_in 30.95\n",
      " 44 Episode in  8800 steps, reward -200.00, reward_in 64.67\n",
      " 45 Episode in  9000 steps, reward -200.00, reward_in 48.65\n",
      " 46 Episode in  9192 steps, reward -192.00, reward_in 57.21\n",
      " 47 Episode in  9367 steps, reward -175.00, reward_in 63.13\n",
      " 48 Episode in  9559 steps, reward -192.00, reward_in 70.53\n",
      " 49 Episode in  9738 steps, reward -179.00, reward_in 53.12\n",
      " 50 Episode in  9852 steps, reward -114.00, reward_in 19.47\n",
      " 51 Episode in  9951 steps, reward -99.00, reward_in 38.87\n",
      " 52 Episode in 10128 steps, reward -177.00, reward_in 27.38\n",
      " 53 Episode in 10301 steps, reward -173.00, reward_in 63.72\n",
      " 54 Episode in 10465 steps, reward -164.00, reward_in 20.52\n",
      " 55 Episode in 10636 steps, reward -171.00, reward_in 40.02\n",
      " 56 Episode in 10729 steps, reward -93.00, reward_in 5.31\n",
      " 57 Episode in 10892 steps, reward -163.00, reward_in 16.94\n",
      " 58 Episode in 10986 steps, reward -94.00, reward_in 19.07\n",
      " 59 Episode in 11084 steps, reward -98.00, reward_in 13.84\n",
      " 60 Episode in 11241 steps, reward -157.00, reward_in 37.19\n",
      " 61 Episode in 11403 steps, reward -162.00, reward_in 37.30\n",
      " 62 Episode in 11566 steps, reward -163.00, reward_in 5.90\n",
      " 63 Episode in 11683 steps, reward -117.00, reward_in 24.42\n",
      " 64 Episode in 11776 steps, reward -93.00, reward_in 3.51\n",
      " 65 Episode in 11948 steps, reward -172.00, reward_in 43.74\n",
      " 66 Episode in 12103 steps, reward -155.00, reward_in 20.24\n",
      " 67 Episode in 12192 steps, reward -89.00, reward_in 15.90\n",
      " 68 Episode in 12342 steps, reward -150.00, reward_in 25.72\n",
      " 69 Episode in 12542 steps, reward -200.00, reward_in 38.68\n",
      " 70 Episode in 12713 steps, reward -171.00, reward_in 51.95\n",
      " 71 Episode in 12832 steps, reward -119.00, reward_in 5.11\n",
      " 72 Episode in 12924 steps, reward -92.00, reward_in 2.28\n",
      " 73 Episode in 13044 steps, reward -120.00, reward_in 15.57\n",
      " 74 Episode in 13143 steps, reward -99.00, reward_in 8.38\n",
      " 75 Episode in 13272 steps, reward -129.00, reward_in 16.38\n",
      " 76 Episode in 13370 steps, reward -98.00, reward_in 6.75\n",
      " 77 Episode in 13463 steps, reward -93.00, reward_in 1.90\n",
      " 78 Episode in 13627 steps, reward -164.00, reward_in 40.76\n",
      " 79 Episode in 13726 steps, reward -99.00, reward_in 2.63\n",
      " 80 Episode in 13844 steps, reward -118.00, reward_in 20.00\n",
      " 81 Episode in 13947 steps, reward -103.00, reward_in 22.62\n",
      " 82 Episode in 14075 steps, reward -128.00, reward_in 1.03\n",
      " 83 Episode in 14187 steps, reward -112.00, reward_in 2.16\n",
      " 84 Episode in 14352 steps, reward -165.00, reward_in 35.90\n",
      " 85 Episode in 14480 steps, reward -128.00, reward_in 2.44\n",
      " 86 Episode in 14598 steps, reward -118.00, reward_in 1.58\n",
      " 87 Episode in 14698 steps, reward -100.00, reward_in 3.46\n",
      " 88 Episode in 14823 steps, reward -125.00, reward_in 8.09\n",
      " 89 Episode in 14963 steps, reward -140.00, reward_in 11.32\n",
      " 90 Episode in 15099 steps, reward -136.00, reward_in 14.64\n",
      " 91 Episode in 15224 steps, reward -125.00, reward_in 3.19\n",
      " 92 Episode in 15351 steps, reward -127.00, reward_in 22.78\n",
      " 93 Episode in 15499 steps, reward -148.00, reward_in 1.66\n",
      " 94 Episode in 15689 steps, reward -190.00, reward_in 11.63\n",
      " 95 Episode in 15859 steps, reward -170.00, reward_in 50.78\n",
      " 96 Episode in 15985 steps, reward -126.00, reward_in 1.94\n",
      " 97 Episode in 16101 steps, reward -116.00, reward_in 0.93\n",
      " 98 Episode in 16295 steps, reward -194.00, reward_in 3.94\n",
      " 99 Episode in 16409 steps, reward -114.00, reward_in 24.59\n",
      "100 Episode in 16572 steps, reward -163.00, reward_in 4.44\n",
      "101 Episode in 16739 steps, reward -167.00, reward_in 4.53\n",
      "102 Episode in 16907 steps, reward -168.00, reward_in 13.69\n",
      "103 Episode in 17068 steps, reward -161.00, reward_in 21.12\n",
      "104 Episode in 17268 steps, reward -200.00, reward_in 11.48\n",
      "105 Episode in 17434 steps, reward -166.00, reward_in 37.47\n",
      "106 Episode in 17550 steps, reward -116.00, reward_in 0.64\n",
      "107 Episode in 17707 steps, reward -157.00, reward_in 2.88\n",
      "108 Episode in 17864 steps, reward -157.00, reward_in 7.43\n",
      "109 Episode in 17964 steps, reward -100.00, reward_in 7.20\n",
      "110 Episode in 18164 steps, reward -200.00, reward_in 7.45\n",
      "111 Episode in 18319 steps, reward -155.00, reward_in 35.57\n",
      "112 Episode in 18434 steps, reward -115.00, reward_in 1.78\n",
      "113 Episode in 18541 steps, reward -107.00, reward_in 1.10\n",
      "114 Episode in 18661 steps, reward -120.00, reward_in 3.62\n",
      "115 Episode in 18788 steps, reward -127.00, reward_in 27.80\n",
      "116 Episode in 18955 steps, reward -167.00, reward_in 4.05\n",
      "117 Episode in 19070 steps, reward -115.00, reward_in 18.34\n",
      "118 Episode in 19192 steps, reward -122.00, reward_in 7.40\n",
      "119 Episode in 19311 steps, reward -119.00, reward_in 2.35\n",
      "120 Episode in 19430 steps, reward -119.00, reward_in 38.04\n",
      "121 Episode in 19528 steps, reward -98.00, reward_in 1.57\n",
      "122 Episode in 19688 steps, reward -160.00, reward_in 2.01\n",
      "123 Episode in 19855 steps, reward -167.00, reward_in 2.89\n",
      "124 Episode in 19955 steps, reward -100.00, reward_in 10.45\n",
      "125 Episode in 20076 steps, reward -121.00, reward_in 1.59\n",
      "126 Episode in 20201 steps, reward -125.00, reward_in 8.25\n",
      "127 Episode in 20314 steps, reward -113.00, reward_in 4.16\n",
      "128 Episode in 20487 steps, reward -173.00, reward_in 1.95\n",
      "129 Episode in 20601 steps, reward -114.00, reward_in 7.65\n",
      "130 Episode in 20697 steps, reward -96.00, reward_in 2.83\n",
      "131 Episode in 20861 steps, reward -164.00, reward_in 18.60\n",
      "132 Episode in 21054 steps, reward -193.00, reward_in 5.42\n",
      "133 Episode in 21221 steps, reward -167.00, reward_in 15.05\n",
      "134 Episode in 21379 steps, reward -158.00, reward_in 5.53\n",
      "135 Episode in 21534 steps, reward -155.00, reward_in 4.91\n",
      "136 Episode in 21693 steps, reward -159.00, reward_in 10.52\n",
      "137 Episode in 21827 steps, reward -134.00, reward_in 7.73\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138 Episode in 21944 steps, reward -117.00, reward_in 8.76\n",
      "139 Episode in 22111 steps, reward -167.00, reward_in 2.98\n",
      "140 Episode in 22276 steps, reward -165.00, reward_in 9.67\n",
      "141 Episode in 22398 steps, reward -122.00, reward_in 1.54\n",
      "142 Episode in 22531 steps, reward -133.00, reward_in 34.59\n",
      "143 Episode in 22654 steps, reward -123.00, reward_in 3.44\n",
      "144 Episode in 22811 steps, reward -157.00, reward_in 2.88\n",
      "145 Episode in 22966 steps, reward -155.00, reward_in 4.54\n",
      "146 Episode in 23085 steps, reward -119.00, reward_in 12.82\n",
      "147 Episode in 23201 steps, reward -116.00, reward_in 4.64\n",
      "148 Episode in 23344 steps, reward -143.00, reward_in 3.82\n",
      "149 Episode in 23471 steps, reward -127.00, reward_in 6.70\n",
      "150 Episode in 23567 steps, reward -96.00, reward_in 2.38\n",
      "151 Episode in 23702 steps, reward -135.00, reward_in 6.80\n",
      "152 Episode in 23792 steps, reward -90.00, reward_in 2.70\n",
      "153 Episode in 23950 steps, reward -158.00, reward_in 12.81\n",
      "154 Episode in 24068 steps, reward -118.00, reward_in 4.93\n",
      "155 Episode in 24198 steps, reward -130.00, reward_in 9.01\n",
      "156 Episode in 24341 steps, reward -143.00, reward_in 11.88\n",
      "157 Episode in 24450 steps, reward -109.00, reward_in 4.15\n",
      "158 Episode in 24614 steps, reward -164.00, reward_in 9.63\n",
      "159 Episode in 24733 steps, reward -119.00, reward_in 2.53\n",
      "160 Episode in 24852 steps, reward -119.00, reward_in 19.16\n",
      "161 Episode in 24974 steps, reward -122.00, reward_in 3.85\n",
      "162 Episode in 25132 steps, reward -158.00, reward_in 3.11\n",
      "163 Episode in 25227 steps, reward -95.00, reward_in 22.20\n",
      "164 Episode in 25344 steps, reward -117.00, reward_in 1.81\n",
      "165 Episode in 25474 steps, reward -130.00, reward_in 1.39\n",
      "166 Episode in 25605 steps, reward -131.00, reward_in 1.40\n",
      "167 Episode in 25705 steps, reward -100.00, reward_in 6.01\n",
      "168 Episode in 25802 steps, reward -97.00, reward_in 4.56\n",
      "169 Episode in 25921 steps, reward -119.00, reward_in 1.79\n",
      "170 Episode in 26090 steps, reward -169.00, reward_in 28.25\n",
      "171 Episode in 26182 steps, reward -92.00, reward_in 1.57\n",
      "172 Episode in 26299 steps, reward -117.00, reward_in 1.32\n",
      "173 Episode in 26423 steps, reward -124.00, reward_in 2.87\n",
      "174 Episode in 26541 steps, reward -118.00, reward_in 4.07\n",
      "175 Episode in 26657 steps, reward -116.00, reward_in 6.86\n",
      "176 Episode in 26818 steps, reward -161.00, reward_in 16.16\n",
      "177 Episode in 26916 steps, reward -98.00, reward_in 2.65\n",
      "178 Episode in 27009 steps, reward -93.00, reward_in 2.22\n",
      "179 Episode in 27107 steps, reward -98.00, reward_in 2.39\n",
      "180 Episode in 27205 steps, reward -98.00, reward_in 7.64\n",
      "181 Episode in 27336 steps, reward -131.00, reward_in 5.76\n",
      "182 Episode in 27431 steps, reward -95.00, reward_in 3.05\n",
      "183 Episode in 27614 steps, reward -183.00, reward_in 11.44\n",
      "184 Episode in 27709 steps, reward -95.00, reward_in 1.83\n",
      "185 Episode in 27801 steps, reward -92.00, reward_in 2.04\n",
      "186 Episode in 27895 steps, reward -94.00, reward_in 2.14\n",
      "187 Episode in 28003 steps, reward -108.00, reward_in 7.47\n",
      "188 Episode in 28141 steps, reward -138.00, reward_in 18.71\n",
      "189 Episode in 28229 steps, reward -88.00, reward_in 2.54\n",
      "190 Episode in 28331 steps, reward -102.00, reward_in 3.24\n",
      "191 Episode in 28423 steps, reward -92.00, reward_in 4.71\n",
      "192 Episode in 28518 steps, reward -95.00, reward_in 1.95\n",
      "193 Episode in 28657 steps, reward -139.00, reward_in 3.99\n",
      "194 Episode in 28829 steps, reward -172.00, reward_in 10.80\n",
      "195 Episode in 28919 steps, reward -90.00, reward_in 10.62\n",
      "196 Episode in 29009 steps, reward -90.00, reward_in 1.99\n",
      "197 Episode in 29167 steps, reward -158.00, reward_in 3.63\n",
      "198 Episode in 29294 steps, reward -127.00, reward_in 6.43\n",
      "199 Episode in 29397 steps, reward -103.00, reward_in 7.35\n",
      "200 Episode in 29514 steps, reward -117.00, reward_in 1.92\n",
      "201 Episode in 29607 steps, reward -93.00, reward_in 5.53\n",
      "202 Episode in 29740 steps, reward -133.00, reward_in 5.55\n",
      "203 Episode in 29915 steps, reward -175.00, reward_in 7.03\n",
      "204 Episode in 30049 steps, reward -134.00, reward_in 3.84\n",
      "205 Episode in 30180 steps, reward -131.00, reward_in 6.06\n",
      "206 Episode in 30287 steps, reward -107.00, reward_in 5.69\n",
      "207 Episode in 30442 steps, reward -155.00, reward_in 2.80\n",
      "208 Episode in 30534 steps, reward -92.00, reward_in 2.12\n",
      "209 Episode in 30699 steps, reward -165.00, reward_in 21.18\n",
      "210 Episode in 30877 steps, reward -178.00, reward_in 2.49\n",
      "211 Episode in 30978 steps, reward -101.00, reward_in 28.82\n",
      "212 Episode in 31130 steps, reward -152.00, reward_in 1.25\n",
      "213 Episode in 31227 steps, reward -97.00, reward_in 0.46\n",
      "214 Episode in 31384 steps, reward -157.00, reward_in 1.13\n",
      "215 Episode in 31473 steps, reward -89.00, reward_in 0.44\n",
      "216 Episode in 31589 steps, reward -116.00, reward_in 1.92\n",
      "217 Episode in 31705 steps, reward -116.00, reward_in 0.85\n",
      "218 Episode in 31905 steps, reward -200.00, reward_in 5.27\n",
      "219 Episode in 32016 steps, reward -111.00, reward_in 3.07\n",
      "220 Episode in 32113 steps, reward -97.00, reward_in 1.53\n",
      "221 Episode in 32248 steps, reward -135.00, reward_in 6.18\n",
      "222 Episode in 32445 steps, reward -197.00, reward_in 14.69\n",
      "223 Episode in 32604 steps, reward -159.00, reward_in 6.64\n",
      "224 Episode in 32777 steps, reward -173.00, reward_in 1.60\n",
      "225 Episode in 32868 steps, reward -91.00, reward_in 5.08\n",
      "226 Episode in 32965 steps, reward -97.00, reward_in 3.38\n",
      "227 Episode in 33124 steps, reward -159.00, reward_in 4.87\n",
      "228 Episode in 33247 steps, reward -123.00, reward_in 7.97\n",
      "229 Episode in 33401 steps, reward -154.00, reward_in 6.60\n",
      "230 Episode in 33518 steps, reward -117.00, reward_in 3.54\n",
      "231 Episode in 33620 steps, reward -102.00, reward_in 3.38\n",
      "232 Episode in 33720 steps, reward -100.00, reward_in 1.27\n",
      "233 Episode in 33810 steps, reward -90.00, reward_in 1.74\n",
      "234 Episode in 33944 steps, reward -134.00, reward_in 8.14\n",
      "235 Episode in 34105 steps, reward -161.00, reward_in 6.52\n",
      "236 Episode in 34238 steps, reward -133.00, reward_in 1.47\n",
      "237 Episode in 34359 steps, reward -121.00, reward_in 3.98\n",
      "238 Episode in 34472 steps, reward -113.00, reward_in 3.08\n",
      "239 Episode in 34578 steps, reward -106.00, reward_in 4.10\n",
      "240 Episode in 34669 steps, reward -91.00, reward_in 2.08\n",
      "241 Episode in 34795 steps, reward -126.00, reward_in 1.89\n",
      "242 Episode in 34889 steps, reward -94.00, reward_in 2.54\n",
      "243 Episode in 34982 steps, reward -93.00, reward_in 2.59\n",
      "244 Episode in 35090 steps, reward -108.00, reward_in 3.86\n",
      "245 Episode in 35204 steps, reward -114.00, reward_in 1.17\n",
      "246 Episode in 35298 steps, reward -94.00, reward_in 1.03\n",
      "247 Episode in 35386 steps, reward -88.00, reward_in 35.52\n",
      "248 Episode in 35502 steps, reward -116.00, reward_in 1.23\n",
      "249 Episode in 35596 steps, reward -94.00, reward_in 0.48\n",
      "250 Episode in 35684 steps, reward -88.00, reward_in 0.49\n",
      "251 Episode in 35837 steps, reward -153.00, reward_in 1.41\n",
      "252 Episode in 35943 steps, reward -106.00, reward_in 0.63\n",
      "253 Episode in 36033 steps, reward -90.00, reward_in 0.49\n",
      "254 Episode in 36163 steps, reward -130.00, reward_in 1.68\n",
      "255 Episode in 36276 steps, reward -113.00, reward_in 1.23\n",
      "256 Episode in 36365 steps, reward -89.00, reward_in 0.31\n",
      "257 Episode in 36522 steps, reward -157.00, reward_in 1.85\n",
      "258 Episode in 36648 steps, reward -126.00, reward_in 2.13\n",
      "259 Episode in 36795 steps, reward -147.00, reward_in 2.23\n",
      "260 Episode in 36919 steps, reward -124.00, reward_in 1.02\n",
      "261 Episode in 37086 steps, reward -167.00, reward_in 7.90\n",
      "262 Episode in 37194 steps, reward -108.00, reward_in 0.79\n",
      "263 Episode in 37290 steps, reward -96.00, reward_in 0.89\n",
      "264 Episode in 37382 steps, reward -92.00, reward_in 1.96\n",
      "265 Episode in 37540 steps, reward -158.00, reward_in 1.39\n",
      "266 Episode in 37713 steps, reward -173.00, reward_in 11.21\n",
      "267 Episode in 37805 steps, reward -92.00, reward_in 2.69\n",
      "268 Episode in 37917 steps, reward -112.00, reward_in 0.75\n",
      "269 Episode in 38022 steps, reward -105.00, reward_in 1.30\n",
      "270 Episode in 38136 steps, reward -114.00, reward_in 1.66\n",
      "271 Episode in 38287 steps, reward -151.00, reward_in 5.52\n",
      "272 Episode in 38420 steps, reward -133.00, reward_in 3.31\n",
      "273 Episode in 38533 steps, reward -113.00, reward_in 6.89\n",
      "274 Episode in 38708 steps, reward -175.00, reward_in 2.68\n",
      "275 Episode in 38796 steps, reward -88.00, reward_in 0.64\n",
      "276 Episode in 38918 steps, reward -122.00, reward_in 0.94\n",
      "277 Episode in 39009 steps, reward -91.00, reward_in 2.68\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "278 Episode in 39120 steps, reward -111.00, reward_in 2.47\n",
      "279 Episode in 39233 steps, reward -113.00, reward_in 1.87\n",
      "280 Episode in 39328 steps, reward -95.00, reward_in 1.70\n",
      "281 Episode in 39437 steps, reward -109.00, reward_in 3.45\n",
      "282 Episode in 39550 steps, reward -113.00, reward_in 1.32\n",
      "283 Episode in 39656 steps, reward -106.00, reward_in 2.08\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-c1633f41ec53>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mep_reward_in\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/gym/core.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode, **kwargs)\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'human'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 275\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/gym/envs/classic_control/mountain_car.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcartrans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_rotation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreturn_rgb_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'rgb_array'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/gym/envs/classic_control/rendering.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, return_rgb_array)\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m             \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0monetime_geoms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0marr\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_rgb_array\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misopen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pyglet/window/xlib/__init__.py\u001b[0m in \u001b[0;36mflip\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    504\u001b[0m         \u001b[0;31m# TODO canvas.flip?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 506\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sync_resize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pyglet/gl/xlib.py\u001b[0m in \u001b[0;36mflip\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vsync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 358\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait_vsync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    359\u001b[0m         \u001b[0mglx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglXSwapBuffers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx_display\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglx_window\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pyglet/gl/xlib.py\u001b[0m in \u001b[0;36m_wait_vsync\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    238\u001b[0m             \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc_uint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m             \u001b[0mglxext_arb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglXGetVideoSyncSGI\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbyref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m             \u001b[0mglxext_arb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglXWaitVideoSyncSGI\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbyref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# play!\n",
    "for i in range(1, n_episodes + 1):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    ep_reward = 0\n",
    "    ep_reward_in = 0.\n",
    "    while not done:\n",
    "        env.render()\n",
    "        if np.random.rand() < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = select_action(obs, target_net)\n",
    "\n",
    "        _obs, reward, done, _ = env.step(action)\n",
    "        \n",
    "        if not is_learned:\n",
    "            reward_in = 0.\n",
    "        else:\n",
    "            reward_in = calculate_reward_in(pred_net, rand_net, _obs)\n",
    "\n",
    "        obs_memory.append(obs)\n",
    "        rep_memory.append((obs, action, reward, reward_in, _obs, done))\n",
    "\n",
    "        obs = _obs\n",
    "        total_steps += 1\n",
    "        ep_reward += reward\n",
    "        ep_reward_in += reward_in\n",
    "        \n",
    "        if use_eps_decay:\n",
    "            epsilon -= epsilon * decay_rate\n",
    "            epsilon = max(eps_min, epsilon)\n",
    "\n",
    "        if len(rep_memory) >= learn_start:\n",
    "            if len(rep_memory) == learn_start:\n",
    "                print('\\n====================  Start Learning  ====================\\n')\n",
    "                is_learned = True\n",
    "            mean, std = get_norm_params(obs_memory)    \n",
    "            learn(net, target_net, pred_net, rand_net,\n",
    "                  net_optim, pred_optim, rep_memory)\n",
    "            learn_steps += 1\n",
    "\n",
    "        if learn_steps == update_frq:\n",
    "            # target smoothing update\n",
    "            for t, n in zip(target_net.parameters(), net.parameters()):\n",
    "                t.data = UP_COEF * n.data + (1 - UP_COEF) * t.data\n",
    "            learn_steps = 0\n",
    "    if done:\n",
    "        rewards.append(ep_reward)\n",
    "        reward_eval.append(ep_reward)\n",
    "        print('{:3} Episode in {:5} steps, reward {:.2f}, reward_in {:.2f}'.format(\n",
    "            i, total_steps, ep_reward, ep_reward_in))\n",
    "\n",
    "        if len(reward_eval) >= n_eval:\n",
    "            if np.mean(reward_eval) >= env.spec.reward_threshold:\n",
    "                print('\\n{} is sloved! {:3} Episode in {:3} steps'.format(\n",
    "                    env.spec.id, i, total_steps))\n",
    "                torch.save(target_net.state_dict(),\n",
    "                           f'../test/saved_models/{env.spec.id}_ep{i}_clear_model_dddqn_r.pt')\n",
    "                break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "plt.title('Reward')\n",
    "plt.plot(rewards)\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.title('Loss')\n",
    "plt.plot(losses)\n",
    "plt.show()\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.title('f_Loss')\n",
    "plt.plot(f_losses[30:])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[\n",
    "    ('CartPole-v0', 355, 0.5),\n",
    "    ('CartPole-v1', 484, 0.025),\n",
    "    ('MountainCar-v0', 506, 0.1),\n",
    "    ('LunarLander-v2', 454, 0.5)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "C51_tensorflow.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
