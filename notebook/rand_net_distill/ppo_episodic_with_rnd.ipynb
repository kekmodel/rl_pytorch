{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IWnm3qot3o1W"
   },
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "from copy import deepcopy\n",
    "\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 708,
     "status": "ok",
     "timestamp": 1534482400648,
     "user": {
      "displayName": "윤승제",
      "photoUrl": "//lh5.googleusercontent.com/-EucKC7DmcQI/AAAAAAAAAAI/AAAAAAAAAGA/gQU1NPEmNFA/s50-c-k-no/photo.jpg",
      "userId": "105654037995838004821"
     },
     "user_tz": -540
    },
    "id": "maRVADiTlzHD",
    "outputId": "783b7610-95c2-4b54-b2ce-d8e853c484ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda: True\n"
     ]
    }
   ],
   "source": [
    "SEED = 5\n",
    "BATCH_SIZE = 4\n",
    "LR = 0.0001\n",
    "EPOCHS = 4\n",
    "CLIP = 0.1\n",
    "GAMMA_EX = 0.999\n",
    "GAMMA_IN = 0.99\n",
    "LAMBDA = 0.95\n",
    "ENT_COEF = 0.001\n",
    "EX_COEF = 2.0\n",
    "IN_COEF = 1.0\n",
    "EPS = 1.1920929e-07\n",
    "\n",
    "# set device\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print('cuda:', use_cuda)\n",
    "device = torch.device('cuda' if use_cuda else 'cpu')\n",
    "\n",
    "# random seed\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if use_cuda:\n",
    "    torch.cuda.manual_seed_all(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9Ffkl_5C4R81"
   },
   "outputs": [],
   "source": [
    "class ActorCriticNet(nn.Module):\n",
    "    def __init__(self, obs_space, action_space):\n",
    "        super().__init__()\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(obs_space, 256),\n",
    "            nn.SELU()\n",
    "        )\n",
    "\n",
    "        self.pol = nn.Sequential(\n",
    "            nn.Linear(256, 256),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(256, action_space)\n",
    "        )\n",
    "\n",
    "        self.val_ex = nn.Sequential(\n",
    "            nn.Linear(256, 256),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "\n",
    "        self.val_in = nn.Sequential(\n",
    "            nn.Linear(256, 256),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "\n",
    "        self.log_softmax = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.head(x)\n",
    "        logit = self.pol(out).reshape(out.shape[0], -1)\n",
    "        value_ex = self.val_ex(out).reshape(out.shape[0], 1)\n",
    "        value_in = self.val_in(out).reshape(out.shape[0], 1)\n",
    "        log_probs = self.log_softmax(logit)\n",
    "\n",
    "        return log_probs, value_ex, value_in\n",
    "\n",
    "\n",
    "class RandomNet(nn.Module):\n",
    "    def __init__(self, obs_space):\n",
    "        super().__init__()\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(obs_space, 256),\n",
    "            nn.SELU()\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(256, 512)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.head(x)\n",
    "        obs_feature = self.fc(out).reshape(out.shape[0], -1)\n",
    "\n",
    "        return obs_feature\n",
    "\n",
    "\n",
    "class PredictNet(nn.Module):\n",
    "    def __init__(self, obs_space):\n",
    "        super().__init__()\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(obs_space, 256),\n",
    "            nn.SELU()\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(256, 256),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(256, 512)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.head(x)\n",
    "        obs_feature = self.fc(out).reshape(out.shape[0], -1)\n",
    "\n",
    "        return obs_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "m_losses = []\n",
    "f_losses = []\n",
    "\n",
    "\n",
    "def learn(net, old_net, pred_net, rand_net, net_optim, pred_optim, train_memory):\n",
    "    net.train()\n",
    "    old_net.train()\n",
    "    \n",
    "    dataloader = DataLoader(\n",
    "        train_memory,\n",
    "        shuffle=True,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        pin_memory=use_cuda\n",
    "    )\n",
    "\n",
    "    for i in range(EPOCHS):\n",
    "        for (s, a, _s, ret_ex, ret_in, adv_ex, adv_in) in dataloader:\n",
    "            s = s.to(device).float()\n",
    "            a  = a.to(device).long()\n",
    "            _s = _s.to(device).float()\n",
    "            _s_norm_np = normalize_obs(_s.detach().cpu().numpy())\n",
    "            _s_norm = torch.tensor(_s_norm_np).to(device).float()\n",
    "            ret_ex = ret_ex.to(device).float()\n",
    "            ret_in = ret_in.to(device).float()\n",
    "            adv = (adv_ex + adv_in).to(device).float()\n",
    "            batch_size = s.shape[0]\n",
    "\n",
    "            with torch.no_grad():\n",
    "                rand_f = rand_net(_s_norm)\n",
    "                log_p_old, v_ex_old, v_in_old = old_net(s)\n",
    "                log_p_a_old = log_p_old[range(batch_size), a]\n",
    "\n",
    "            pred_f = pred_net(_s_norm)\n",
    "            log_p, v_ex, v_in = net(s)\n",
    "            log_p_a = log_p[range(batch_size), a]\n",
    "            p_ratio = (log_p_a - log_p_a_old).exp()\n",
    "            p_r_clip = torch.clamp(p_ratio, 1. - CLIP, 1. + CLIP)\n",
    "            p_loss = torch.min(p_ratio * adv, p_r_clip * adv).mean()\n",
    "            v_ex_loss = (ret_ex - v_ex).pow(2)\n",
    "            v_in_loss = (ret_in - v_in).pow(2)\n",
    "            v_loss = (v_ex_loss + v_in_loss).mean() \n",
    "            entropy = -(log_p.exp() * log_p).sum(dim=1).mean()\n",
    "\n",
    "            # loss\n",
    "            m_loss = -(p_loss - v_loss + ENT_COEF * entropy)\n",
    "            m_losses.append(m_loss)\n",
    "\n",
    "            f_loss = (pred_f - rand_f).pow(2).sum(dim=1).mean()\n",
    "            f_losses.append(f_loss)\n",
    "            \n",
    "            loss = m_loss + f_loss\n",
    "            losses.append(loss)\n",
    "            \n",
    "            net_optim.zero_grad()\n",
    "            pred_optim.zero_grad()\n",
    "            loss.backward()\n",
    "            net_optim.step()\n",
    "            pred_optim.step()\n",
    "\n",
    "\n",
    "def get_action_and_value(obs, net):\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        state = torch.tensor([obs]).to(device).float()\n",
    "        log_p, v_ex, v_in = net(state)\n",
    "        m = Categorical(log_p.exp())\n",
    "        action = m.sample()\n",
    "\n",
    "    return action.item(), v_ex.item(), v_in.item()\n",
    "\n",
    "\n",
    "def compute_adv_with_gae(rews_ex, rews_in, vals_ex, vals_in, roll_memory):\n",
    "    rew_ex = np.array(rews_ex, 'float')\n",
    "    rew_in = np.array(rews_in, 'float')\n",
    "    val_ex = np.array(vals_ex[:-1], 'float')\n",
    "    val_in = np.array(vals_in[:-1], 'float')\n",
    "    _val_ex = np.array(vals_ex[1:], 'float')\n",
    "    _val_in = np.array(vals_in[1:], 'float')\n",
    "    dt_ex = rew_ex + GAMMA_EX * _val_ex - val_ex\n",
    "    dt_in = rew_in + GAMMA_IN * _val_in - val_in\n",
    "    dis_r_ex = np.array([GAMMA_EX**(i) * r for i, r in enumerate(rews_ex)], 'float')\n",
    "    dis_r_in = np.array([GAMMA_IN**(i) * r for i, r in enumerate(rews_in)], 'float')\n",
    "    gae_ex = np.array([(GAMMA_EX * LAMBDA)**(i) * dt for i, dt in enumerate(dt_ex.tolist())], 'float')\n",
    "    gae_in = np.array([(GAMMA_IN * LAMBDA)**(i) * dt for i, dt in enumerate(dt_in.tolist())], 'float')\n",
    "    for i, data in enumerate(roll_memory):\n",
    "        data.append(sum(dis_r_ex[i:] / GAMMA_EX**(i)))\n",
    "        data.append(sum(dis_r_in[i:] / GAMMA_IN**(i)))\n",
    "        data.append(sum(gae_ex[i:] / (GAMMA_EX * LAMBDA)**(i)))\n",
    "        data.append(sum(gae_in[i:] / (GAMMA_IN * LAMBDA)**(i)))\n",
    "\n",
    "    return roll_memory\n",
    "\n",
    "\n",
    "def get_norm_params(obs_memory):\n",
    "    global obs_apace\n",
    "\n",
    "    obses = [[] for _ in range(obs_space)]\n",
    "    for obs in obs_memory:\n",
    "        for j in range(obs_space):\n",
    "            obses[j].append(obs[j])\n",
    "\n",
    "    mean = np.zeros(obs_space, 'float')\n",
    "    std = np.zeros(obs_space, 'float')\n",
    "    for i, obs_ in enumerate(obses):\n",
    "        mean[i] = np.mean(obs_)\n",
    "        std[i] = np.std(obs_)\n",
    "\n",
    "    return mean, np.clip(std, a_min=EPS, a_max=None)\n",
    "\n",
    "\n",
    "def normalize_obs(obs):\n",
    "    global mean, std\n",
    "    norm_obs = (obs - mean) / std\n",
    "#     return np.clip(norm_obs, -5, 5)\n",
    "    return norm_obs\n",
    "\n",
    "\n",
    "def calculate_reward_in(pred_net, rand_net, obs):\n",
    "    norm_obs = normalize_obs(obs)\n",
    "    state = torch.tensor([norm_obs]).to(device).float()\n",
    "    with torch.no_grad():\n",
    "        pred_obs = pred_net(state)\n",
    "        rand_obs = rand_net(state)\n",
    "        reward = (pred_obs - rand_obs).pow(2).sum()\n",
    "        clipped_reward = torch.clamp(reward, -1, 1)\n",
    "\n",
    "    return clipped_reward.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 3377
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 135196,
     "status": "ok",
     "timestamp": 1534482559393,
     "user": {
      "displayName": "윤승제",
      "photoUrl": "//lh5.googleusercontent.com/-EucKC7DmcQI/AAAAAAAAAAI/AAAAAAAAAGA/gQU1NPEmNFA/s50-c-k-no/photo.jpg",
      "userId": "105654037995838004821"
     },
     "user_tz": -540
    },
    "id": "PnifSBJglzHh",
    "outputId": "94177345-918e-4a96-d9a8-d8aba0a4bc9a",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# make an environment\n",
    "# env = gym.make('CartPole-v0')\n",
    "# env = gym.make('CartPole-v1')\n",
    "env = gym.make('MountainCar-v0')\n",
    "# env = gym.make('LunarLander-v2')\n",
    "\n",
    "env.seed(SEED)\n",
    "obs_space = env.observation_space.shape[0]\n",
    "action_space = env.action_space.n\n",
    "\n",
    "# hyperparameter\n",
    "n_episodes = 5000\n",
    "n_roll_ep = 1\n",
    "update_frq = 50\n",
    "n_eval = env.spec.trials\n",
    "\n",
    "# global values\n",
    "steps = 0\n",
    "learn_steps = 0\n",
    "mean = 0.\n",
    "std = 0.\n",
    "ep_rewards = []\n",
    "is_rollout = False\n",
    "is_solved = False\n",
    "\n",
    "# make memories\n",
    "net_memory = deque(maxlen=2)\n",
    "train_memory = []\n",
    "roll_memory = []\n",
    "obs_memory = []\n",
    "rews_ex = []\n",
    "rews_in = []\n",
    "vals_ex = []\n",
    "vals_in = []\n",
    "\n",
    "# make nerual networks\n",
    "net = ActorCriticNet(obs_space, action_space).to(device)\n",
    "old_net = deepcopy(net)\n",
    "net_memory.appendleft(net.state_dict())\n",
    "pred_net = PredictNet(obs_space).to(device)\n",
    "rand_net = RandomNet(obs_space).to(device)\n",
    "\n",
    "# make optimizers\n",
    "net_optim = torch.optim.Adam(net.parameters(), lr=LR, eps=1e-5)\n",
    "pred_optim = torch.optim.Adam(pred_net.parameters(), lr=LR, eps=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "use_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.spec.max_episode_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.spec.trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-110.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.spec.reward_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 3377
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 135196,
     "status": "ok",
     "timestamp": 1534482559393,
     "user": {
      "displayName": "윤승제",
      "photoUrl": "//lh5.googleusercontent.com/-EucKC7DmcQI/AAAAAAAAAAI/AAAAAAAAAGA/gQU1NPEmNFA/s50-c-k-no/photo.jpg",
      "userId": "105654037995838004821"
     },
     "user_tz": -540
    },
    "id": "PnifSBJglzHh",
    "outputId": "94177345-918e-4a96-d9a8-d8aba0a4bc9a"
   },
   "outputs": [],
   "source": [
    "# simulation\n",
    "for i in range(1, update_frq+1):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "#         env.render()\n",
    "        action = env.action_space.sample()\n",
    "        _obs, _, done, _ = env.step(action)\n",
    "        obs_memory.append(_obs)\n",
    "\n",
    "mean, std = get_norm_params(obs_memory)\n",
    "obs_memory.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 3377
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 135196,
     "status": "ok",
     "timestamp": 1534482559393,
     "user": {
      "displayName": "윤승제",
      "photoUrl": "//lh5.googleusercontent.com/-EucKC7DmcQI/AAAAAAAAAAI/AAAAAAAAAGA/gQU1NPEmNFA/s50-c-k-no/photo.jpg",
      "userId": "105654037995838004821"
     },
     "user_tz": -540
    },
    "id": "PnifSBJglzHh",
    "outputId": "94177345-918e-4a96-d9a8-d8aba0a4bc9a",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1 Episode in   200 steps, reward_ex -200.00, reward_in 200.00\n",
      "  2 Episode in   400 steps, reward_ex -200.00, reward_in 187.80\n",
      "  3 Episode in   600 steps, reward_ex -200.00, reward_in 200.00\n",
      "  4 Episode in   800 steps, reward_ex -200.00, reward_in 149.94\n",
      "  5 Episode in  1000 steps, reward_ex -200.00, reward_in 110.00\n",
      "  6 Episode in  1200 steps, reward_ex -200.00, reward_in 35.12\n",
      "  7 Episode in  1400 steps, reward_ex -200.00, reward_in 46.93\n",
      "  8 Episode in  1600 steps, reward_ex -200.00, reward_in 32.27\n",
      "  9 Episode in  1800 steps, reward_ex -200.00, reward_in 158.03\n",
      " 10 Episode in  2000 steps, reward_ex -200.00, reward_in 73.66\n",
      " 11 Episode in  2200 steps, reward_ex -200.00, reward_in 136.92\n",
      " 12 Episode in  2400 steps, reward_ex -200.00, reward_in 72.17\n",
      " 13 Episode in  2600 steps, reward_ex -200.00, reward_in 28.84\n",
      " 14 Episode in  2800 steps, reward_ex -200.00, reward_in 123.66\n",
      " 15 Episode in  3000 steps, reward_ex -200.00, reward_in 53.56\n",
      " 16 Episode in  3200 steps, reward_ex -200.00, reward_in 38.61\n",
      " 17 Episode in  3400 steps, reward_ex -200.00, reward_in 26.48\n",
      " 18 Episode in  3600 steps, reward_ex -200.00, reward_in 13.76\n",
      " 19 Episode in  3800 steps, reward_ex -200.00, reward_in 90.75\n",
      " 20 Episode in  4000 steps, reward_ex -200.00, reward_in 32.97\n",
      " 21 Episode in  4200 steps, reward_ex -200.00, reward_in 44.96\n",
      " 22 Episode in  4400 steps, reward_ex -200.00, reward_in 21.11\n",
      " 23 Episode in  4600 steps, reward_ex -200.00, reward_in 33.75\n",
      " 24 Episode in  4800 steps, reward_ex -200.00, reward_in 16.79\n",
      " 25 Episode in  5000 steps, reward_ex -200.00, reward_in 37.34\n",
      " 26 Episode in  5200 steps, reward_ex -200.00, reward_in 23.83\n",
      " 27 Episode in  5400 steps, reward_ex -200.00, reward_in 8.37\n",
      " 28 Episode in  5600 steps, reward_ex -200.00, reward_in 22.36\n",
      " 29 Episode in  5800 steps, reward_ex -200.00, reward_in 12.34\n",
      " 30 Episode in  6000 steps, reward_ex -200.00, reward_in 97.86\n",
      " 31 Episode in  6200 steps, reward_ex -200.00, reward_in 43.28\n",
      " 32 Episode in  6400 steps, reward_ex -200.00, reward_in 17.62\n",
      " 33 Episode in  6600 steps, reward_ex -200.00, reward_in 26.49\n",
      " 34 Episode in  6800 steps, reward_ex -200.00, reward_in 95.05\n",
      " 35 Episode in  7000 steps, reward_ex -200.00, reward_in 40.22\n",
      " 36 Episode in  7200 steps, reward_ex -200.00, reward_in 11.10\n",
      " 37 Episode in  7400 steps, reward_ex -200.00, reward_in 8.53\n",
      " 38 Episode in  7600 steps, reward_ex -200.00, reward_in 29.17\n",
      " 39 Episode in  7800 steps, reward_ex -200.00, reward_in 12.37\n",
      " 40 Episode in  8000 steps, reward_ex -200.00, reward_in 13.82\n",
      " 41 Episode in  8200 steps, reward_ex -200.00, reward_in 17.52\n",
      " 42 Episode in  8400 steps, reward_ex -200.00, reward_in 11.45\n",
      " 43 Episode in  8600 steps, reward_ex -200.00, reward_in 16.43\n",
      " 44 Episode in  8800 steps, reward_ex -200.00, reward_in 11.89\n",
      " 45 Episode in  9000 steps, reward_ex -200.00, reward_in 29.11\n",
      " 46 Episode in  9200 steps, reward_ex -200.00, reward_in 39.24\n",
      " 47 Episode in  9400 steps, reward_ex -200.00, reward_in 18.38\n",
      " 48 Episode in  9600 steps, reward_ex -200.00, reward_in 32.12\n",
      " 49 Episode in  9800 steps, reward_ex -200.00, reward_in 27.97\n",
      " 50 Episode in 10000 steps, reward_ex -200.00, reward_in 26.81\n",
      " 51 Episode in 10200 steps, reward_ex -200.00, reward_in 13.99\n",
      " 52 Episode in 10400 steps, reward_ex -200.00, reward_in 9.25\n",
      " 53 Episode in 10600 steps, reward_ex -200.00, reward_in 70.48\n",
      " 54 Episode in 10800 steps, reward_ex -200.00, reward_in 28.12\n",
      " 55 Episode in 11000 steps, reward_ex -200.00, reward_in 8.91\n",
      " 56 Episode in 11200 steps, reward_ex -200.00, reward_in 7.47\n",
      " 57 Episode in 11400 steps, reward_ex -200.00, reward_in 11.13\n",
      " 58 Episode in 11600 steps, reward_ex -200.00, reward_in 8.65\n",
      " 59 Episode in 11800 steps, reward_ex -200.00, reward_in 8.25\n",
      " 60 Episode in 12000 steps, reward_ex -200.00, reward_in 6.06\n",
      " 61 Episode in 12200 steps, reward_ex -200.00, reward_in 9.52\n",
      " 62 Episode in 12400 steps, reward_ex -200.00, reward_in 6.06\n",
      " 63 Episode in 12600 steps, reward_ex -200.00, reward_in 5.57\n",
      " 64 Episode in 12800 steps, reward_ex -200.00, reward_in 10.43\n",
      " 65 Episode in 13000 steps, reward_ex -200.00, reward_in 8.10\n",
      " 66 Episode in 13200 steps, reward_ex -200.00, reward_in 6.18\n",
      " 67 Episode in 13400 steps, reward_ex -200.00, reward_in 8.55\n",
      " 68 Episode in 13600 steps, reward_ex -200.00, reward_in 21.83\n",
      " 69 Episode in 13800 steps, reward_ex -200.00, reward_in 19.59\n",
      " 70 Episode in 14000 steps, reward_ex -200.00, reward_in 15.75\n",
      " 71 Episode in 14200 steps, reward_ex -200.00, reward_in 23.51\n",
      " 72 Episode in 14400 steps, reward_ex -200.00, reward_in 18.58\n",
      " 73 Episode in 14600 steps, reward_ex -200.00, reward_in 8.04\n",
      " 74 Episode in 14800 steps, reward_ex -200.00, reward_in 4.35\n",
      " 75 Episode in 15000 steps, reward_ex -200.00, reward_in 4.41\n",
      " 76 Episode in 15200 steps, reward_ex -200.00, reward_in 4.08\n",
      " 77 Episode in 15400 steps, reward_ex -200.00, reward_in 9.32\n",
      " 78 Episode in 15600 steps, reward_ex -200.00, reward_in 43.84\n",
      " 79 Episode in 15800 steps, reward_ex -200.00, reward_in 18.22\n",
      " 80 Episode in 16000 steps, reward_ex -200.00, reward_in 12.45\n",
      " 81 Episode in 16200 steps, reward_ex -200.00, reward_in 8.10\n",
      " 82 Episode in 16400 steps, reward_ex -200.00, reward_in 11.17\n",
      " 83 Episode in 16600 steps, reward_ex -200.00, reward_in 7.79\n",
      " 84 Episode in 16800 steps, reward_ex -200.00, reward_in 5.00\n",
      " 85 Episode in 17000 steps, reward_ex -200.00, reward_in 14.94\n",
      " 86 Episode in 17200 steps, reward_ex -200.00, reward_in 6.13\n",
      " 87 Episode in 17400 steps, reward_ex -200.00, reward_in 4.54\n",
      " 88 Episode in 17600 steps, reward_ex -200.00, reward_in 5.77\n",
      " 89 Episode in 17800 steps, reward_ex -200.00, reward_in 6.59\n",
      " 90 Episode in 18000 steps, reward_ex -200.00, reward_in 27.38\n",
      " 91 Episode in 18200 steps, reward_ex -200.00, reward_in 6.37\n",
      " 92 Episode in 18400 steps, reward_ex -200.00, reward_in 5.05\n",
      " 93 Episode in 18600 steps, reward_ex -200.00, reward_in 5.13\n",
      " 94 Episode in 18800 steps, reward_ex -200.00, reward_in 8.64\n",
      " 95 Episode in 19000 steps, reward_ex -200.00, reward_in 10.15\n",
      " 96 Episode in 19200 steps, reward_ex -200.00, reward_in 10.25\n",
      " 97 Episode in 19400 steps, reward_ex -200.00, reward_in 6.34\n",
      " 98 Episode in 19600 steps, reward_ex -200.00, reward_in 4.56\n",
      " 99 Episode in 19800 steps, reward_ex -200.00, reward_in 6.36\n",
      "100 Episode in 20000 steps, reward_ex -200.00, reward_in 19.53\n",
      "101 Episode in 20200 steps, reward_ex -200.00, reward_in 25.54\n",
      "102 Episode in 20400 steps, reward_ex -200.00, reward_in 4.70\n",
      "103 Episode in 20600 steps, reward_ex -200.00, reward_in 3.87\n",
      "104 Episode in 20800 steps, reward_ex -200.00, reward_in 3.73\n",
      "105 Episode in 21000 steps, reward_ex -200.00, reward_in 33.60\n",
      "106 Episode in 21200 steps, reward_ex -200.00, reward_in 28.18\n",
      "107 Episode in 21400 steps, reward_ex -200.00, reward_in 6.66\n",
      "108 Episode in 21600 steps, reward_ex -200.00, reward_in 3.77\n",
      "109 Episode in 21800 steps, reward_ex -200.00, reward_in 12.95\n",
      "110 Episode in 22000 steps, reward_ex -200.00, reward_in 6.91\n",
      "111 Episode in 22200 steps, reward_ex -200.00, reward_in 3.50\n",
      "112 Episode in 22400 steps, reward_ex -200.00, reward_in 15.91\n",
      "113 Episode in 22600 steps, reward_ex -200.00, reward_in 14.75\n",
      "114 Episode in 22800 steps, reward_ex -200.00, reward_in 6.42\n",
      "115 Episode in 23000 steps, reward_ex -200.00, reward_in 3.48\n",
      "116 Episode in 23200 steps, reward_ex -200.00, reward_in 8.33\n",
      "117 Episode in 23400 steps, reward_ex -200.00, reward_in 4.83\n",
      "118 Episode in 23600 steps, reward_ex -200.00, reward_in 3.81\n",
      "119 Episode in 23800 steps, reward_ex -200.00, reward_in 8.69\n",
      "120 Episode in 24000 steps, reward_ex -200.00, reward_in 20.24\n",
      "121 Episode in 24200 steps, reward_ex -200.00, reward_in 5.99\n",
      "122 Episode in 24400 steps, reward_ex -200.00, reward_in 4.73\n",
      "123 Episode in 24600 steps, reward_ex -200.00, reward_in 3.36\n",
      "124 Episode in 24800 steps, reward_ex -200.00, reward_in 3.28\n",
      "125 Episode in 25000 steps, reward_ex -200.00, reward_in 4.06\n",
      "126 Episode in 25200 steps, reward_ex -200.00, reward_in 7.54\n",
      "127 Episode in 25400 steps, reward_ex -200.00, reward_in 4.86\n",
      "128 Episode in 25600 steps, reward_ex -200.00, reward_in 5.95\n",
      "129 Episode in 25800 steps, reward_ex -200.00, reward_in 51.37\n",
      "130 Episode in 26000 steps, reward_ex -200.00, reward_in 13.18\n",
      "131 Episode in 26200 steps, reward_ex -200.00, reward_in 3.80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132 Episode in 26400 steps, reward_ex -200.00, reward_in 2.77\n",
      "133 Episode in 26600 steps, reward_ex -200.00, reward_in 4.30\n",
      "134 Episode in 26800 steps, reward_ex -200.00, reward_in 15.19\n",
      "135 Episode in 27000 steps, reward_ex -200.00, reward_in 35.45\n",
      "136 Episode in 27200 steps, reward_ex -200.00, reward_in 3.54\n",
      "137 Episode in 27400 steps, reward_ex -200.00, reward_in 10.23\n",
      "138 Episode in 27600 steps, reward_ex -200.00, reward_in 4.73\n",
      "139 Episode in 27800 steps, reward_ex -200.00, reward_in 3.46\n",
      "140 Episode in 28000 steps, reward_ex -200.00, reward_in 3.45\n",
      "141 Episode in 28200 steps, reward_ex -200.00, reward_in 2.87\n",
      "142 Episode in 28400 steps, reward_ex -200.00, reward_in 5.84\n",
      "143 Episode in 28600 steps, reward_ex -200.00, reward_in 4.22\n",
      "144 Episode in 28800 steps, reward_ex -200.00, reward_in 3.58\n",
      "145 Episode in 29000 steps, reward_ex -200.00, reward_in 3.66\n",
      "146 Episode in 29200 steps, reward_ex -200.00, reward_in 3.24\n",
      "147 Episode in 29400 steps, reward_ex -200.00, reward_in 2.93\n",
      "148 Episode in 29600 steps, reward_ex -200.00, reward_in 5.77\n",
      "149 Episode in 29800 steps, reward_ex -200.00, reward_in 8.67\n",
      "150 Episode in 30000 steps, reward_ex -200.00, reward_in 2.88\n",
      "151 Episode in 30200 steps, reward_ex -200.00, reward_in 60.91\n",
      "152 Episode in 30400 steps, reward_ex -200.00, reward_in 21.15\n",
      "153 Episode in 30600 steps, reward_ex -200.00, reward_in 12.43\n",
      "154 Episode in 30800 steps, reward_ex -200.00, reward_in 6.21\n",
      "155 Episode in 31000 steps, reward_ex -200.00, reward_in 3.35\n",
      "156 Episode in 31200 steps, reward_ex -200.00, reward_in 5.86\n",
      "157 Episode in 31400 steps, reward_ex -200.00, reward_in 2.85\n",
      "158 Episode in 31600 steps, reward_ex -200.00, reward_in 4.18\n",
      "159 Episode in 31800 steps, reward_ex -200.00, reward_in 3.21\n",
      "160 Episode in 32000 steps, reward_ex -200.00, reward_in 2.66\n",
      "161 Episode in 32200 steps, reward_ex -200.00, reward_in 50.91\n",
      "162 Episode in 32400 steps, reward_ex -200.00, reward_in 14.52\n",
      "163 Episode in 32600 steps, reward_ex -200.00, reward_in 10.43\n",
      "164 Episode in 32800 steps, reward_ex -200.00, reward_in 6.73\n",
      "165 Episode in 33000 steps, reward_ex -200.00, reward_in 4.99\n",
      "166 Episode in 33200 steps, reward_ex -200.00, reward_in 3.11\n",
      "167 Episode in 33400 steps, reward_ex -200.00, reward_in 11.41\n",
      "168 Episode in 33600 steps, reward_ex -200.00, reward_in 5.17\n",
      "169 Episode in 33800 steps, reward_ex -200.00, reward_in 3.27\n",
      "170 Episode in 34000 steps, reward_ex -200.00, reward_in 3.88\n",
      "171 Episode in 34200 steps, reward_ex -200.00, reward_in 3.41\n",
      "172 Episode in 34400 steps, reward_ex -200.00, reward_in 2.64\n",
      "173 Episode in 34600 steps, reward_ex -200.00, reward_in 2.63\n",
      "174 Episode in 34800 steps, reward_ex -200.00, reward_in 8.14\n",
      "175 Episode in 35000 steps, reward_ex -200.00, reward_in 23.72\n",
      "176 Episode in 35200 steps, reward_ex -200.00, reward_in 2.85\n",
      "177 Episode in 35400 steps, reward_ex -200.00, reward_in 2.47\n",
      "178 Episode in 35600 steps, reward_ex -200.00, reward_in 5.68\n",
      "179 Episode in 35800 steps, reward_ex -200.00, reward_in 9.56\n",
      "180 Episode in 36000 steps, reward_ex -200.00, reward_in 4.01\n",
      "181 Episode in 36200 steps, reward_ex -200.00, reward_in 14.86\n",
      "182 Episode in 36400 steps, reward_ex -200.00, reward_in 9.04\n",
      "183 Episode in 36600 steps, reward_ex -200.00, reward_in 51.67\n",
      "184 Episode in 36800 steps, reward_ex -200.00, reward_in 47.52\n",
      "185 Episode in 37000 steps, reward_ex -200.00, reward_in 5.77\n",
      "186 Episode in 37200 steps, reward_ex -200.00, reward_in 3.21\n",
      "187 Episode in 37400 steps, reward_ex -200.00, reward_in 17.75\n",
      "188 Episode in 37600 steps, reward_ex -200.00, reward_in 12.04\n",
      "189 Episode in 37800 steps, reward_ex -200.00, reward_in 6.91\n",
      "190 Episode in 38000 steps, reward_ex -200.00, reward_in 3.42\n",
      "191 Episode in 38200 steps, reward_ex -200.00, reward_in 2.91\n",
      "192 Episode in 38400 steps, reward_ex -200.00, reward_in 37.59\n",
      "193 Episode in 38600 steps, reward_ex -200.00, reward_in 14.08\n",
      "194 Episode in 38800 steps, reward_ex -200.00, reward_in 2.75\n",
      "195 Episode in 39000 steps, reward_ex -200.00, reward_in 13.59\n",
      "196 Episode in 39200 steps, reward_ex -200.00, reward_in 5.86\n",
      "197 Episode in 39400 steps, reward_ex -200.00, reward_in 3.32\n",
      "198 Episode in 39600 steps, reward_ex -200.00, reward_in 3.88\n",
      "199 Episode in 39800 steps, reward_ex -200.00, reward_in 14.36\n",
      "200 Episode in 40000 steps, reward_ex -200.00, reward_in 11.05\n",
      "201 Episode in 40200 steps, reward_ex -200.00, reward_in 3.43\n",
      "202 Episode in 40400 steps, reward_ex -200.00, reward_in 4.49\n",
      "203 Episode in 40600 steps, reward_ex -200.00, reward_in 2.92\n",
      "204 Episode in 40800 steps, reward_ex -200.00, reward_in 3.71\n",
      "205 Episode in 41000 steps, reward_ex -200.00, reward_in 2.81\n",
      "206 Episode in 41200 steps, reward_ex -200.00, reward_in 2.97\n",
      "207 Episode in 41400 steps, reward_ex -200.00, reward_in 2.66\n",
      "208 Episode in 41600 steps, reward_ex -200.00, reward_in 2.62\n",
      "209 Episode in 41800 steps, reward_ex -200.00, reward_in 2.55\n",
      "210 Episode in 42000 steps, reward_ex -200.00, reward_in 2.43\n",
      "211 Episode in 42200 steps, reward_ex -200.00, reward_in 3.35\n",
      "212 Episode in 42400 steps, reward_ex -200.00, reward_in 13.71\n",
      "213 Episode in 42600 steps, reward_ex -200.00, reward_in 6.24\n",
      "214 Episode in 42800 steps, reward_ex -200.00, reward_in 4.74\n",
      "215 Episode in 43000 steps, reward_ex -200.00, reward_in 4.63\n",
      "216 Episode in 43200 steps, reward_ex -200.00, reward_in 2.61\n",
      "217 Episode in 43400 steps, reward_ex -200.00, reward_in 3.58\n",
      "218 Episode in 43600 steps, reward_ex -200.00, reward_in 2.24\n",
      "219 Episode in 43800 steps, reward_ex -200.00, reward_in 5.48\n",
      "220 Episode in 44000 steps, reward_ex -200.00, reward_in 7.78\n",
      "221 Episode in 44200 steps, reward_ex -200.00, reward_in 4.30\n",
      "222 Episode in 44400 steps, reward_ex -200.00, reward_in 5.78\n",
      "223 Episode in 44600 steps, reward_ex -200.00, reward_in 2.09\n",
      "224 Episode in 44800 steps, reward_ex -200.00, reward_in 2.84\n",
      "225 Episode in 45000 steps, reward_ex -200.00, reward_in 4.01\n",
      "226 Episode in 45200 steps, reward_ex -200.00, reward_in 12.54\n",
      "227 Episode in 45400 steps, reward_ex -200.00, reward_in 3.79\n",
      "228 Episode in 45600 steps, reward_ex -200.00, reward_in 3.11\n",
      "229 Episode in 45800 steps, reward_ex -200.00, reward_in 1.80\n",
      "230 Episode in 46000 steps, reward_ex -200.00, reward_in 2.47\n",
      "231 Episode in 46200 steps, reward_ex -200.00, reward_in 3.66\n"
     ]
    }
   ],
   "source": [
    "# play!\n",
    "for i in range(1, n_episodes + 1):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    ep_rew_ex = 0.\n",
    "    ep_rew_in = 0.\n",
    "    while not done:\n",
    "#         env.render()\n",
    "\n",
    "        action, val_ex, val_in = get_action_and_value(obs, net)\n",
    "        _obs, rew_ex, done, _ = env.step(action)\n",
    "        \n",
    "        rew_in = calculate_reward_in(pred_net, rand_net, _obs)\n",
    "        \n",
    "        # store\n",
    "        roll_memory.append([obs, action, _obs])\n",
    "        obs_memory.append(_obs)\n",
    "        rews_ex.append(EX_COEF * rew_ex)\n",
    "        rews_in.append(IN_COEF * rew_in)\n",
    "        vals_ex.append(val_ex)\n",
    "        vals_in.append(val_in)\n",
    "        \n",
    "        obs = _obs\n",
    "        steps += 1\n",
    "        ep_rew_ex += rew_ex\n",
    "        ep_rew_in += rew_in\n",
    "    \n",
    "    if done:\n",
    "        vals_ex.append(0.)\n",
    "        vals_in.append(0.)\n",
    "        train_memory.extend(\n",
    "            compute_adv_with_gae(rews_ex, rews_in, vals_ex, vals_in, roll_memory)\n",
    "        )\n",
    "        rews_ex.clear()\n",
    "        rews_in.clear()\n",
    "        vals_ex.clear()\n",
    "        vals_in.clear()\n",
    "        roll_memory.clear()\n",
    "        \n",
    "        if i % n_roll_ep == 0:\n",
    "            net_memory.appendleft(net.state_dict())\n",
    "            old_net.load_state_dict(net_memory.pop())\n",
    "            learn(net, old_net, pred_net, rand_net, net_optim, pred_optim, train_memory)\n",
    "            train_memory.clear()\n",
    "            learn_steps += 1\n",
    "            \n",
    "        if i % update_frq == 0:\n",
    "            mean, std = get_norm_params(obs_memory)\n",
    "            obs_memory.clear()\n",
    "\n",
    "        ep_rewards.append(ep_rew_ex)\n",
    "        print('{:3} Episode in {:5} steps, reward_ex {:.2f}, '\n",
    "              'reward_in {:.2f}'.format(i, steps, ep_rew_ex, ep_rew_in))\n",
    "        \n",
    "        if env.spec.id == 'MountainCar-v0' and ep_rew_ex > -200:\n",
    "            print('Wow!')\n",
    "\n",
    "        if len(ep_rewards) >= n_eval:\n",
    "            if np.mean(list(reversed(ep_rewards))[: n_eval]) >= env.spec.reward_threshold:\n",
    "                print('\\n{} is sloved! {:3} Episode in {:3} steps'.format(env.spec.id, i, steps))\n",
    "                torch.save(net.state_dict(),\n",
    "                           f'../test/saved_models/{env.spec.id}_ep{i}_clear_model_ppo_r.pt')\n",
    "                break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "plt.title('Reward')\n",
    "plt.plot(ep_rewards)\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.title('Loss')\n",
    "plt.plot(losses)\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.title('m_Loss')\n",
    "plt.plot(m_losses)\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.title('f_Loss')\n",
    "plt.plot(f_losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[\n",
    "    ('CartPole-v0', 161, 32, 256),\n",
    "    ('CartPole-v1', 162, 32, 256),\n",
    "    ('MountainCar-v0', 660),\n",
    "    ('LunarLander-v2', 260)\n",
    "]"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "C51_tensorflow.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
