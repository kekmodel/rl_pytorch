{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import deque\n",
    "from copy import deepcopy\n",
    "\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1\n",
    "BATCH_SIZE = 256\n",
    "LR = 0.00030\n",
    "UP_COEF = 0.005\n",
    "GAMMA = 0.99\n",
    "EPS = 1e-8\n",
    "GRAD_NORM = False\n",
    "\n",
    "# set device\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device('cuda' if use_cuda else 'cpu')\n",
    "\n",
    "# random seed\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if use_cuda:\n",
    "    torch.cuda.manual_seed_all(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DuelingDQN(nn.Module):\n",
    "    def __init__(self, obs_space, action_space):\n",
    "        super().__init__()\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(obs_space, obs_space*10),\n",
    "            nn.SELU()\n",
    "        )\n",
    "\n",
    "        self.val = nn.Sequential(\n",
    "            nn.Linear(obs_space*10, 512),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(512, 1)\n",
    "        )\n",
    "\n",
    "        self.adv = nn.Sequential(\n",
    "            nn.Linear(obs_space*10, 512),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(512, action_space)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.head(x)\n",
    "        val_out = self.val(out).reshape(out.shape[0], 1)\n",
    "        adv_out = self.adv(out).reshape(out.shape[0], -1)\n",
    "        adv_mean = adv_out.mean(dim=1, keepdim=True)\n",
    "        q = val_out + adv_out - adv_mean\n",
    "\n",
    "        return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "\n",
    "\n",
    "def learn(net, tgt_net, optimizer, rep_memory):\n",
    "    net.train()\n",
    "    tgt_net.train()\n",
    "\n",
    "    train_data = random.sample(rep_memory, BATCH_SIZE)\n",
    "    dataloader = DataLoader(\n",
    "        train_data, batch_size=BATCH_SIZE, pin_memory=use_cuda)\n",
    "    # double DQN\n",
    "    for i, (s, a, r, _s, d) in enumerate(dataloader):\n",
    "        s_batch = s.to(device).float()\n",
    "        a_batch = a.detach().to(device).long()\n",
    "        _s_batch = _s.to(device).float()\n",
    "        r_batch = r.detach().to(device).float()\n",
    "        done_mask = 1 - d.detach().to(device).float()\n",
    "        \n",
    "        _q_batch = net(_s_batch)\n",
    "        _a_batch = torch.argmax(_q_batch, dim=1)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            _q_batch_tgt = tgt_net(_s_batch)\n",
    "            _q_best_tgt = _q_batch_tgt[range(BATCH_SIZE), _a_batch]\n",
    "\n",
    "        q_batch = net(s_batch)\n",
    "        q_acting = q_batch[range(BATCH_SIZE), a_batch]\n",
    "\n",
    "        # loss\n",
    "        loss = ((r_batch + GAMMA * done_mask* _q_best_tgt) - q_acting).pow(2).mean()\n",
    "        losses.append(loss)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        if GRAD_NORM:\n",
    "            nn.utils.clip_grad_norm_(net.parameters(), max_norm=0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "def select_action(obs, tgt_net):\n",
    "    tgt_net.eval()\n",
    "    with torch.no_grad():\n",
    "        state = torch.tensor([obs]).to(device).float()\n",
    "        q = tgt_net(state)\n",
    "        action = torch.argmax(q, dim=1)\n",
    "\n",
    "    return action.item()\n",
    "\n",
    "\n",
    "def plot():\n",
    "    clear_output(True)\n",
    "    plt.figure(figsize=(16, 5))\n",
    "    plt.subplot(121)\n",
    "    plt.plot(rewards)\n",
    "    plt.title('Reward')\n",
    "    plt.subplot(122)\n",
    "    plt.plot(losses)\n",
    "    plt.title('Loss')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jay/anaconda3/lib/python3.7/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    }
   ],
   "source": [
    "# make an environment\n",
    "# env = gym.make('CartPole-v0')\n",
    "# env = gym.make('CartPole-v1')\n",
    "# env = gym.make('MountainCar-v0')\n",
    "# env = gym.make('LunarLander-v2')\n",
    "env = gym.make('Breakout-ram-v0')\n",
    "\n",
    "env.seed(SEED)\n",
    "obs_space = env.observation_space.shape[0]\n",
    "action_space = env.action_space.n\n",
    "\n",
    "# hyperparameter\n",
    "n_episodes = 100000\n",
    "learn_start = 3000\n",
    "memory_size = 100000\n",
    "update_frq = 1\n",
    "use_eps_decay = False\n",
    "epsilon = 1.0\n",
    "eps_min = 0.02\n",
    "decay_rate = 0.0001\n",
    "n_eval = env.spec.trials\n",
    "\n",
    "# global values\n",
    "total_steps = 0\n",
    "learn_steps = 0\n",
    "rewards = []\n",
    "reward_eval = deque(maxlen=n_eval)\n",
    "is_learned = False\n",
    "is_solved = False\n",
    "\n",
    "# make two nerual networks\n",
    "net = DuelingDQN(obs_space, action_space).to(device)\n",
    "target_net = deepcopy(net)\n",
    "\n",
    "# make optimizer\n",
    "optimizer = optim.Adam(net.parameters(), lr=LR, eps=EPS)\n",
    "\n",
    "# make memory\n",
    "rep_memory = deque(maxlen=memory_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# play\n",
    "for i in range(1, n_episodes + 1):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    ep_reward = 0\n",
    "    while not done:\n",
    "#         env.render()\n",
    "        if np.random.rand() < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = select_action(obs, target_net)\n",
    "\n",
    "        _obs, reward, done, _ = env.step(action)\n",
    "\n",
    "        rep_memory.append((obs, action, reward, _obs, done))\n",
    "\n",
    "        obs = _obs\n",
    "        total_steps += 1\n",
    "        ep_reward += reward\n",
    "\n",
    "        if use_eps_decay:\n",
    "            epsilon -= epsilon * decay_rate\n",
    "            epsilon = max(eps_min, epsilon)\n",
    "\n",
    "        if len(rep_memory) >= learn_start:\n",
    "            if len(rep_memory) == learn_start:\n",
    "                epsilon = eps_min\n",
    "                print('\\n============  Start Learning  ============\\n')\n",
    "            learn(net, target_net, optimizer, rep_memory)\n",
    "            learn_steps += 1\n",
    "\n",
    "        if learn_steps == update_frq:\n",
    "            # target smoothing update\n",
    "            for t, n in zip(target_net.parameters(), net.parameters()):\n",
    "                t.data = UP_COEF * n.data + (1 - UP_COEF) * t.data\n",
    "            learn_steps = 0\n",
    "    if done:\n",
    "        rewards.append(ep_reward)\n",
    "        reward_eval.append(ep_reward)\n",
    "        plot()\n",
    "#         print('{:3} Episode in {:5} steps, reward {:.2f}'.format(\n",
    "#             i, total_steps, ep_reward))\n",
    "\n",
    "        if len(reward_eval) >= n_eval:\n",
    "#             if np.mean(reward_eval) >= env.spec.reward_threshold:\n",
    "            if np.mean(reward_eval) >= 400:\n",
    "                print('\\n{} is sloved! {:3} Episode in {:3} steps'.format(\n",
    "                    env.spec.id, i, total_steps))\n",
    "                torch.save(target_net.state_dict(),\n",
    "                           f'./test/saved_models/{env.spec.id}_ep{i}_clear_model_dddqn.pt')\n",
    "                break\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
