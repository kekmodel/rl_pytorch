{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IWnm3qot3o1W"
   },
   "outputs": [],
   "source": [
    "# built-in\n",
    "import random\n",
    "from collections import deque\n",
    "from copy import deepcopy\n",
    "\n",
    "# tihrd party\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 708,
     "status": "ok",
     "timestamp": 1534482400648,
     "user": {
      "displayName": "윤승제",
      "photoUrl": "//lh5.googleusercontent.com/-EucKC7DmcQI/AAAAAAAAAAI/AAAAAAAAAGA/gQU1NPEmNFA/s50-c-k-no/photo.jpg",
      "userId": "105654037995838004821"
     },
     "user_tz": -540
    },
    "id": "maRVADiTlzHD",
    "outputId": "783b7610-95c2-4b54-b2ce-d8e853c484ba"
   },
   "outputs": [],
   "source": [
    "# for C51\n",
    "V_MAX = 10\n",
    "V_MIN = -10\n",
    "N_ATOMS = 51\n",
    "DELTA_Z = (V_MAX - V_MIN) / (N_ATOMS - 1)\n",
    "# for learning\n",
    "SEED = 1\n",
    "BATCH_SIZE = 256\n",
    "GAMMA = 0.99\n",
    "LR = 0.03\n",
    "EPS = 1e-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9Ffkl_5C4R81"
   },
   "outputs": [],
   "source": [
    "class CategoricalDuelingDQNN(nn.Module):\n",
    "    def __init__(self, obs_space, action_space, n_atoms):\n",
    "        super().__init__()\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(obs_space, obs_space*10),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(obs_space*10, (obs_space+action_space)*5),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.val = nn.Sequential(\n",
    "            nn.Linear((obs_space+action_space)*5, action_space*10),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(action_space*10, n_atoms)\n",
    "        )\n",
    "\n",
    "        self.adv = nn.Sequential(\n",
    "            nn.Linear((obs_space+action_space)*5, action_space*10),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(action_space*10, action_space * n_atoms)\n",
    "        )\n",
    "\n",
    "        self.log_softmax = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "        self.register_buffer(\n",
    "            'support', torch.arange(V_MIN, V_MAX + DELTA_Z, DELTA_Z))\n",
    "\n",
    "    def _make_layer(self, linear, n_hidden, n_layers):\n",
    "        layers = [linear(n_hidden) for _ in range(n_layers)]\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.head(x)\n",
    "        val_out = self.val(out).view(out.size(0), 1, N_ATOMS)\n",
    "        adv_out = self.adv(out).view(out.size(0), -1, N_ATOMS)\n",
    "        adv_mean = adv_out.mean(dim=1, keepdim=True)\n",
    "        out = val_out + adv_out - adv_mean\n",
    "        out = self.log_softmax(out)\n",
    "        probs = out.exp()\n",
    "\n",
    "        return probs\n",
    "\n",
    "\n",
    "class CategoricalDuelingDQN(nn.Module):\n",
    "    def __init__(self, obs_space, action_space, n_atoms):\n",
    "        super().__init__()\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(obs_space, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.val = nn.Sequential(\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, n_atoms)\n",
    "        )\n",
    "\n",
    "        self.adv = nn.Sequential(\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, action_space * n_atoms)\n",
    "        )\n",
    "\n",
    "        self.log_softmax = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "        self.register_buffer(\n",
    "            'support', torch.arange(V_MIN, V_MAX + DELTA_Z, DELTA_Z))\n",
    "\n",
    "    def _make_layer(self, linear, n_hidden, n_layers):\n",
    "        layers = [linear(n_hidden) for _ in range(n_layers)]\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.head(x)\n",
    "        val_out = self.val(out).view(out.size(0), 1, N_ATOMS)\n",
    "        adv_out = self.adv(out).view(out.size(0), -1, N_ATOMS)\n",
    "        adv_mean = adv_out.mean(dim=1, keepdim=True)\n",
    "        out = val_out + adv_out - adv_mean\n",
    "        out = self.log_softmax(out)\n",
    "        probs = out.exp()\n",
    "\n",
    "        return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "\n",
    "\n",
    "def learn(net, tgt_net, optimizer, rep_memory):\n",
    "    net.train()\n",
    "    tgt_net.train()\n",
    "\n",
    "    train_data = []\n",
    "    train_data.extend(random.sample(rep_memory, BATCH_SIZE))\n",
    "\n",
    "    dataloader = DataLoader(train_data,\n",
    "                            batch_size=BATCH_SIZE,\n",
    "                            pin_memory=use_cuda)\n",
    "    # like a double DQN\n",
    "    for i, (s, a, r, _s, d) in enumerate(dataloader):\n",
    "        s_batch = s.to(device).float()\n",
    "        a_batch = a.to(device).long()\n",
    "        _s_batch = _s.to(device).float()\n",
    "        rewards = r.detach().cpu().numpy().astype(np.longlong)\n",
    "        dones = d.detach().cpu().numpy().astype(np.bool)\n",
    "\n",
    "        _p_batch = net(_s_batch)\n",
    "        _weights = _p_batch * net.support\n",
    "        _q_batch = _weights.sum(dim=2)\n",
    "        _q_batch_np = _q_batch.detach().cpu().numpy()[0]\n",
    "        _action_batch_np = np.argmax(_q_batch_np)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            tgt_p_batch = tgt_net(_s_batch)\n",
    "            _p_best = tgt_p_batch[range(BATCH_SIZE), _action_batch_np]\n",
    "            _p_best_np = _p_best.cpu().numpy()\n",
    "\n",
    "        proj_p_np = projection(_p_best_np, rewards, dones)\n",
    "        proj_p = torch.tensor(proj_p_np).to(device).float()\n",
    "\n",
    "        p_batch = net(s_batch)\n",
    "        p_acting = p_batch[range(BATCH_SIZE), a_batch.data]\n",
    "\n",
    "        # loss\n",
    "        eps = np.finfo(np.float32).eps.item()\n",
    "        loss = -(proj_p * (p_acting + eps).log()).sum(dim=1).mean()\n",
    "        losses.append(loss)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FFhmC736lzHS"
   },
   "outputs": [],
   "source": [
    "def projection(next_p, rewards, dones):\n",
    "    proj_p = np.zeros((BATCH_SIZE, N_ATOMS), dtype=np.float32)\n",
    "    for atom in range(N_ATOMS):\n",
    "        z = np.minimum(V_MAX, np.maximum(\n",
    "            V_MIN, rewards + (V_MIN + atom * DELTA_Z) * GAMMA))\n",
    "        b = (z - V_MIN) / DELTA_Z\n",
    "        l = np.floor(b).astype(np.int64)\n",
    "        u = np.ceil(b).astype(np.int64)\n",
    "\n",
    "        eq_mask = u == l\n",
    "        proj_p[eq_mask, l[eq_mask]] += next_p[eq_mask, atom]\n",
    "        ne_mask = u != l\n",
    "        proj_p[ne_mask, l[ne_mask]] += next_p[ne_mask, atom] * (u - b)[ne_mask]\n",
    "        proj_p[ne_mask, u[ne_mask]] += next_p[ne_mask, atom] * (b - l)[ne_mask]\n",
    "\n",
    "        if dones.any():\n",
    "            proj_p[dones] = 0.0\n",
    "            z = np.minimum(V_MAX, np.maximum(V_MIN, rewards[dones]))\n",
    "            b = (z - V_MIN) / DELTA_Z\n",
    "            l = np.floor(b).astype(np.int64)\n",
    "            u = np.ceil(b).astype(np.int64)\n",
    "\n",
    "            eq_mask = u == l\n",
    "            eq_dones = dones.copy()\n",
    "            eq_dones[dones] = eq_mask\n",
    "            if eq_dones.any():\n",
    "                proj_p[eq_dones, l] = 1.0\n",
    "\n",
    "            ne_mask = u != l\n",
    "            ne_dones = dones.copy()\n",
    "            ne_dones[dones] = ne_mask\n",
    "            if ne_dones.any():\n",
    "                proj_p[ne_dones, l] = (u - b)[ne_mask]\n",
    "                proj_p[ne_dones, u] = (b - l)[ne_mask]\n",
    "\n",
    "    return proj_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 3377
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 135196,
     "status": "ok",
     "timestamp": 1534482559393,
     "user": {
      "displayName": "윤승제",
      "photoUrl": "//lh5.googleusercontent.com/-EucKC7DmcQI/AAAAAAAAAAI/AAAAAAAAAGA/gQU1NPEmNFA/s50-c-k-no/photo.jpg",
      "userId": "105654037995838004821"
     },
     "user_tz": -540
    },
    "id": "PnifSBJglzHh",
    "outputId": "94177345-918e-4a96-d9a8-d8aba0a4bc9a",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda: True\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jay/anaconda3/lib/python3.6/site-packages/gym/__init__.py:22: UserWarning: DEPRECATION WARNING: to improve load times, gym no longer automatically loads gym.spaces. Please run \"import gym.spaces\" to load gym.spaces on your own. This warning will turn into an error in a future version of gym.\n",
      "  warnings.warn('DEPRECATION WARNING: to improve load times, gym no longer automatically loads gym.spaces. Please run \"import gym.spaces\" to load gym.spaces on your own. This warning will turn into an error in a future version of gym.')\n"
     ]
    }
   ],
   "source": [
    "# set device\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print('cuda:', use_cuda)\n",
    "device = torch.device('cuda' if use_cuda else 'cpu')\n",
    "\n",
    "# random seed\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if use_cuda:\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# make an environment\n",
    "# env = gym.make('CartPole-v0')\n",
    "env = gym.make('CartPole-v1')\n",
    "# env = gym.make('MountainCar-v0')\n",
    "# env = gym.make('LunarLander-v2')\n",
    "env.seed(SEED)\n",
    "obs_space = env.observation_space.shape[0]\n",
    "action_space = env.action_space.n\n",
    "max_steps = env.spec.max_episode_steps*env.spec.timestep_limit\n",
    "\n",
    "# hyperparameter\n",
    "learn_start = env.spec.timestep_limit*5\n",
    "memory_size = learn_start*20\n",
    "update_frq = int(env.spec.timestep_limit/10)\n",
    "epsilon = 1.\n",
    "eps_min = 0.0025\n",
    "eps_decay = 1. - np.exp(np.log(eps_min)/(max_steps/2))\n",
    "num_eval = 10\n",
    "\n",
    "# global values\n",
    "total_steps = 0\n",
    "learn_steps = 0\n",
    "rewards = []\n",
    "reward_eval = deque(maxlen=num_eval)\n",
    "is_learned = False\n",
    "is_solved = False\n",
    "\n",
    "# make two nerual networks\n",
    "net = CategoricalDuelingDQN(obs_space, action_space, N_ATOMS).to(device)\n",
    "target_net = deepcopy(net)\n",
    "\n",
    "# make optimizer\n",
    "# optimizer = optim.SGD(net.parameters(), momentum=0.9, lr=LR, weight_decay=1e-4)\n",
    "optimizer = optim.Adam(net.parameters(), lr=LR, eps=EPS)\n",
    "\n",
    "# make memory\n",
    "rep_memory = deque(maxlen=memory_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.spec.max_episode_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.spec.reward_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.spec.timestep_limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 3377
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 135196,
     "status": "ok",
     "timestamp": 1534482559393,
     "user": {
      "displayName": "윤승제",
      "photoUrl": "//lh5.googleusercontent.com/-EucKC7DmcQI/AAAAAAAAAAI/AAAAAAAAAGA/gQU1NPEmNFA/s50-c-k-no/photo.jpg",
      "userId": "105654037995838004821"
     },
     "user_tz": -540
    },
    "id": "PnifSBJglzHh",
    "outputId": "94177345-918e-4a96-d9a8-d8aba0a4bc9a",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1 Episode in    60 steps, reward -126.51\n",
      "  2 Episode in   131 steps, reward -182.64\n",
      "  3 Episode in   203 steps, reward -106.16\n",
      "  4 Episode in   267 steps, reward -151.84\n",
      "  5 Episode in   337 steps, reward -156.44\n",
      "  6 Episode in   397 steps, reward -170.28\n",
      "  7 Episode in   489 steps, reward -199.73\n",
      "  8 Episode in   579 steps, reward -206.62\n",
      "  9 Episode in   645 steps, reward -155.45\n",
      " 10 Episode in   717 steps, reward -174.54\n",
      " 11 Episode in   791 steps, reward -168.89\n",
      " 12 Episode in   908 steps, reward -203.93\n",
      " 13 Episode in   985 steps, reward -281.83\n",
      " 14 Episode in  1089 steps, reward -243.68\n",
      " 15 Episode in  1153 steps, reward -169.49\n",
      " 16 Episode in  1271 steps, reward -187.91\n",
      " 17 Episode in  1370 steps, reward -266.76\n",
      " 18 Episode in  1443 steps, reward -275.86\n",
      " 19 Episode in  1511 steps, reward -135.63\n",
      " 20 Episode in  1612 steps, reward -335.32\n",
      " 21 Episode in  1741 steps, reward -155.99\n",
      " 22 Episode in  1806 steps, reward -269.69\n",
      " 23 Episode in  1926 steps, reward -424.74\n",
      " 24 Episode in  2035 steps, reward -346.32\n",
      " 25 Episode in  2143 steps, reward -383.43\n",
      " 26 Episode in  2263 steps, reward -204.01\n",
      " 27 Episode in  2326 steps, reward -148.76\n",
      " 28 Episode in  2400 steps, reward -164.86\n",
      " 29 Episode in  2458 steps, reward -117.33\n",
      " 30 Episode in  2555 steps, reward -479.47\n",
      " 31 Episode in  2658 steps, reward -219.11\n",
      " 32 Episode in  2755 steps, reward -160.66\n",
      " 33 Episode in  2858 steps, reward -168.60\n",
      " 34 Episode in  2932 steps, reward -109.12\n",
      " 35 Episode in  3027 steps, reward -398.35\n",
      " 36 Episode in  3094 steps, reward -138.72\n",
      " 37 Episode in  3164 steps, reward -216.25\n",
      " 38 Episode in  3253 steps, reward -174.10\n",
      " 39 Episode in  3352 steps, reward -115.50\n",
      " 40 Episode in  3494 steps, reward -214.16\n",
      " 41 Episode in  3607 steps, reward -307.64\n",
      " 42 Episode in  3691 steps, reward -111.78\n",
      " 43 Episode in  3838 steps, reward -153.17\n",
      " 44 Episode in  3943 steps, reward -356.11\n",
      " 45 Episode in  4052 steps, reward -456.27\n",
      " 46 Episode in  4142 steps, reward -168.53\n",
      " 47 Episode in  4290 steps, reward -558.24\n",
      " 48 Episode in  4379 steps, reward -180.80\n",
      " 49 Episode in  4476 steps, reward -195.09\n",
      " 50 Episode in  4552 steps, reward -152.49\n",
      " 51 Episode in  4640 steps, reward -462.28\n",
      " 52 Episode in  4714 steps, reward -85.03\n",
      " 53 Episode in  4819 steps, reward -303.36\n",
      " 54 Episode in  4943 steps, reward -193.32\n",
      "\n",
      "============  Start Learning  ============\n",
      "\n",
      " 55 Episode in  5030 steps, reward -258.33\n",
      " 56 Episode in  5144 steps, reward -169.09\n",
      " 57 Episode in  5214 steps, reward -191.10\n",
      " 58 Episode in  5348 steps, reward -192.03\n",
      " 59 Episode in  5490 steps, reward -111.76\n",
      " 60 Episode in  5570 steps, reward -106.70\n",
      " 61 Episode in  5644 steps, reward -150.19\n",
      " 62 Episode in  5734 steps, reward -208.48\n",
      " 63 Episode in  5852 steps, reward -222.88\n",
      " 64 Episode in  5969 steps, reward -227.08\n",
      " 65 Episode in  6085 steps, reward -277.48\n",
      " 66 Episode in  6158 steps, reward -168.38\n",
      " 67 Episode in  6262 steps, reward -192.05\n",
      " 68 Episode in  6380 steps, reward -200.05\n",
      " 69 Episode in  6475 steps, reward -154.69\n",
      " 70 Episode in  6573 steps, reward -358.66\n",
      " 71 Episode in  6650 steps, reward -12.19\n",
      " 72 Episode in  6730 steps, reward -141.76\n",
      " 73 Episode in  6832 steps, reward -208.32\n",
      " 74 Episode in  6969 steps, reward -167.87\n",
      " 75 Episode in  7031 steps, reward -128.49\n",
      " 76 Episode in  7134 steps, reward -278.49\n",
      " 77 Episode in  7277 steps, reward -523.80\n",
      " 78 Episode in  7349 steps, reward -167.01\n",
      " 79 Episode in  7455 steps, reward -226.27\n",
      " 80 Episode in  7551 steps, reward -242.83\n",
      " 81 Episode in  7622 steps, reward -176.74\n",
      " 82 Episode in  7688 steps, reward -118.55\n",
      " 83 Episode in  7790 steps, reward -164.61\n",
      " 84 Episode in  7858 steps, reward -103.75\n",
      " 85 Episode in  7926 steps, reward -150.08\n",
      " 86 Episode in  8017 steps, reward -178.86\n",
      " 87 Episode in  8110 steps, reward -406.66\n",
      " 88 Episode in  8211 steps, reward -309.54\n",
      " 89 Episode in  8313 steps, reward -165.29\n",
      " 90 Episode in  8392 steps, reward -146.30\n",
      " 91 Episode in  8505 steps, reward -144.80\n",
      " 92 Episode in  8576 steps, reward -160.62\n",
      " 93 Episode in  8640 steps, reward -146.05\n",
      " 94 Episode in  8716 steps, reward -26.42\n",
      " 95 Episode in  8788 steps, reward -133.77\n",
      " 96 Episode in  8863 steps, reward -158.66\n",
      " 97 Episode in  8963 steps, reward -599.09\n",
      " 98 Episode in  9032 steps, reward -158.28\n",
      " 99 Episode in  9129 steps, reward -172.65\n",
      "100 Episode in  9197 steps, reward -228.85\n",
      "101 Episode in  9304 steps, reward -206.32\n",
      "102 Episode in  9444 steps, reward -239.17\n",
      "103 Episode in  9542 steps, reward -146.34\n",
      "104 Episode in  9654 steps, reward -347.07\n",
      "105 Episode in  9730 steps, reward -127.56\n",
      "106 Episode in  9794 steps, reward -106.93\n",
      "107 Episode in  9876 steps, reward -316.11\n",
      "108 Episode in  9972 steps, reward -175.08\n",
      "109 Episode in 10068 steps, reward -356.95\n",
      "110 Episode in 10198 steps, reward -134.80\n",
      "111 Episode in 10319 steps, reward -155.87\n",
      "112 Episode in 10426 steps, reward -126.54\n",
      "113 Episode in 10525 steps, reward -135.23\n",
      "114 Episode in 10631 steps, reward -315.79\n",
      "115 Episode in 10770 steps, reward -140.97\n",
      "116 Episode in 10857 steps, reward -189.06\n",
      "117 Episode in 10942 steps, reward -89.80\n",
      "118 Episode in 11060 steps, reward -234.50\n",
      "119 Episode in 11173 steps, reward -299.94\n",
      "120 Episode in 11266 steps, reward -173.01\n",
      "121 Episode in 11346 steps, reward -201.70\n",
      "122 Episode in 11430 steps, reward -276.48\n",
      "123 Episode in 11539 steps, reward -190.48\n",
      "124 Episode in 11626 steps, reward -137.04\n",
      "125 Episode in 11699 steps, reward -202.21\n",
      "126 Episode in 11832 steps, reward -153.44\n",
      "127 Episode in 11906 steps, reward -130.50\n",
      "128 Episode in 12018 steps, reward -185.44\n",
      "129 Episode in 12114 steps, reward -473.85\n",
      "130 Episode in 12198 steps, reward -255.59\n",
      "131 Episode in 12261 steps, reward -142.20\n",
      "132 Episode in 12340 steps, reward -107.08\n",
      "133 Episode in 12457 steps, reward -100.66\n",
      "134 Episode in 12518 steps, reward -121.56\n",
      "135 Episode in 12616 steps, reward -147.62\n",
      "136 Episode in 12728 steps, reward -137.32\n",
      "137 Episode in 12847 steps, reward -245.95\n",
      "138 Episode in 12928 steps, reward -216.96\n",
      "139 Episode in 12990 steps, reward -125.96\n",
      "140 Episode in 13113 steps, reward -186.10\n",
      "141 Episode in 13222 steps, reward -153.55\n",
      "142 Episode in 13338 steps, reward -129.78\n",
      "143 Episode in 13413 steps, reward -118.57\n",
      "144 Episode in 13502 steps, reward -193.46\n",
      "145 Episode in 13587 steps, reward -183.92\n",
      "146 Episode in 13733 steps, reward -110.09\n",
      "147 Episode in 13797 steps, reward -236.90\n",
      "148 Episode in 13860 steps, reward -138.04\n",
      "149 Episode in 13948 steps, reward -120.76\n",
      "150 Episode in 14066 steps, reward -238.44\n",
      "151 Episode in 14143 steps, reward -139.40\n",
      "152 Episode in 14231 steps, reward -121.82\n",
      "153 Episode in 14330 steps, reward -117.53\n",
      "154 Episode in 14416 steps, reward -148.59\n",
      "155 Episode in 14545 steps, reward -123.54\n",
      "156 Episode in 14648 steps, reward -170.81\n",
      "157 Episode in 14742 steps, reward -208.09\n",
      "158 Episode in 14843 steps, reward -154.31\n",
      "159 Episode in 14958 steps, reward -341.34\n",
      "160 Episode in 15041 steps, reward -273.00\n",
      "161 Episode in 15139 steps, reward -123.10\n",
      "162 Episode in 15280 steps, reward -180.62\n",
      "163 Episode in 15408 steps, reward -204.66\n",
      "164 Episode in 15518 steps, reward -238.69\n",
      "165 Episode in 15668 steps, reward -262.64\n",
      "166 Episode in 15742 steps, reward -158.12\n",
      "167 Episode in 15809 steps, reward -135.47\n",
      "168 Episode in 15899 steps, reward -136.65\n",
      "169 Episode in 15996 steps, reward -119.17\n",
      "170 Episode in 16132 steps, reward -359.27\n",
      "171 Episode in 16205 steps, reward -144.14\n",
      "172 Episode in 16313 steps, reward -182.43\n",
      "173 Episode in 16407 steps, reward -89.08\n",
      "174 Episode in 16526 steps, reward -156.59\n",
      "175 Episode in 16600 steps, reward -175.37\n",
      "176 Episode in 16683 steps, reward -181.54\n",
      "177 Episode in 16860 steps, reward -84.91\n",
      "178 Episode in 16935 steps, reward -116.15\n",
      "179 Episode in 17004 steps, reward -151.02\n",
      "180 Episode in 17123 steps, reward -172.41\n",
      "181 Episode in 17230 steps, reward -158.01\n",
      "182 Episode in 17294 steps, reward -132.84\n",
      "183 Episode in 17417 steps, reward -143.23\n",
      "184 Episode in 17533 steps, reward -128.11\n",
      "185 Episode in 17628 steps, reward -90.62\n",
      "186 Episode in 17757 steps, reward -122.50\n",
      "187 Episode in 17832 steps, reward -140.66\n",
      "188 Episode in 17947 steps, reward -441.76\n",
      "189 Episode in 18108 steps, reward -89.10\n",
      "190 Episode in 18186 steps, reward -86.90\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "191 Episode in 18283 steps, reward -155.31\n",
      "192 Episode in 18354 steps, reward -123.69\n",
      "193 Episode in 18420 steps, reward -96.51\n",
      "194 Episode in 18554 steps, reward -346.20\n",
      "195 Episode in 18674 steps, reward -212.46\n",
      "196 Episode in 18783 steps, reward -184.14\n",
      "197 Episode in 18900 steps, reward -244.31\n",
      "198 Episode in 19010 steps, reward -400.08\n",
      "199 Episode in 19124 steps, reward -181.77\n",
      "200 Episode in 19279 steps, reward -85.77\n",
      "201 Episode in 19412 steps, reward -118.68\n",
      "202 Episode in 19523 steps, reward -207.00\n",
      "203 Episode in 19655 steps, reward -77.45\n",
      "204 Episode in 19742 steps, reward -111.51\n",
      "205 Episode in 19822 steps, reward -160.05\n",
      "206 Episode in 19927 steps, reward -274.04\n",
      "207 Episode in 20043 steps, reward -394.49\n",
      "208 Episode in 20147 steps, reward -127.98\n",
      "209 Episode in 20243 steps, reward -180.43\n",
      "210 Episode in 20348 steps, reward -150.00\n",
      "211 Episode in 20468 steps, reward -344.45\n",
      "212 Episode in 20578 steps, reward -52.45\n",
      "213 Episode in 20668 steps, reward -267.32\n",
      "214 Episode in 20749 steps, reward -117.23\n",
      "215 Episode in 20849 steps, reward -192.33\n",
      "216 Episode in 20954 steps, reward -259.69\n",
      "217 Episode in 21053 steps, reward -95.41\n",
      "218 Episode in 21203 steps, reward -137.10\n",
      "219 Episode in 21276 steps, reward -117.34\n",
      "220 Episode in 21371 steps, reward -126.37\n",
      "221 Episode in 21488 steps, reward -355.94\n",
      "222 Episode in 21579 steps, reward -111.09\n",
      "223 Episode in 21647 steps, reward -104.30\n",
      "224 Episode in 21760 steps, reward -150.12\n",
      "225 Episode in 21858 steps, reward -175.44\n",
      "226 Episode in 21934 steps, reward -115.24\n",
      "227 Episode in 22136 steps, reward -391.26\n",
      "228 Episode in 22254 steps, reward -316.98\n",
      "229 Episode in 22357 steps, reward -131.04\n",
      "230 Episode in 22438 steps, reward -106.77\n",
      "231 Episode in 22543 steps, reward -122.09\n",
      "232 Episode in 22653 steps, reward -148.50\n",
      "233 Episode in 22755 steps, reward -147.56\n",
      "234 Episode in 22829 steps, reward -139.22\n",
      "235 Episode in 22904 steps, reward -138.85\n",
      "236 Episode in 23005 steps, reward -155.01\n",
      "237 Episode in 23121 steps, reward -252.34\n",
      "238 Episode in 23278 steps, reward -183.87\n",
      "239 Episode in 23342 steps, reward -122.16\n",
      "240 Episode in 23466 steps, reward -388.06\n",
      "241 Episode in 23556 steps, reward -118.76\n",
      "242 Episode in 23654 steps, reward -146.43\n",
      "243 Episode in 23771 steps, reward -119.84\n",
      "244 Episode in 23848 steps, reward -177.54\n",
      "245 Episode in 23936 steps, reward -53.13\n",
      "246 Episode in 24060 steps, reward -112.01\n",
      "247 Episode in 24154 steps, reward -212.29\n",
      "248 Episode in 24238 steps, reward -114.18\n",
      "249 Episode in 24369 steps, reward -377.82\n",
      "250 Episode in 24451 steps, reward -76.49\n",
      "251 Episode in 24600 steps, reward -163.62\n",
      "252 Episode in 24798 steps, reward -336.17\n",
      "253 Episode in 24889 steps, reward -97.51\n",
      "254 Episode in 24994 steps, reward -121.95\n",
      "255 Episode in 25103 steps, reward -141.29\n",
      "256 Episode in 25223 steps, reward -131.81\n",
      "257 Episode in 25353 steps, reward -288.94\n",
      "258 Episode in 25506 steps, reward -243.32\n",
      "259 Episode in 25630 steps, reward -148.54\n",
      "260 Episode in 25783 steps, reward -283.06\n",
      "261 Episode in 25872 steps, reward -83.74\n",
      "262 Episode in 25960 steps, reward -113.12\n",
      "263 Episode in 26094 steps, reward -267.77\n",
      "264 Episode in 26262 steps, reward -47.07\n",
      "265 Episode in 26567 steps, reward -274.87\n",
      "266 Episode in 26686 steps, reward -143.63\n",
      "267 Episode in 26767 steps, reward -133.42\n",
      "268 Episode in 26861 steps, reward -235.52\n",
      "269 Episode in 26931 steps, reward -133.57\n",
      "270 Episode in 27066 steps, reward -188.15\n",
      "271 Episode in 27159 steps, reward -213.69\n",
      "272 Episode in 27298 steps, reward -174.85\n",
      "273 Episode in 27413 steps, reward -446.69\n",
      "274 Episode in 27536 steps, reward -189.81\n",
      "275 Episode in 27677 steps, reward -377.44\n",
      "276 Episode in 27760 steps, reward -114.71\n",
      "277 Episode in 27875 steps, reward -90.66\n",
      "278 Episode in 27969 steps, reward -78.34\n",
      "279 Episode in 28049 steps, reward -88.11\n",
      "280 Episode in 28197 steps, reward -32.62\n",
      "281 Episode in 28332 steps, reward -90.17\n",
      "282 Episode in 28464 steps, reward -215.14\n",
      "283 Episode in 28601 steps, reward -139.98\n",
      "284 Episode in 28754 steps, reward -130.70\n",
      "285 Episode in 28836 steps, reward -148.84\n",
      "286 Episode in 28933 steps, reward -85.04\n",
      "287 Episode in 29058 steps, reward -335.86\n",
      "288 Episode in 29160 steps, reward -216.22\n",
      "289 Episode in 29272 steps, reward -538.68\n",
      "290 Episode in 29385 steps, reward -172.43\n",
      "291 Episode in 29452 steps, reward -114.93\n",
      "292 Episode in 29543 steps, reward -74.43\n",
      "293 Episode in 29663 steps, reward -150.31\n",
      "294 Episode in 29809 steps, reward -207.52\n",
      "295 Episode in 29887 steps, reward -97.47\n",
      "296 Episode in 29995 steps, reward -335.84\n",
      "297 Episode in 30208 steps, reward -88.20\n",
      "298 Episode in 30312 steps, reward -101.09\n",
      "299 Episode in 30408 steps, reward -113.39\n",
      "300 Episode in 30512 steps, reward -149.07\n",
      "301 Episode in 30624 steps, reward -209.40\n",
      "302 Episode in 30795 steps, reward -344.99\n",
      "303 Episode in 30913 steps, reward -370.89\n",
      "304 Episode in 31044 steps, reward -222.49\n",
      "305 Episode in 31174 steps, reward -201.86\n",
      "306 Episode in 31261 steps, reward -137.91\n",
      "307 Episode in 31357 steps, reward -69.24\n",
      "308 Episode in 31470 steps, reward -154.92\n",
      "309 Episode in 31593 steps, reward -98.43\n",
      "310 Episode in 31679 steps, reward -119.94\n",
      "311 Episode in 31793 steps, reward -81.81\n",
      "312 Episode in 31941 steps, reward -331.86\n",
      "313 Episode in 32068 steps, reward -310.35\n",
      "314 Episode in 32147 steps, reward -91.36\n",
      "315 Episode in 32251 steps, reward -150.81\n",
      "316 Episode in 32376 steps, reward -251.16\n",
      "317 Episode in 32516 steps, reward -156.87\n",
      "318 Episode in 32633 steps, reward -81.33\n",
      "319 Episode in 32718 steps, reward -100.90\n",
      "320 Episode in 32848 steps, reward -314.31\n",
      "321 Episode in 32929 steps, reward -74.49\n",
      "322 Episode in 33068 steps, reward -173.78\n",
      "323 Episode in 33191 steps, reward -236.14\n",
      "324 Episode in 33312 steps, reward -291.85\n",
      "325 Episode in 33398 steps, reward -154.44\n",
      "326 Episode in 33470 steps, reward -74.11\n",
      "327 Episode in 33557 steps, reward -72.93\n",
      "328 Episode in 33683 steps, reward -102.51\n",
      "329 Episode in 33840 steps, reward -193.64\n",
      "330 Episode in 33988 steps, reward -136.09\n",
      "331 Episode in 34108 steps, reward -72.64\n",
      "332 Episode in 34207 steps, reward -89.03\n",
      "333 Episode in 34323 steps, reward -117.21\n",
      "334 Episode in 34406 steps, reward -47.57\n",
      "335 Episode in 34503 steps, reward -71.87\n",
      "336 Episode in 34637 steps, reward -126.66\n",
      "337 Episode in 34739 steps, reward -171.07\n",
      "338 Episode in 34914 steps, reward -311.06\n",
      "339 Episode in 35000 steps, reward -84.23\n",
      "340 Episode in 35108 steps, reward -158.76\n",
      "341 Episode in 35217 steps, reward -301.63\n",
      "342 Episode in 35384 steps, reward -4.81\n",
      "343 Episode in 35469 steps, reward -93.33\n",
      "344 Episode in 35568 steps, reward -247.53\n",
      "345 Episode in 35673 steps, reward -106.27\n",
      "346 Episode in 35777 steps, reward -144.40\n",
      "347 Episode in 35848 steps, reward -89.61\n",
      "348 Episode in 35978 steps, reward -172.14\n",
      "349 Episode in 36280 steps, reward -423.59\n",
      "350 Episode in 36467 steps, reward -167.66\n",
      "351 Episode in 36620 steps, reward -64.38\n",
      "352 Episode in 36777 steps, reward -54.82\n",
      "353 Episode in 36873 steps, reward -109.17\n",
      "354 Episode in 36964 steps, reward -89.40\n",
      "355 Episode in 37105 steps, reward -394.75\n",
      "356 Episode in 37191 steps, reward -127.32\n",
      "357 Episode in 37294 steps, reward -183.82\n",
      "358 Episode in 37437 steps, reward -101.46\n",
      "359 Episode in 37626 steps, reward -45.47\n",
      "360 Episode in 37772 steps, reward -108.31\n",
      "361 Episode in 38093 steps, reward -294.97\n",
      "362 Episode in 38206 steps, reward -189.09\n",
      "363 Episode in 38346 steps, reward -191.72\n",
      "364 Episode in 38436 steps, reward -119.09\n",
      "365 Episode in 38531 steps, reward -288.18\n",
      "366 Episode in 38649 steps, reward -144.59\n",
      "367 Episode in 38749 steps, reward -164.11\n",
      "368 Episode in 38923 steps, reward -370.41\n",
      "369 Episode in 39033 steps, reward -295.24\n",
      "370 Episode in 39177 steps, reward -72.45\n",
      "371 Episode in 39307 steps, reward -114.34\n",
      "372 Episode in 39411 steps, reward -125.13\n",
      "373 Episode in 39535 steps, reward -99.67\n",
      "374 Episode in 39632 steps, reward -71.92\n",
      "375 Episode in 39806 steps, reward -84.93\n",
      "376 Episode in 39940 steps, reward -40.63\n",
      "377 Episode in 40069 steps, reward -126.97\n",
      "378 Episode in 40191 steps, reward -82.57\n",
      "379 Episode in 40371 steps, reward -89.42\n",
      "380 Episode in 40597 steps, reward -147.96\n",
      "381 Episode in 40767 steps, reward -81.22\n",
      "382 Episode in 40905 steps, reward -259.45\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "383 Episode in 41113 steps, reward -111.21\n",
      "384 Episode in 41240 steps, reward -236.26\n"
     ]
    }
   ],
   "source": [
    "# play\n",
    "for i in range(1, env.spec.max_episode_steps+1):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    ep_reward = 0\n",
    "    while not done:\n",
    "        env.render()\n",
    "        if np.random.rand() < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            target_net.eval()\n",
    "            with torch.no_grad():\n",
    "                state = torch.tensor([obs]).to(device).float()\n",
    "                probs = target_net(state)\n",
    "                weights = probs * net.support\n",
    "                q = weights.sum(dim=2)\n",
    "                q_np = q.cpu().numpy()[0]\n",
    "                action = np.argmax(q_np)\n",
    "\n",
    "        _obs, reward, done, _ = env.step(action)\n",
    "\n",
    "        rep_memory.append((obs, action, reward, _obs, done))\n",
    "\n",
    "        obs = _obs\n",
    "        total_steps += 1\n",
    "        ep_reward += reward\n",
    "        epsilon -= epsilon * eps_decay\n",
    "        epsilon = max(eps_min, epsilon)\n",
    "\n",
    "        if len(rep_memory) >= learn_start:\n",
    "            if len(rep_memory) == learn_start:\n",
    "                print('\\n============  Start Learning  ============\\n')\n",
    "            learn(net, target_net, optimizer, rep_memory)\n",
    "            learn_steps += 1\n",
    "\n",
    "        if learn_steps == update_frq:\n",
    "            target_net.load_state_dict(net.state_dict())\n",
    "            learn_steps = 0\n",
    "    if done:\n",
    "        rewards.append(ep_reward)\n",
    "        reward_eval.append(ep_reward)\n",
    "        print('{:3} Episode in {:5} steps, reward {:.2f}'.format(\n",
    "            i, total_steps, ep_reward))\n",
    "\n",
    "        if len(reward_eval) >= num_eval:\n",
    "            if np.mean(reward_eval) >= env.spec.reward_threshold:\n",
    "                print('\\n{} is sloved! {:3} Episode in {:3} steps'.format(\n",
    "                    env.spec.id, i, total_steps))\n",
    "                break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "plt.title('reward')\n",
    "plt.plot(rewards)\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.title('loss')\n",
    "plt.plot(losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "C51_tensorflow.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
