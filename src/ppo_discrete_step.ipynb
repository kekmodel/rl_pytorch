{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IWnm3qot3o1W"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import deque\n",
    "from copy import deepcopy\n",
    "\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "from torch.utils.data import DataLoader\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IWnm3qot3o1W"
   },
   "outputs": [],
   "source": [
    "SEED = 5\n",
    "BATCH_SIZE = 256\n",
    "LR = 0.01\n",
    "EPOCHS = 10\n",
    "CLIP = 0.2\n",
    "GAMMA = 0.999\n",
    "LAMBDA = 0.98\n",
    "ENT_COEF = 0.01\n",
    "V_COEF = 0.5\n",
    "V_CLIP = True\n",
    "LIN_REDUCE = False\n",
    "GRAD_NORM = False\n",
    "# set device\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print('cuda:', use_cuda)\n",
    "device = torch.device('cuda' if use_cuda else 'cpu')\n",
    "\n",
    "# random seed\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if use_cuda:\n",
    "    torch.cuda.manual_seed_all(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IWnm3qot3o1W"
   },
   "outputs": [],
   "source": [
    "class ActorCriticNet(nn.Module):\n",
    "    def __init__(self, obs_space, action_space):\n",
    "        super().__init__()\n",
    "        h = 32\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(obs_space, h),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.pol = nn.Sequential(\n",
    "            nn.Linear(h, h),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(h, action_space)\n",
    "        )\n",
    "        self.val = nn.Sequential(\n",
    "            nn.Linear(h, h),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(h, 1)\n",
    "        )\n",
    "        self.log_softmax = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.head(x)\n",
    "        logit = self.pol(out).reshape(out.shape[0], -1)\n",
    "        log_p = self.log_softmax(logit)\n",
    "        v = self.val(out).reshape(out.shape[0], 1)\n",
    "\n",
    "        return log_p, v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IWnm3qot3o1W"
   },
   "outputs": [],
   "source": [
    "losses = []\n",
    "\n",
    "\n",
    "def learn(net, old_net, optimizer, train_memory):\n",
    "    global CLIP, LR\n",
    "    global total_epochs\n",
    "    net.train()\n",
    "    old_net.train()\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        if LIN_REDUCE:\n",
    "            lr = LR - (LR * epoch / total_epochs)\n",
    "            clip = CLIP - (CLIP * epoch / total_epochs)\n",
    "        else:\n",
    "            lr = LR\n",
    "            clip = CLIP\n",
    "        \n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "        \n",
    "        dataloader = DataLoader(\n",
    "            train_memory,\n",
    "            shuffle=True,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            pin_memory=use_cuda\n",
    "        )\n",
    "        \n",
    "        for (s, a, ret, adv) in dataloader:\n",
    "            s_batch = s.to(device).float()\n",
    "            a_batch = a.to(device).long()\n",
    "            ret_batch = ret.to(device).float()\n",
    "            ret_batch = (ret_batch - ret_batch.mean()) / \\\n",
    "                (ret_batch.std() + 1e-6)\n",
    "            adv_batch = adv.to(device).float()\n",
    "            adv_batch = (adv_batch - adv_batch.mean()) / \\\n",
    "                (adv_batch.std() + 1e-6)\n",
    "            with torch.no_grad():\n",
    "                log_p_batch_old, v_batch_old = old_net(s_batch)\n",
    "                log_p_acting_old = log_p_batch_old[range(BATCH_SIZE), a_batch]\n",
    "\n",
    "            log_p_batch, v_batch = net(s_batch)\n",
    "            log_p_acting = log_p_batch[range(BATCH_SIZE), a_batch]\n",
    "            p_ratio = (log_p_acting - log_p_acting_old).exp()\n",
    "            p_ratio_clip = torch.clamp(p_ratio, 1 - clip, 1 + clip)\n",
    "            p_loss = torch.min(p_ratio * adv_batch,\n",
    "                               p_ratio_clip * adv_batch).mean()\n",
    "            if V_CLIP:\n",
    "                v_clip = v_batch_old + \\\n",
    "                    torch.clamp(v_batch - v_batch_old, -clip, clip)\n",
    "                v_loss1 = (ret_batch - v_clip).pow(2)\n",
    "                v_loss2 = (ret_batch - v_batch).pow(2)\n",
    "                v_loss = 0.5 * torch.max(v_loss1, v_loss2).mean()\n",
    "            else:\n",
    "                v_loss = 0.5 * (ret_batch - v_batch).pow(2).mean()\n",
    "\n",
    "            log_p, _ = net(s_batch)\n",
    "            entropy = -(log_p.exp() * log_p).sum(dim=1).mean()\n",
    "\n",
    "            # loss\n",
    "            loss = -(p_loss - V_COEF * v_loss + ENT_COEF * entropy)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            if GRAD_NORM:\n",
    "                nn.utils.clip_grad_norm_(net.parameters(), max_norm=0.5)\n",
    "            optimizer.step()\n",
    "    train_memory.clear()\n",
    "\n",
    "\n",
    "def get_action_and_value(obs, old_net):\n",
    "    old_net.eval()\n",
    "    with torch.no_grad():\n",
    "        state = torch.tensor([obs]).to(device).float()\n",
    "        log_p, v = old_net(state)\n",
    "        m = Categorical(log_p.exp())\n",
    "        action = m.sample()\n",
    "\n",
    "    return action.item(), v.item()\n",
    "\n",
    "\n",
    "def compute_adv_with_gae(rewards, values, roll_memory):\n",
    "    rew = np.array(rewards, 'float')\n",
    "    val = np.array(values[:-1], 'float')\n",
    "    _val = np.array(values[1:], 'float')\n",
    "    delta = rew + GAMMA * _val - val\n",
    "    dis_r = np.array([GAMMA**(i) * r for i, r in enumerate(rewards)], 'float')\n",
    "    gae_dt = np.array([(GAMMA * LAMBDA)**(i) * dt for i,\n",
    "                       dt in enumerate(delta.tolist())], 'float')\n",
    "    for i, data in enumerate(roll_memory):\n",
    "        data.append(sum(dis_r[i:] / GAMMA**(i)))\n",
    "        data.append(sum(gae_dt[i:] / (GAMMA * LAMBDA)**(i)))\n",
    "\n",
    "    rewards.clear()\n",
    "    values.clear()\n",
    "\n",
    "    return roll_memory\n",
    "\n",
    "\n",
    "def plot():\n",
    "    clear_output(True)\n",
    "    plt.figure(figsize=(16, 5))\n",
    "    plt.subplot(121)\n",
    "    plt.plot(ep_rewards, alpha=0.5)\n",
    "    plt.subplot(121)\n",
    "    plt.plot(reward_eval)\n",
    "    plt.title(f'Reward: '\n",
    "              f'{reward_eval[-1]}')\n",
    "    plt.subplot(122)\n",
    "    plt.plot(losses, alpha=0.5)\n",
    "    plt.title(f'Loss: '\n",
    "              f'{np.mean(list(reversed(losses))[: n_eval]).round(decimals=2)}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IWnm3qot3o1W",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# make an environment\n",
    "# env = gym.make('CartPole-v0')\n",
    "# env = gym.make('CartPole-v1')\n",
    "env = gym.make('MountainCar-v0')\n",
    "# env = gym.make('LunarLander-v2')\n",
    "\n",
    "env.seed(SEED)\n",
    "obs_space = env.observation_space.shape[0]\n",
    "action_space = env.action_space.n\n",
    "\n",
    "# hyperparameter\n",
    "n_episodes = 100000\n",
    "roll_len = 2048\n",
    "total_epochs = roll_len // BATCH_SIZE\n",
    "n_eval = 10\n",
    "\n",
    "# global values\n",
    "steps = 0\n",
    "ep_rewards = []\n",
    "reward_eval = []\n",
    "is_rollout = False\n",
    "is_solved = False\n",
    "\n",
    "# make memories\n",
    "train_memory = []\n",
    "roll_memory = []\n",
    "rewards = []\n",
    "values = []\n",
    "\n",
    "# make nerual networks\n",
    "net = ActorCriticNet(obs_space, action_space).to(device)\n",
    "old_net = deepcopy(net)\n",
    "# no_decay = ['bias']\n",
    "# grouped_parameters = [\n",
    "#         {'params': [p for n, p in net.named_parameters() if not any(\n",
    "#             nd in n for nd in no_decay)], 'weight_decay': 0.0},\n",
    "#         {'params': [p for n, p in net.named_parameters() if any(\n",
    "#             nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "# ]\n",
    "optimizer = torch.optim.AdamW(net.parameters(), lr=LR, eps=1e-6)\n",
    "\n",
    "# play!\n",
    "for i in range(1, n_episodes + 1):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    ep_reward = 0\n",
    "    while not done:\n",
    "        env.render()\n",
    "        action, value = get_action_and_value(obs, old_net)\n",
    "        _obs, reward, done, _ = env.step(action)\n",
    "\n",
    "        # store\n",
    "        roll_memory.append([obs, action])\n",
    "        rewards.append(reward)\n",
    "        values.append(value)\n",
    "\n",
    "        obs = _obs\n",
    "        steps += 1\n",
    "        ep_reward += reward\n",
    "\n",
    "        if done or steps % roll_len == 0:\n",
    "            if done:\n",
    "                _value = 0.\n",
    "            else:\n",
    "                _, _value = get_action_and_value(_obs, old_net)\n",
    "\n",
    "            values.append(_value)\n",
    "            train_memory.extend(compute_adv_with_gae(\n",
    "                rewards, values, roll_memory))\n",
    "            roll_memory.clear()\n",
    "\n",
    "        if steps % roll_len == 0:\n",
    "            learn(net, old_net, optimizer, train_memory)\n",
    "            old_net.load_state_dict(net.state_dict())\n",
    "\n",
    "    if done:\n",
    "        ep_rewards.append(ep_reward)\n",
    "        reward_eval.append(\n",
    "            np.mean(list(reversed(ep_rewards))[: n_eval]).round(decimals=2))\n",
    "        plot()\n",
    "#         print('{:3} Episode in {:5} steps, reward {:.2f}'.format(\n",
    "#             i, steps, ep_reward))\n",
    "\n",
    "        if len(ep_rewards) >= n_eval:\n",
    "            if reward_eval[-1] >= env.spec.reward_threshold:\n",
    "                #             if reward_eval[-1] >= 495:\n",
    "                print('\\n{} is sloved! {:3} Episode in {:3} steps'.format(\n",
    "                    env.spec.id, i, steps))\n",
    "                torch.save(net.state_dict(),\n",
    "                           f'./test/saved_models/{env.spec.id}_ep{i}_clear_model_ppo_st.pt')\n",
    "                break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.spec.max_episode_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IWnm3qot3o1W"
   },
   "outputs": [],
   "source": [
    "[\n",
    "    ('CartPole-v0', 889, 2048, 0.2, 10, 0.5, 0.01, False, 0.999, 0.98),\n",
    "    ('CartPole-v1', 801, 2048, 0.2, 10, 0.5, 0.01, False, 0.999, 0.98),\n",
    "    ('MountainCar-v0', None),\n",
    "    ('LunarLander-v2', 876, 2048, 0.2, 10, 1.0, 0.01, False, 0.999, 0.98)\n",
    "]"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "C51_tensorflow.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
