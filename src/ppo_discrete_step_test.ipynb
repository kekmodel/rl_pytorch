{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IWnm3qot3o1W"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import deque\n",
    "from copy import deepcopy\n",
    "\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "from torch.utils.data import DataLoader\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from running_mean_std import RunningMeanStd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IWnm3qot3o1W"
   },
   "outputs": [],
   "source": [
    "SEED = 5\n",
    "BATCH_SIZE = 2048\n",
    "P_LR = 3e-4\n",
    "V_LR = 1e-3\n",
    "ITER = 80\n",
    "CLIP = 0.2\n",
    "GAMMA = 0.999\n",
    "LAMBDA = 0.97\n",
    "BETA = 3.0\n",
    "# ENT_COEF = 0.0\n",
    "GRAD_NORM = False\n",
    "OBS_NORM = True\n",
    "VIEW_CURVE = False\n",
    "\n",
    "# set device\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device('cuda' if use_cuda else 'cpu')\n",
    "\n",
    "# random seed\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if use_cuda:\n",
    "    torch.cuda.manual_seed_all(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IWnm3qot3o1W"
   },
   "outputs": [],
   "source": [
    "class ActorCriticNet(nn.Module):\n",
    "    def __init__(self, obs_space, action_space):\n",
    "        super().__init__()\n",
    "        h = 32\n",
    "        self.pol = nn.Sequential(\n",
    "            nn.Linear(obs_space, h),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(h, h),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(h, action_space)\n",
    "        )\n",
    "        self.val = nn.Sequential(\n",
    "            nn.Linear(obs_space, h),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(h, h),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(h, 1)\n",
    "        )\n",
    "        self.log_softmax = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        logit = self.pol(x).reshape(x.shape[0], -1)\n",
    "        log_p = self.log_softmax(logit)\n",
    "        v = self.val(x).reshape(x.shape[0], 1)\n",
    "        return log_p, v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IWnm3qot3o1W"
   },
   "outputs": [],
   "source": [
    "losses = []\n",
    "kl_divs = []\n",
    "\n",
    "def learn(net, old_net, optimizer, train_memory):\n",
    "    net.train()\n",
    "    old_net.train()\n",
    "    dataloader = DataLoader(\n",
    "            train_memory,\n",
    "            shuffle=False,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            pin_memory=True,\n",
    "            num_workers=0,\n",
    "        )\n",
    "    for _ in range(ITER):        \n",
    "        for i, (s, a, ret, adv) in enumerate(dataloader):\n",
    "            s = s.to(device).float()\n",
    "            a = a.to(device).long()\n",
    "            ret = ret.to(device).float()\n",
    "            adv = adv.to(device).float()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                log_p_old, v_old = old_net(s)\n",
    "                log_p_act_old = log_p_old[range(BATCH_SIZE), a]\n",
    "\n",
    "            log_p, v = net(s)\n",
    "            log_p_act = log_p[range(BATCH_SIZE), a]\n",
    "            p_ratio = (log_p_act - log_p_act_old).exp()\n",
    "            p_ratio_clip = torch.clamp(p_ratio, 1 - CLIP, 1 + CLIP)\n",
    "            p_loss = -(torch.min(p_ratio * adv, p_ratio_clip * adv).mean())\n",
    "#             kl_div = (log_p.exp() * (log_p - log_p_old)).sum(dim=-1)\n",
    "#             p_loss = -(p_ratio * adv - BETA * kl_div).mean()\n",
    "            v_loss = (ret - v).pow(2).mean()\n",
    "#             log_p, _ = net(s)\n",
    "#             entropy = -(log_p.exp() * log_p).sum(dim=1).mean()\n",
    "            # loss\n",
    "#             loss = p_loss + v_loss - ENT_COEF * entropy\n",
    "            kl_div = ((log_p.exp() * (log_p - log_p_old)).sum(dim=-1).mean()).detach().item()\n",
    "            loss = p_loss + v_loss\n",
    "            losses.append(loss.item())\n",
    "            kl_divs.append(kl_div)\n",
    "            if kl_div <= 0.01 * 1.5:\n",
    "                optimizer[0].zero_grad()\n",
    "                p_loss.backward()\n",
    "                if GRAD_NORM:\n",
    "                    nn.utils.clip_grad_norm_(net.parameters() , max_norm=1.0)\n",
    "                optimizer[0].step()\n",
    "            else:\n",
    "                if not VIEW_CURVE:\n",
    "                    print(\"Pass the Pi update!\")\n",
    "            optimizer[1].zero_grad()\n",
    "            v_loss.backward()\n",
    "            if GRAD_NORM:\n",
    "                nn.utils.clip_grad_norm_(net.parameters(), max_norm=1.0)\n",
    "            optimizer[1].step()\n",
    "\n",
    "\n",
    "def get_action_and_value(obs, old_net):\n",
    "    old_net.eval()\n",
    "    with torch.no_grad():\n",
    "        state = torch.tensor([obs]).to(device).float()\n",
    "        log_p, v = old_net(state)\n",
    "        m = Categorical(log_p.exp())\n",
    "        action = m.sample()\n",
    "    return action.item(), v.item()\n",
    "\n",
    "\n",
    "def compute_adv_with_gae(rewards, values, roll_memory):\n",
    "    rew = np.array(rewards, np.float32)\n",
    "    val = np.array(values[:-1], np.float32)\n",
    "    _val = np.array(values[1:], np.float32)\n",
    "    delta = rew + GAMMA * _val - val\n",
    "    dis_r = np.array([GAMMA**(i) * r for i, r in enumerate(rewards)], np.float32)\n",
    "    gae_dt = np.array([(GAMMA * LAMBDA)**(i) * dt for i, dt in enumerate(delta.tolist())], np.float32)\n",
    "    for i, data in enumerate(roll_memory):\n",
    "        data.append(sum(dis_r[i:] / GAMMA**(i)))\n",
    "        data.append(sum(gae_dt[i:] / (GAMMA * LAMBDA)**(i)))\n",
    "\n",
    "    rewards.clear()\n",
    "    values.clear()\n",
    "    return roll_memory\n",
    "\n",
    "\n",
    "def plot():\n",
    "    clear_output(True)\n",
    "    plt.figure(figsize=(16, 5))\n",
    "    plt.subplot(131)\n",
    "    plt.plot(ep_rewards, alpha=0.5)\n",
    "    plt.subplot(131)\n",
    "    plt.plot(reward_eval)\n",
    "    plt.title(f'Reward: {reward_eval[-1]:.0f}')\n",
    "    if losses:\n",
    "        plt.subplot(132)\n",
    "        plt.plot(losses, alpha=0.5)\n",
    "        plt.title(f'Loss: {losses[-1]:.2f}')\n",
    "        plt.subplot(133)\n",
    "        plt.plot(kl_divs, alpha=0.5)\n",
    "        plt.title(f'kl_div: {kl_divs[-1]:.4f}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IWnm3qot3o1W",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_episodes: 625\n",
      "reward_threshold: 300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/media/ai/Storage/gym/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "new(): argument 'size' must be tuple of ints, but found element of type tuple at pos 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-6d2d760f9365>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;31m# make nerual networks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mActorCriticNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs_space\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_space\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0mold_net\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-a1d814110dc1>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, obs_space, action_space)\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         self.pol = nn.Sequential(\n\u001b[0;32m----> 6\u001b[0;31m             \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs_space\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m             \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, in_features, out_features, bias)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0min_features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout_features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: new(): argument 'size' must be tuple of ints, but found element of type tuple at pos 2"
     ]
    }
   ],
   "source": [
    "# make an environment\n",
    "# env = gym.make('CartPole-v0')\n",
    "# env = gym.make('CartPole-v1')\n",
    "# env = gym.make('MountainCar-v0')\n",
    "# env = gym.make('LunarLander-v2')\n",
    "env = gym.make('BipedalWalker-v3')\n",
    "\n",
    "env.seed(SEED)\n",
    "obs_space = env.observation_space.shape\n",
    "action_space = env.action_space.shape\n",
    "\n",
    "# hyperparameter\n",
    "n_episodes = int(1e6 / env.spec.max_episode_steps)\n",
    "roll_len = 2048\n",
    "n_eval = 100\n",
    "\n",
    "print(f\"n_episodes: {n_episodes}\")\n",
    "print(f\"reward_threshold: {env.spec.reward_threshold}\")\n",
    "\n",
    "# global values\n",
    "steps = 0\n",
    "ep_rewards = []\n",
    "reward_eval = []\n",
    "is_rollout = False\n",
    "is_solved = False\n",
    "\n",
    "# make memories\n",
    "train_memory = []\n",
    "roll_memory = []\n",
    "rewards = []\n",
    "values = []\n",
    "norm_obs = RunningMeanStd(shape=env.observation_space.shape)\n",
    "\n",
    "# make nerual networks\n",
    "net = ActorCriticNet(obs_space, action_space).to(device)\n",
    "old_net = deepcopy(net)\n",
    "\n",
    "param_p = [p for n, p in net.named_parameters() if 'pol' in n]\n",
    "param_v = [p for n, p in net.named_parameters() if 'val' in n]\n",
    "optim_p = torch.optim.AdamW(param_p, lr=P_LR, eps=1e-6, weight_decay=0.01)\n",
    "optim_v = torch.optim.AdamW(param_v, lr=V_LR, eps=1e-6, weight_decay=0.01)\n",
    "optimizer = [optim_p, optim_v]\n",
    "\n",
    "# play!\n",
    "for i in range(1, n_episodes + 1):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    ep_reward = 0\n",
    "    while not done:\n",
    "#         env.render()\n",
    "        if OBS_NORM:\n",
    "            print(obs)\n",
    "            norm_obs.update(np.array([obs]))\n",
    "            obs_norm = np.clip((obs - norm_obs.mean) / np.sqrt(norm_obs.var), -10, 10)\n",
    "            action, value = get_action_and_value(obs_norm, old_net)\n",
    "        else:\n",
    "            action, value = get_action_and_value(obs, old_net)\n",
    "        \n",
    "        _obs, reward, done, _ = env.step(action)\n",
    "\n",
    "        # store\n",
    "        if OBS_NORM:\n",
    "            roll_memory.append([obs_norm, action])\n",
    "        else:\n",
    "            roll_memory.append([obs, action])\n",
    "            \n",
    "\n",
    "        rewards.append(reward)\n",
    "        values.append(value)\n",
    "\n",
    "        obs = _obs\n",
    "        steps += 1\n",
    "        ep_reward += reward\n",
    "\n",
    "        if done or steps % roll_len == 0:\n",
    "            if OBS_NORM:\n",
    "                norm_obs.update(np.array([_obs]))\n",
    "            if done:\n",
    "                _value = 0.\n",
    "            else:\n",
    "                if OBS_NORM:\n",
    "                    _obs_norm = np.clip((_obs - norm_obs.mean) / np.sqrt(norm_obs.var), -10, 10)\n",
    "                    _, _value = get_action_and_value(_obs_norm, old_net)\n",
    "                else:\n",
    "                    _, _value = get_action_and_value(_obs, old_net)\n",
    "\n",
    "            values.append(_value)\n",
    "            train_memory.extend(compute_adv_with_gae(rewards, values, roll_memory))\n",
    "            roll_memory.clear()\n",
    "\n",
    "            if steps % roll_len == 0:\n",
    "                # adv normalize\n",
    "                advs = []\n",
    "                for m in train_memory:\n",
    "                    advs.append(m[3])\n",
    "                advs = np.array(advs)\n",
    "                for m in train_memory:\n",
    "                    m[3] = (m[3] - np.mean(advs)) /  np.std(advs)\n",
    "                \n",
    "                learn(net, old_net, optimizer, train_memory)\n",
    "                old_net.load_state_dict(net.state_dict())\n",
    "                train_memory.clear()\n",
    "                break\n",
    "\n",
    "    if done:\n",
    "        ep_rewards.append(ep_reward)\n",
    "        reward_eval.append(np.mean(ep_rewards[-n_eval:]))\n",
    "        if VIEW_CURVE:\n",
    "            plot()\n",
    "        else:\n",
    "            print(f'{i:3} Episode in {steps:5} steps, reward {ep_reward:.2f}')\n",
    "\n",
    "        if len(ep_rewards) >= n_eval:\n",
    "            if reward_eval[-1] >= env.spec.reward_threshold:\n",
    "                print('\\n{} is sloved! {:3} Episode in {:3} steps'.format(\n",
    "                    env.spec.id, i, steps))\n",
    "                torch.save(net.state_dict(),\n",
    "                           f'./test/saved_models/{env.spec.id}_ep{i}_clear_model_ppo_st.pt')\n",
    "                break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook PPO.ipynb to script\n",
      "[NbConvertApp] Writing 8573 bytes to PPO.py\n"
     ]
    }
   ],
   "source": [
    "# !jupyter nbconvert --to script PPO.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "C51_tensorflow.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
