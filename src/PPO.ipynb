{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.optim import Adam, AdamW\n",
    "import gym\n",
    "import time\n",
    "import core\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "from running_mean_std import RunningMeanStd\n",
    "\n",
    "\n",
    "class PPOBuffer(object):\n",
    "    def __init__(self, obs_dim, act_dim, size, gamma=0.999, lam=0.97):\n",
    "        self.obs_buf = np.zeros(core.combined_shape(size, obs_dim), dtype=np.float32)\n",
    "        self.act_buf = np.zeros(core.combined_shape(size, act_dim), dtype=np.float32)\n",
    "        self.adv_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.rew_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.ret_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.val_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.logp_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.gamma, self.lam = gamma, lam\n",
    "        self.ptr, self.path_start_idx, self.max_size = 0, 0, size\n",
    "\n",
    "    def store(self, obs, act, rew, val, logp):\n",
    "        assert self.ptr < self.max_size\n",
    "        self.obs_buf[self.ptr] = obs\n",
    "        self.act_buf[self.ptr] = act\n",
    "        self.rew_buf[self.ptr] = rew\n",
    "        self.val_buf[self.ptr] = val\n",
    "        self.logp_buf[self.ptr] = logp\n",
    "        self.ptr += 1\n",
    "\n",
    "    def finish_path(self, last_val=0):\n",
    "        path_slice = slice(self.path_start_idx, self.ptr)\n",
    "        rews = np.append(self.rew_buf[path_slice], last_val)\n",
    "        vals = np.append(self.val_buf[path_slice], last_val)\n",
    "        \n",
    "        deltas = rews[:-1] + self.gamma * vals[1:] - vals[:-1]\n",
    "        self.adv_buf[path_slice] = core.discount_cumsum(deltas, self.gamma * self.lam)\n",
    "        self.ret_buf[path_slice] = core.discount_cumsum(rews, self.gamma)[:-1]\n",
    "        self.path_start_idx = self.ptr\n",
    "\n",
    "    def get(self):\n",
    "        assert self.ptr == self.max_size\n",
    "        self.ptr, self.path_start_idx = 0, 0\n",
    "        adv_mean = np.mean(self.adv_buf)\n",
    "        adv_std = np.std(self.adv_buf)\n",
    "        self.adv_buf = (self.adv_buf - adv_mean) / adv_std\n",
    "        data = dict(obs=self.obs_buf, act=self.act_buf, ret=self.ret_buf,\n",
    "                    adv=self.adv_buf, logp=self.logp_buf)\n",
    "        return {k: torch.as_tensor(v, dtype=torch.float32) for k,v in data.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(ep_ret_buf, eval_ret_buf, loss_buf):\n",
    "    clear_output(True)\n",
    "    plt.figure(figsize=(16, 5))\n",
    "    plt.subplot(131)\n",
    "    plt.plot(ep_ret_buf, alpha=0.5)\n",
    "    plt.subplot(131)\n",
    "    plt.plot(eval_ret_buf)\n",
    "    plt.title(f\"Reward: {eval_ret_buf[-1]:.0f}\")\n",
    "    plt.subplot(132)\n",
    "    plt.plot(loss_buf['pi'], alpha=0.5)\n",
    "    plt.title(f\"Pi_Loss: {np.mean(loss_buf['pi'][:-10:]):.3f}\")\n",
    "    plt.subplot(133)\n",
    "    plt.plot(loss_buf['vf'], alpha=0.5)\n",
    "    plt.title(f\"Vf_Loss: {np.mean(loss_buf['vf'][-10:]):.2f}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss_pi(data, ac, clip_ratio):\n",
    "    obs, act, adv, logp_old = data['obs'], data['act'], data['adv'], data['logp']\n",
    "\n",
    "    # Policy loss\n",
    "    pi, logp = ac.pi(obs, act)\n",
    "    ratio = torch.exp(logp - logp_old)\n",
    "    clip_adv = torch.clamp(ratio, 1-clip_ratio, 1+clip_ratio) * adv\n",
    "    loss_pi = -(torch.min(ratio * adv, clip_adv)).mean()\n",
    "\n",
    "    # Useful extra info\n",
    "#     approx_kl = (logp_old - logp).mean().item()\n",
    "    kl_div = ((logp.exp() * (logp - logp_old)).mean()).detach().item()\n",
    "    ent = pi.entropy().mean().detach().item()\n",
    "    clipped = ratio.gt(1+clip_ratio) | ratio.lt(1-clip_ratio)\n",
    "    clipfrac = torch.as_tensor(clipped, dtype=torch.float32).mean().detach().item()\n",
    "    pi_info = dict(kl=kl_div, ent=ent, cf=clipfrac)\n",
    "\n",
    "    return loss_pi, pi_info\n",
    "\n",
    "def compute_loss_v(data, ac):\n",
    "    obs, ret = data['obs'], data['ret']\n",
    "    return ((ac.v(obs) - ret)**2).mean()\n",
    "\n",
    "\n",
    "def update(buf, train_pi_iters, train_vf_iters, clip_ratio, target_kl, ac, pi_optimizer, vf_optimizer, loss_buf):\n",
    "    data = buf.get()\n",
    "\n",
    "    # Train policy with multiple steps of gradient descent\n",
    "    for i in range(train_pi_iters):\n",
    "        pi_optimizer.zero_grad()\n",
    "        loss_pi, pi_info = compute_loss_pi(data, ac, clip_ratio)\n",
    "        loss_buf['pi'].append(loss_pi.item())\n",
    "        kl = pi_info['kl']\n",
    "        if kl > 1.5 * target_kl:\n",
    "            print('Early stopping at step %d due to reaching max kl.'%i)\n",
    "            break\n",
    "        loss_pi.backward()\n",
    "        pi_optimizer.step()\n",
    "\n",
    "    # Value function learning\n",
    "    for i in range(train_vf_iters):\n",
    "        vf_optimizer.zero_grad()\n",
    "        loss_vf = compute_loss_v(data, ac)\n",
    "        loss_buf['vf'].append(loss_vf.item())\n",
    "        loss_vf.backward()\n",
    "        vf_optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    actor_critic=core.MLPActorCritic\n",
    "    hidden_size = 64\n",
    "    activation = torch.nn.Tanh\n",
    "    seed=5\n",
    "    steps_per_epoch=2048\n",
    "    epochs=1000\n",
    "    gamma=0.999\n",
    "    lam=0.97\n",
    "    clip_ratio=0.2\n",
    "    pi_lr=3e-4\n",
    "    vf_lr=1e-3\n",
    "    train_pi_iters=80\n",
    "    train_vf_iters=80\n",
    "    max_ep_len=1000\n",
    "    target_kl=0.01\n",
    "    save_freq=10\n",
    "    obs_norm = True\n",
    "    view_curve = False\n",
    "\n",
    "    # make an environment\n",
    "#     env = gym.make('CartPole-v0')\n",
    "#     env = gym.make('CartPole-v1')\n",
    "#     env = gym.make('MountainCar-v0')\n",
    "    env = gym.make('LunarLander-v2')\n",
    "#     env = gym.make('BipedalWalker-v3')\n",
    "    print(f\"reward_threshold: {env.spec.reward_threshold}\")\n",
    "\n",
    "    obs_dim = env.observation_space.shape\n",
    "    act_dim = env.action_space.shape\n",
    "\n",
    "    # Random seed\n",
    "    env.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # Create actor-critic module\n",
    "    ac = actor_critic(env.observation_space, env.action_space, (hidden_size, hidden_size), activation)\n",
    "    \n",
    "    # Set up optimizers for policy and value function\n",
    "    pi_optimizer = AdamW(ac.pi.parameters(), lr=pi_lr, eps=1e-6)\n",
    "    vf_optimizer = AdamW(ac.v.parameters(), lr=vf_lr, eps=1e-6)\n",
    "\n",
    "    # Count variables\n",
    "    var_counts = tuple(core.count_vars(module) for module in [ac.pi, ac.v])\n",
    "\n",
    "    # Set up experience buffer\n",
    "    local_steps_per_epoch = int(steps_per_epoch)\n",
    "    buf = PPOBuffer(obs_dim, act_dim, local_steps_per_epoch, gamma, lam)\n",
    "    \n",
    "    # Prepare for interaction with environment\n",
    "    start_time = time.time()\n",
    "    o, ep_ret, ep_len = env.reset(), 0, 0\n",
    "    ep_num = 0\n",
    "    ep_ret_buf, eval_ret_buf = [], []\n",
    "    loss_buf = {'pi': [], 'vf': []}\n",
    "    obs_normalizer = RunningMeanStd(shape=env.observation_space.shape)\n",
    "    # Main loop: collect experience in env and update/log each epoch\n",
    "    for epoch in range(epochs):\n",
    "        for t in range(local_steps_per_epoch):\n",
    "            if obs_norm:\n",
    "                obs_normalizer.update(np.array([o]))\n",
    "                o_norm = np.clip((o - obs_normalizer.mean) / np.sqrt(obs_normalizer.var), -10, 10)\n",
    "                a, v, logp = ac.step(torch.as_tensor(o_norm, dtype=torch.float32))\n",
    "            else:\n",
    "                a, v, logp = ac.step(torch.as_tensor(o, dtype=torch.float32))\n",
    "\n",
    "            next_o, r, d, _ = env.step(a)\n",
    "            ep_ret += r\n",
    "            ep_len += 1\n",
    "\n",
    "            # save and log\n",
    "            if obs_norm:\n",
    "                buf.store(o_norm, a, r, v, logp)\n",
    "            else:\n",
    "                buf.store(o, a, r, v, logp)\n",
    "\n",
    "            # Update obs\n",
    "            o = next_o\n",
    "\n",
    "            timeout = ep_len == max_ep_len\n",
    "            terminal = d or timeout\n",
    "            epoch_ended = t==local_steps_per_epoch-1\n",
    "\n",
    "            if terminal or epoch_ended:\n",
    "                if timeout or epoch_ended:\n",
    "                    if obs_norm:\n",
    "                        obs_normalizer.update(np.array([o]))\n",
    "                        o_norm = np.clip((o - obs_normalizer.mean) / np.sqrt(obs_normalizer.var), -10, 10)\n",
    "                        _, v, _ = ac.step(torch.as_tensor(o_norm, dtype=torch.float32))\n",
    "                    else:\n",
    "                        _, v, _ = ac.step(torch.as_tensor(o, dtype=torch.float32))\n",
    "                else:\n",
    "                    if obs_norm:\n",
    "                        obs_normalizer.update(np.array([o]))\n",
    "                    v = 0\n",
    "                buf.finish_path(v)\n",
    "                if terminal:\n",
    "                    ep_ret_buf.append(ep_ret)\n",
    "                    eval_ret_buf.append(np.mean(ep_ret_buf[-100:]))\n",
    "                    ep_num += 1\n",
    "                    if view_curve:\n",
    "                        plot(ep_ret_buf, eval_ret_buf, loss_buf)\n",
    "                    else:\n",
    "                        print(f'Episode: {ep_num:3} Reward: {ep_reward:3}')\n",
    "                    if eval_ret_buf[-1] >= env.spec.reward_threshold:\n",
    "                        print(f\"\\n{env.spec.id} is sloved! {ep_num} Episode\")\n",
    "                        return\n",
    "\n",
    "                o, ep_ret, ep_len = env.reset(), 0, 0\n",
    "        # Perform PPO update!\n",
    "        update(buf, train_pi_iters, train_vf_iters, clip_ratio, target_kl, ac, pi_optimizer, vf_optimizer, loss_buf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
